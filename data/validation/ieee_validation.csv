"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment","J. Xu; Y. Rao; X. Yu; G. Chen; J. Zhou; J. Lu","Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China",2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"27 Sep 2022","2022","","","2939","2948","Most existing action quality assessment methods rely on the deep features of an entire video to predict the score, which is less reliable due to the non-transparent inference process and poor interpretability. We argue that understanding both high-level semantics and internal temporal structures of actions in competitive sports videos is the key to making predictions accurate and interpretable. Towards this goal, we construct a new fine-grained dataset, called FineDiving, developed on diverse diving events with detailed annotations on action procedures. We also propose a procedure-aware approach for action quality assessment, learned by a new Temporal Segmentation Attention module. Specifically, we propose to parse pairwise query and exemplar action instances into consecutive steps with diverse semantic and temporal correspondences. The procedure-aware cross-attention is proposed to learn embeddings between query and exemplar steps to discover their semantic, spatial, and temporal correspondences, and further serve for fine-grained contrastive regression to derive a reliable scoring mechanism. Extensive experiments demonstrate that our approach achieves substantial improvements over the state-of-the-art methods with better interpretability. The dataset and code are available at https://github.com/xujinglin/FineDiving.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00296","National Natural Science Foundation of China(grant numbers:62125603,62106124,U1813218); China Postdoctoral Science Foundation(grant numbers:2020M680564); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879654","Video analysis and understanding; Action and event recognition","Computer vision;Codes;Annotations;Semantics;Quality assessment;Pattern recognition;Reliability","","85","","50","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"LOGO: A Long-Form Video Dataset for Group Action Quality Assessment","S. Zhang; W. Dai; S. Wang; X. Shen; J. Lu; J. Zhou; Y. Tang","Shenzhen International Graduate School, Tsinghua University; Shenzhen International Graduate School, Tsinghua University; Shenzhen International Graduate School, Tsinghua University; Shenzhen International Graduate School, Tsinghua University; Department of Automation, Tsinghua University; Department of Automation, Tsinghua University; Shenzhen International Graduate School, Tsinghua University",2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"22 Aug 2023","2023","","","2405","2414","Action quality assessment (AQA) has become an emerging topic since it can be extensively applied in numerous scenarios. However, most existing methods and datasets focus on single-person short-sequence scenes, hindering the application of AQA in more complex situations. To address this issue, we construct a new multi-person long-form video dataset for action quality assessment named LOGO. Distinguished in scenario complexity, our dataset contains 200 videos from 26 artistic swimming events with 8 athletes in each sample along with an average duration of 204.2 seconds. As for richness in annotations, LOGO includes formation labels to depict group information of multiple athletes and detailed annotations on action procedures. Furthermore, we propose a simple yet effective method to model relations among athletes and reason about the potential temporal logic in long-form videos. Specifically, we design a group-aware attention module, which can be easily plugged into existing AQA methods, to enrich the clip-wise representations based on contextual group information. To benchmark LOGO, we systematically conduct investigations on the performance of several popular methods in AQA and action segmentation. The results reveal the challenges our dataset brings. Extensive experiments also show that our approach achieves state-of-the-art on the LOGO dataset. The dataset and code will be released at https://github.com/shiyi-zh0408/LOGO.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00238","National Natural Science Foundation of China(grant numbers:62206153,62125603); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204722","Video: Action and event understanding","Computer vision;Codes;Annotations;Fuses;Benchmark testing;Quality assessment;Pattern recognition","","31","","58","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment","A. Dadashzadeh; S. Duan; A. Whone; M. Mirmehdi","School of Computer Science, University of Bristol, UK; School of Computer Science, University of Bristol, UK; Translational Health Sciences, University of Bristol, UK; School of Computer Science, University of Bristol, UK",2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),"9 Apr 2024","2024","","","42","52","The limited availability of labelled data in Action Quality Assessment (AQA), has forced previous works to fine-tune their models pretrained on large-scale domain-general datasets. This common approach results in weak generalisation, particularly when there is a significant domain shift. We propose a novel, parameter efficient, continual pretraining framework, PECoP, to reduce such domain shift via an additional pretraining stage. In PECoP, we introduce 3D-Adapters, inserted into the pretrained model, to learn spatiotemporal, in-domain information via self-supervised learning where only the adapter modules’ parameters are updated. We demonstrate PECoP’s ability to enhance the performance of recent state-of-the-art methods (MUSDL, CoRe, and TSA) applied to AQA, leading to considerable improvements on benchmark datasets, JIGSAWS (↑ 6.0%), MTL-AQA (↑ 0.99%), and FineDiving (↑ 2.54%). We also present a new Parkinson’s Disease dataset, PD4T, of real patients performing four various actions, where we surpass (↑ 3.56%) the state-of-the-art in comparison. Our code, pretrained models, and the PD4T dataset are available at https://github.com/Plrbear/PECoP.","2642-9381","979-8-3503-1892-0","10.1109/WACV57701.2024.00012","University of Bristol; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484297","Algorithms;Machine learning architectures;formulations;and algorithms;Algorithms;Biometrics;face;gesture;body pose;Algorithms;Video recognition and understanding","Training;Computer vision;Costs;Self-supervised learning;Benchmark testing;Data models;Quality assessment","","17","","51","IEEE","9 Apr 2024","","","IEEE","IEEE Conferences"
"FineParser: A Fine-Grained Spatio-Temporal Action Parser for Human-Centric Action Quality Assessment","J. Xu; S. Yin; G. Zhao; Z. Wang; Y. Peng","School of Intelligence Science and Technology, University of Science and Technology Beijing; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","14628","14637","Existing action quality assessment (AQA) methods mainly learn deep representations at the video level for scoring diverse actions. Due to the lack of a fine-grained understanding of actions in videos, they harshly suffer from low credibility and interpretability, thus insufficient for stringent applications, such as Olympic diving events. We argue that a fine-grained understanding of actions requires the model to perceive and parse actions in both time and space, which is also the key to the credibility and inter-pretability of the AQA technique. Based on this insight, we propose a new fine-grained spatial-temporal action parser named FineParser. It learns human-centric foreground action representations by focusing on target action regions within each frame and exploiting their fine-grained alignments in time and space to minimize the impact of in-valid backgrounds during the assessment. In addition, we construct fine-grained annotations of human-centric fore-ground action masks for the FineDiving dataset, called FineDiving-HM. With refined annotations on diverse target action procedures, FineDiving-HM can promote the development of real-world AQA systems. Through extensive experiments, we demonstrate the effectiveness of FineParser, which outperforms state-of-the-art methods while supporting more tasks of fine-grained action understanding. Data and code are available at https://github.com/PKU-ICST-MIPL/FineParser_CVPR2024.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.01386","National Natural Science Foundation of China(grant numbers:61925201,62132001,62373043); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10658132","Action Quality Assessment;Fine-grained Action Understanding;Human-centric Foreground Action Masks","Computer vision;Codes;Annotations;Focusing;Quality assessment;Pattern recognition","","16","","42","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"A Survey of Video-based Action Quality Assessment","S. Wang; D. Yang; P. Zhai; Q. Yu; T. Suo; Z. Sun; K. Li; L. Zhang","Institute of AI & Robotics, Fudan University, Shanghai, China; Institute of AI & Robotics, Fudan University, Shanghai, China; Institute of AI & Robotics, Fudan University, Shanghai, China; ZhongShan Hospital, Shanghai, China; ZhongShan Hospital, Shanghai, China; ZhongShan Hospital, Shanghai, China; ZhongShan Hospital, Shanghai, China; Institute of AI & Robotics, Fudan University, Shanghai, China",2021 International Conference on Networking Systems of AI (INSAI),"19 Apr 2022","2021","","","1","9","Human action recognition and analysis have great demand and important application significance in video surveillance, video retrieval, and human-computer interaction. The task of human action quality evaluation requires the intelligent system to automatically and objectively evaluate the action completed by the human. The action quality assessment model can reduce the human and material resources spent in action evaluation and reduce subjectivity. In this paper, we provide a comprehensive survey of existing papers on video-based action quality assessment. Different from human action recognition, the application scenario of action quality assessment is relatively narrow. Most of the existing work focuses on sports and medical care. We first introduce the definition and challenges of human action quality assessment. Then we present the existing datasets and evaluation metrics. In addition, we summarized the methods of sports and medical care according to the model categories and publishing institutions according to the characteristics of the two fields. At the end, combined with recent work, the promising development direction in action quality assessment is discussed.","","978-1-6654-0859-2","10.1109/INSAI54028.2021.00029","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9757944","action quality assessment;human behavior analysis;deep learning;computer vision","Training;Measurement;Human computer interaction;Publishing;Medical services;Video surveillance;Quality assessment","","16","","61","IEEE","19 Apr 2022","","","IEEE","IEEE Conferences"
"Action Quality Assessment Across Multiple Actions","P. Parmar; B. Morris","University of Nevada, Las Vegas; University of Nevada, Las Vegas",2019 IEEE Winter Conference on Applications of Computer Vision (WACV),"7 Mar 2019","2019","","","1468","1476","Can learning to measure the quality of an action help in measuring the quality of other actions? If so, can consolidated samples from multiple actions help improve the performance of current approaches? In this paper, we carry out experiments to see if knowledge transfer is possible in the action quality assessment (AQA) setting. Experiments are carried out on our newly released AQA dataset (http://rtis.oit.unlv.edu/datasets.html) consisting of 1106 action samples from seven actions with quality as measured by expert human judges. Our experimental results show that there is utility in learning a single model across multiple actions.","1550-5790","978-1-7281-1975-5","10.1109/WACV.2019.00161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658709","","Sports;Synchronization;Current measurement;Task analysis;Quality assessment;Cameras;Training","","109","","28","IEEE","7 Mar 2019","","","IEEE","IEEE Conferences"
"Group-aware Contrastive Regression for Action Quality Assessment","X. Yu; Y. Rao; W. Zhao; J. Lu; J. Zhou","Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China",2021 IEEE/CVF International Conference on Computer Vision (ICCV),"28 Feb 2022","2021","","","7899","7908","Assessing action quality is challenging due to the subtle differences between videos and large variations in scores. Most existing approaches tackle this problem by regressing a quality score from a single video, suffering a lot from the large inter-video score variations. In this paper, we show that the relations among videos can provide important clues for more accurate action quality assessment during both training and inference. Specifically, we reformulate the problem of action quality assessment as regressing the relative scores with reference to another video that has shared attributes (e.g., category and difficulty), instead of learning unreferenced scores. Following this formulation, we propose a new Contrastive Regression (CoRe) framework to learn the relative scores by pair-wise comparison, which highlights the differences between videos and guides the models to learn the key hints for assessment. In order to further exploit the relative information between two videos, we devise a group-aware regression tree to convert the conventional score regression into two easier sub-problems: coarse-to-fine classification and regression in small intervals. To demonstrate the effectiveness of CoRe, we conduct extensive experiments on three mainstream AQA datasets including AQA-7, MTL-AQA and JIGSAWS. Our approach outperforms previous methods by a large margin and establishes new state-of-the-art on all three benchmarks.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00782","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711028","Video analysis and understanding","Training;Computer vision;Benchmark testing;Quality assessment;Task analysis;Regression tree analysis;Videos","","85","","42","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Tai Chi Action Quality Assessment and Visual Analysis with a Consumer RGB-D Camera","J. Li; H. Hu; Q. Xing; X. Wang; J. Li; Y. Shen","School of Sports Engineering, Beijing Sports University, Beijing, China; School of Sports Engineering, Beijing Sports University, Beijing, China; School of Sports Engineering, Beijing Sports University, Beijing, China; School of Sports Engineering, Beijing Sports University, Beijing, China; School of Sports Engineering, Beijing Sports University, Beijing, China; School of Sports Engineering, Beijing Sports University, Beijing, China",2022 IEEE 24th International Workshop on Multimedia Signal Processing (MMSP),"22 Nov 2022","2022","","","1","6","Visual-based human action analysis is an important research topic in the field of computer vision, and has great application prospect in intelligent sports. Home-based fitness is increasingly common in recent years, however lacking of accurate feedback and scientific guidance main easily lead to problems such as exercise injuries. In this paper, we propose an analysis system for Tai Chi action quality assessment and visual analysis with a consumer RGB-D camera. The main innovative work is as follows: 1) for home-based fitness action evaluation, we design a real-time intelligent analysis system combined with expert rules through a consumer RGB-D camera; 2) we transform the evaluation of 24-form Tai Chi Chuan into an artificial intelligence (AI) model, and realize action recognition and assessment through computer vision; 2) to train the AI model, we build a new dataset named TaiChi-24, which contains 1,408 samples with RGB-D images and 3D skeletons. We carry out evaluation experiments and analyses, and the experimental results have shown the advantage of applying our evaluation method on the proposed TaiChi-24 dataset.","2473-3628","978-1-6654-7189-3","10.1109/MMSP55362.2022.9949464","National Natural Science Foundation of China(grant numbers:72071018); Open Projects Program of National Laboratory of Pattern Recognition(grant numbers:202100009); Fundamental Research Funds for Central Universities(grant numbers:2021Td006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9949464","Sports performance analysis;Action quality evaluation;Action recognition;Visual analysis","Training;Visualization;Computer vision;Analytical models;Solid modeling;Computational modeling;Transforms","","6","","26","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"Uncertainty-Aware Score Distribution Learning for Action Quality Assessment","Y. Tang; Z. Ni; J. Zhou; D. Zhang; J. Lu; Y. Wu; J. Zhou","Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Electrical and Computer Engineering Department, Northwestern University; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Electrical and Computer Engineering Department, Northwestern University; Department of Automation, Tsinghua University, China",2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"5 Aug 2020","2020","","","9836","9845","Assessing action quality from videos has attracted growing attention in recent years. Most existing approaches usually tackle this problem based on regression algorithms, which ignore the intrinsic ambiguity in the score labels caused by multiple judges or their subjective appraisals. To address this issue, we propose an uncertainty-aware score distribution learning (USDL) approach for action quality assessment (AQA). Specifically, we regard an action as an instance associated with a score distribution, which describes the probability of different evaluated scores. Moreover, under the circumstance where finer-grained score labels are available (e.g., difficulty degree of an action or multiple scores from different judges), we further devise a multi-path uncertainty-aware score distribution learning (MUSDL) method to explore the disentangled components of a score. In order to demonstrate the effectiveness of our proposed methods, We conduct experiments on two AQA datasets containing various Olympic actions. Our approaches set new state-of-the-arts under the Spearman's Rank Correlation (i.e., 0.8102 on AQA-7 and 0.9273 on MTL-AQA).","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.00986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9157684","","Videos;Uncertainty;Games;Gaussian distribution;Task analysis;Quality assessment;Training","","120","","44","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment","P. Parmar; B. T. Morris","University of Nevada, Las Vegas; University of Nevada, Las Vegas",2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"9 Jan 2020","2019","","","304","313","Can performance on the task of action quality assessment (AQA) be improved by exploiting a description of the action and its quality? Current AQA and skills assessment approaches propose to learn features that serve only one task - estimating the final score. In this paper, we propose to learn spatio-temporal features that explain three related tasks - fine-grained action recognition, commentary generation, and estimating the AQA score. A new multitask-AQA dataset, the largest to date, comprising of 1412 diving samples was collected to evaluate our approach (http://rtis.oit.unlv.edu/datasets.html). We show that our MTL approach outperforms STL approach using two different kinds of architectures: C3D-AVG and MSCADC. The C3D-AVG-MTL approach achieves the new state-of-the-art performance with a rank correlation of 90.44%. Detailed experiments were performed to show that MTL offers better generalization than STL, and representations from action recognition models are not sufficient for the AQA task and instead should be learned.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954464","Face;Gesture;and Body Pose;Action Recognition;Datasets and Evaluation;Video Analytics","Computer vision;Correlation;Computational modeling;Computer architecture;Quality assessment;Spatiotemporal phenomena;Pattern recognition","","146","","33","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Piano Skills Assessment","P. Parmar; J. Reddy; B. Morris","Dept. of Computer Science, University of British Columbia, Canada; Dept. of Computer Science, Stanford University, USA; Dept. of Electrical Engineering, University of Nevada, Las Vegas, USA",2021 IEEE 23rd International Workshop on Multimedia Signal Processing (MMSP),"16 Mar 2022","2021","","","1","5","Can a computer determine a piano player’s skill level? Is it preferable to base this assessment on visual analysis of the player’s performance or should we trust our ears over our eyes? Since current convolutional neural networks (CNNs) have difficulty processing long video videos, how can shorter clips be sampled to best reflect the players skill level? In this work, we collect and release a first-of-its-kind dataset for multimodal skill assessment focusing on assessing piano player’s skill level, answer the asked questions, initiate work in automated evaluation of piano playing skills and provide baselines for future work. Dataset can be accessed from: https://github.com/ParitoshParmar/Piano-Skills-Assessment.","2473-3628","978-1-6654-3288-7","10.1109/MMSP53017.2021.9733638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733638","Skills Assessment;Automated Piano Skills Assessment;Action Quality Assessment;Video Understanding;Audio Classification;Video Classification;Computer Vision","Visualization;Computer vision;Conferences;Computational modeling;Focusing;Ear;Signal processing","","26","","31","IEEE","16 Mar 2022","","","IEEE","IEEE Conferences"
"FineRehab: A Multi-modality and Multi-task Dataset for Rehabilitation Analysis","J. Li; J. Xue; R. Cao; X. Du; S. Mo; K. Ran; Z. Zhang","School of Sports Engineering, Beijing Sport University, China; School of Sports Engineering, Beijing Sport University, China; School of Sports Engineering, Beijing Sport University, China; Department of Neurorehabilitation, Rehabilitation Research Center, China; School of Sports Engineering, Beijing Sport University, China; School of Sports Engineering, Beijing Sport University, China; Department of Neurorehabilitation, Rehabilitation Research Center, China",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),"27 Sep 2024","2024","","","3184","3193","The assessment of rehabilitation exercises for neurological and musculoskeletal disorders are crucial for recovery. Traditionally, assessment methods have been subjective, with inherent uncertainty and limitations. This paper introduces a novel multi-modality dataset named FineRehab§ to prompt the study of rehabilitation movement analysis, leveraging advancements in sensor technology and artificial intelligence. FineRehab collects 16 actions from 50 participants, including both patients with musculoskeletal disorders and healthy individuals, and consists of 4,215 action samples captured by two Kinect cameras and 17 IMUs. To benchmark FineRehab, we present a reliable approach to analyze rehabilitation exercises, and make experiments to evaluate the comprehensive movement quality from across multi-dimensions. Comparative experimental analyses have verified the validity of our dataset in distinguishing between the movement of the normal population and patients, which can offer a quantifiable basis for personalized rehabilitation feedback. The introduction of FineRehab will encourage researchers to apply, develop and adapt various methods for rehabilitation exercise analysis.","2160-7516","979-8-3503-6547-4","10.1109/CVPRW63382.2024.00324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10678182","Action recognition;Action quality assessment;Rehabilitation exercise;Dataset","Deep learning;Musculoskeletal system;Computer vision;Uncertainty;Conferences;Multitasking;Cameras","","7","","32","IEEE","27 Sep 2024","","","IEEE","IEEE Conferences"
"A Contrastive Learning Network for Performance Metric and Assessment of Physical Rehabilitation Exercises","L. Yao; Q. Lei; H. Zhang; J. Du; S. Gao","Xiamen Key Laboratory of Computer Vision and Pattern Recognition and the Department of Computer Science and Technology, Huaqiao University, Xiamen, China; Xiamen Key Laboratory of Computer Vision and Pattern Recognition and the Department of Computer Science and Technology, Huaqiao University, Xiamen, China; Fujian Key Laboratory of Big Data Intelligence and Security, Huaqiao University, Xiamen, China; Fujian Key Laboratory of Big Data Intelligence and Security, Huaqiao University, Xiamen, China; Faculty of Engineering, University of Toyama, Toyama-shi, Japan",IEEE Transactions on Neural Systems and Rehabilitation Engineering,"28 Sep 2023","2023","31","","3790","3802","Human activity analysis in the legal monitoring environment plays an important role in the physical rehabilitation field, as it helps patients with physical injuries improve their postoperative conditions and reduce their medical costs. Recently, several deep learning-based action quality assessment (AQA) frameworks have been proposed to evaluate physical rehabilitation exercises. However, most of them treat this problem as a simple regression task, which requires both the action instance and its score label as input. This approach is limited by the fact that the annotations in this field usually consist of healthy or unhealthy labels rather than quality scores provided by professional physicians. Additionally, most of these methods cannot provide informative feedback on a patient’s motion defects, which weakens their practical application. To address these problems, we propose a multi-task contrastive learning framework to learn subtle and critical differences from skeleton sequences to deal with the performance metric and AQA problems of physical rehabilitation exercises. Specifically, we propose a performance metric network that takes triplets of training samples as input for score generation. For the AQA task, the same contrast learning strategy is used, but pairwise training samples are fed into the action quality assessment network for score prediction. Notably, we propose quantifying the deviation of the joint attention matrix between different skeleton sequences and introducing it into the loss function of our learning network. It is proven that considering both score prediction loss and joint attention deviation loss improves physical exercises AQA performance. Furthermore, it helps to obtain informative feedback for patients to improve their motion defects by visualizing the joint attention matrix’s difference. The proposed method is verified on the UI-PRMD and KIMORE datasets. Experimental results show that the proposed method achieves state-of-the-art performance.","1558-0210","","10.1109/TNSRE.2023.3317411","National Natural Science Foundation of China(grant numbers:62001176,61871196); Natural Science Foundation of Fujian Province, China(grant numbers:2020J01085,2019J01082); National Key Research and Development Program of China(grant numbers:2019YFC1604700); Promotion Program for Young and Middle-Aged Teacher in Science and Technology Research of Huaqiao University(grant numbers:ZQN-YX601); Japan Society for the Promotion of Science (JSPS) KAKENHI(grant numbers:JP22H03643); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256137","Physical rehabilitation exercise;action quality assessment;performance metric quantification;contrastive learning;informative feedback","Feature extraction;Skeleton;Training;Measurement;Task analysis;Quality assessment;Sports","Humans;Exercise Therapy;Exercise;Motion","13","","58","CCBY","20 Sep 2023","","","IEEE","IEEE Journals"
"AI Trainer: Autoencoder Based Approach for Squat Analysis and Correction","M. Chariar; S. Rao; A. Irani; S. Suresh; C. S. Asha","Department of Mechatronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, India; Department of Mechatronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, India; Department of Mechatronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, India; Department of Mechatronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, India; Department of Mechatronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, India",IEEE Access,"6 Oct 2023","2023","11","","107135","107149","Artificial intelligence and computer vision have widespread applications in workout analysis. It has been extensively used in sports and the athlete industry to identify errors and improve performance. Furthermore, these methods prevent injuries caused by a lack of instructors or costly infrastructure. One such exercise is the squat, which is a movement in which a standing person descends to a posture with their torso vertical and their knees firmly bent, then returns to their original upright position. Each person’s squat is distinct, with varying limb lengths causing their form to change when observed. It has been observed that the mobility of various joints and muscular strength have a role in this. A squat improves the user by increasing overall leg strength, strengthening knee and hip joints, and lowering the risk of heart disease due to cardiovascular development. This paper presents a method for classifying squat types and recommending the right squat version. This study uses MediaPipe and a deep learning-based technique to decide if squatting is good or bad. A stacked Bidirectional Gated Recurrent Unit (Bi-GRU) model with an attention layer is proposed to consistently and fairly assess each user, categorizing squats into seven classes. This stacked Bi-GRU model with an attention unit is then compared to other cutting-edge models, both with and without the attention layer. The model outperforms other models by attaining an accuracy of 94% and is demonstrated to work the best and most consistently for our dataset. Furthermore, the individual executing the incorrect squat is corrected to the best of their ability, depending on their performance and body proportions, by providing the correct form.","2169-3536","","10.1109/ACCESS.2023.3316009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254650","Action quality assessment;attention;computer vision;curve fitting;gated recurrent unit;pose estimation;squat","Pose estimation;Long short term memory;Hidden Markov models;Three-dimensional displays;Videos;Human activity recognition;Artificial intelligence","","18","","73","CCBYNCND","18 Sep 2023","","","IEEE","IEEE Journals"
"A Video-Based Augmented Reality System for Human-in-the-Loop Muscle Strength Assessment of Juvenile Dermatomyositis","K. Zhou; R. Cai; Y. Ma; Q. Tan; X. Wang; J. Li; H. P. H. Shum; F. W. B. Li; S. Jin; X. Liang","Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Children's Hospital of Capital Institute of Pediatrics, Beijing, China; Children's Hospital of Capital Institute of Pediatrics, Beijing, China; Children's Hospital of Capital Institute of Pediatrics, Beijing, China; Durham University, Durham, United Kingdom; Durham University, Durham, United Kingdom; Beijing Diannaite Medical Technology Co., Ltd., China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China",IEEE Transactions on Visualization and Computer Graphics,"29 Mar 2023","2023","29","5","2456","2466","As the most common idiopathic inflammatory myopathy in children, juvenile dermatomyositis (JDM) is characterized by skin rashes and muscle weakness. The childhood myositis assessment scale (CMAS) is commonly used to measure the degree of muscle involvement for diagnosis or rehabilitation monitoring. On the one hand, human diagnosis is not scalable and may be subject to personal bias. On the other hand, automatic action quality assessment (AQA) algorithms cannot guarantee 100% accuracy, making them not suitable for biomedical applications. As a solution, we propose a video-based augmented reality system for human-in-the-loop muscle strength assessment of children with JDM. We first propose an AQA algorithm for muscle strength assessment of JDM using contrastive regression trained by a JDM dataset. Our core insight is to visualize the AQA results as a virtual character facilitated by a 3D animation dataset, so that users can compare the real-world patient and the virtual character to understand and verify the AQA results. To allow effective comparisons, we propose a video-based augmented reality system. Given a feed, we adapt computer vision algorithms for scene understanding, evaluate the optimal way of augmenting the virtual character into the scene, and highlight important parts for effective human verification. The experimental results confirm the effectiveness of our AQA algorithm, and the results of the user study demonstrate that humans can more accurately and quickly assess the muscle strength of children using our system.","1941-0506","","10.1109/TVCG.2023.3247092","National Natural Science Foundation of China(grant numbers:6227201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049714","Action Quality Assessment;Augmented Reality;Human-in-the-Loop System;Juvenile Dermatomyositis","Muscles;Medical services;Animation;Visualization;Pediatrics;Medical diagnostic imaging;Human in the loop","","16","","50","IEEE","22 Feb 2023","","","IEEE","IEEE Journals"
"Likert Scoring with Grade Decoupling for Long-term Action Assessment","A. Xu; L. -A. Zeng; W. -S. Zheng","School of Computer Science and Engineering, Sun Yat-sen University, China; School of Artificial Intelligence, Sun Yat-sen University, China; School of Computer Science and Engineering, Sun Yat-sen University, China",2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"27 Sep 2022","2022","","","3222","3231","Long-term action quality assessment is a task of evaluating how well an action is performed, namely, estimating a quality score from a long video. Intuitively, long-term actions generally involve parts exhibiting different levels of skill, and we call the levels of skill as performance grades. For example, technical highlights and faults may appear in the same long-term action. Hence, the final score should be determined by the comprehensive effect of different grades exhibited in the video. To explore this latent relationship, we design a novel Likert scoring paradigm in-spired by the Likert scale in psychometrics, in which we quantify the grades explicitly and generate the final quality score by combining the quantitative values and the corresponding responses estimated from the video, instead of performing direct regression. Moreover, we extract grade-specific features, which will be used to estimate the responses of each grade, through a Transformer decoder architecture with diverse learnable queries. The whole model is named as Grade-decoupling Likert Transformer (GDLT), and we achieve state-of-the-art results on two long-term action assessment datasets.11Project page https://isee-ai.cn/-angchi/CVPR22_GDLT.html","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880394","Video analysis and understanding","Computer vision;Computational modeling;Estimation;Computer architecture;Transformers;Feature extraction;Decoding","","28","","48","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Narrative Action Evaluation with Prompt-Guided Multimodal Interaction","S. Zhang; S. Bai; G. Chen; L. Chen; J. Lu; J. Wang; Y. Tang","Shenzhen International Graduate School, Tsinghua University; Shenzhen International Graduate School, Tsinghua University; Carnegie Mellon University, Pittsburgh, PA, USA; Department of Automation, Tsinghua University; Department of Automation, Tsinghua University; Tencent; Carnegie Mellon University, Pittsburgh, PA, USA",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","18430","18439","In this paper, we investigate a new problem called narrative action evaluation (NAE). NAE aims to generate professional commentary that evaluates the execution of an action. Unlike traditional tasks such as score-based action qual-ity assessment and video captioning involving superficial sentences, NAE focuses on creating detailed narratives in natural language. These narratives provide intricate descriptions of actions along with objective evaluations. NAE is a more challenging task because it requires both narrative flex-ibility and evaluation rigor. One existing possible solution is to use multi-task learning, where narrative language and evaluative information are predicted separately. However, this approach results in reduced performance for individual tasks because of variations between tasks and differences in modality between language information and evaluation information. To address this, we propose a prompt-guided multimodal interaction framework. This framework utilizes a pair of transformers to facilitate the interaction between different modalities of information. It also uses prompts to transform the score regression task into a video-text matching task, thus enabling task interactivity. To support further research in this field, we re-annotate the MTL-AQA and FineGym datasets with high-quality and comprehensive action narration. Additionally, we establish benchmarks for NAE. Extensive experiment results prove that our method outperforms separate learning methods and naive multi-task learning methods. Data and code are released at here.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.01744","National Natural Science Foundation of China(grant numbers:62125603,62321005,62336004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10655159","","Learning systems;Computer vision;Codes;Computational modeling;Natural languages;Transforms;Benchmark testing","","4","","45","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
