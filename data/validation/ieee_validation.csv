"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"MMW-AQA: Multimodal In-the-Wild Dataset for Action Quality Assessment","T. Nagai; S. Takeda; S. Suzuki; H. Seshimo","Nippon Telegraph and Telephone Corporation, Yokosuka, Japan; Nippon Telegraph and Telephone Corporation, Yokosuka, Japan; Nippon Telegraph and Telephone Corporation, Yokosuka, Japan; Nippon Telegraph and Telephone Corporation, Yokosuka, Japan",IEEE Access,"10 Jul 2024","2024","12","","92062","92072","Action quality assessment (AQA) is a task for assessing a specific action quality in videos. Since existing AQA datasets provide only two-dimensional (2D) video data captured from fewer viewpoints, existing AQA methods based on deep neural networks (DNNs) often struggle to assess complex three-dimensional (3D) actions accurately, and their robustness against diversified viewpoints remains unknown. We created a dataset called multimodal in-the-wild (MMW)-AQA in freestyle windsurfing that addresses these concerns. In addition to video data, MMW-AQA provides inertial measurement unit (IMU) and global positioning system (GPS) data. The 3D information of IMU data helps DNNs accurately assess complex 3D actions. Moreover, MMW-AQA provides wild video data captured by a single unmanned aerial vehicle (UAV). These wild video data enable us to evaluate whether AQA methods can work well on diversified viewpoints. Furthermore, we also present the baseline multimodalization framework with a transformer-based fusion module. These frameworks multimodalize existing unimodal DNN models easily to assess action quality using multimodal data. Our experimental results demonstrate that multimodal data improves the AQA accuracy compared with unimodal video data.","2169-3536","","10.1109/ACCESS.2024.3423462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10584527","Action quality assessment;deep learning;multimodal dataset;multimodal learning","Videos;Transformers;Global Positioning System;Autonomous aerial vehicles;Three-dimensional displays;Sports;Quality assessment;Quality assessment;Deep learning;Multisensory integration","","1","","55","CCBYNCND","4 Jul 2024","","","IEEE","IEEE Journals"
"Action Quality Assessment with Multi-scale Temporal Attention Mechanism","W. Wang; H. Wang; Y. Hao; Q. Wang","Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; 42 Unit, 81550 Troops, The Chinese People's Liberation Army, Dalian, China",2024 7th International Conference on Advanced Algorithms and Control Engineering (ICAACE),"18 Jun 2024","2024","","","247","251","Action quality assessment is a more challenging topic than action recognition because of its requirement for models to assess quality through fine-grained differences in actions. Current mainstream approaches formalize the problem as a regression task based on video spatio-temporal features. However, most previous methods ignore that motion performance at different stages or time points may have different importance in action quality assessment. In this regard, we propose an action quality assessment method using a multi-scale temporal attention mechanism to assign appropriate weights to different time steps through the temporal attention mechanism. In addition, to address the issues of video feature fusion and subjective noise in the AQA dataset, LSTM-like MLP structures and smooth labeling strategies were applied respectively. Compared to the current state-of-the-art method, CoRe, we improved 1.84% on the AQA-7 dataset and 0.91% on the MTL-AQA dataset.","","979-8-3503-6144-5","10.1109/ICAACE61206.2024.10548995","Dalian Science and Technology Innovation Fund(grant numbers:2022JJ11CG002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548995","Action Quality Assessment;Deep Learning;Temporal attention;Video Understanding","Smoothing methods;Control engineering;Annotations;Aggregates;Noise;Quality assessment;Labeling","","","","17","IEEE","18 Jun 2024","","","IEEE","IEEE Conferences"
"Action Quality Assessment With Ignoring Scene Context","T. Nagai; S. Takeda; M. Matsumura; S. Shimizu; S. Yamamoto","NTT Media Intelligence Laboratories, NTT Corporation, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Japan",2021 IEEE International Conference on Image Processing (ICIP),"23 Aug 2021","2021","","","1189","1193","We propose an action quality assessment (AQA) method that can specifically assess target action quality with ignoring scene context, which is a feature unrelated to the target action. Existing AQA methods have tried to extract spatiotemporal features related to the target action by applying 3D convolution to the video. However, since their models are not explicitly designed to extract the features of the target action, they mis-extract scene context and thus cannot assess the target action quality correctly. To overcome this problem, we impose two losses to an existing AQA model: scene adversarial loss and our newly proposed human-masked regression loss. The scene adversarial loss encourages the model to ignore scene context by adversarial training. Our human-masked regression loss does so by making the correlation between score outputs by an AQA model and human referees undefinable when the target action is not visible. These two losses lead the model to specifically assess the target action quality with ignoring scene context. We evaluated our method on a diving dataset commonly used for AQA and found that it outperformed current state-of-the-art methods. This result shows that our method is effective in ignoring scene context while assessing the target action quality.","2381-8549","978-1-6654-4115-5","10.1109/ICIP42928.2021.9506257","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506257","Action quality assessment;Scene context;Deep learning;Spatiotemporal feature;Regression problem","Training;Correlation;Three-dimensional displays;Shape;Convolution;Predictive models;Feature extraction","","6","","27","IEEE","23 Aug 2021","","","IEEE","IEEE Conferences"
"FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment","J. Xu; Y. Rao; X. Yu; G. Chen; J. Zhou; J. Lu","Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China",2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"27 Sep 2022","2022","","","2939","2948","Most existing action quality assessment methods rely on the deep features of an entire video to predict the score, which is less reliable due to the non-transparent inference process and poor interpretability. We argue that understanding both high-level semantics and internal temporal structures of actions in competitive sports videos is the key to making predictions accurate and interpretable. Towards this goal, we construct a new fine-grained dataset, called FineDiving, developed on diverse diving events with detailed annotations on action procedures. We also propose a procedure-aware approach for action quality assessment, learned by a new Temporal Segmentation Attention module. Specifically, we propose to parse pairwise query and exemplar action instances into consecutive steps with diverse semantic and temporal correspondences. The procedure-aware cross-attention is proposed to learn embeddings between query and exemplar steps to discover their semantic, spatial, and temporal correspondences, and further serve for fine-grained contrastive regression to derive a reliable scoring mechanism. Extensive experiments demonstrate that our approach achieves substantial improvements over the state-of-the-art methods with better interpretability. The dataset and code are available at https://github.com/xujinglin/FineDiving.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00296","National Natural Science Foundation of China(grant numbers:62125603,62106124,U1813218); China Postdoctoral Science Foundation(grant numbers:2020M680564); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879654","Video analysis and understanding; Action and event recognition","Computer vision;Codes;Annotations;Semantics;Quality assessment;Pattern recognition;Reliability","","86","","50","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"LOGO: A Long-Form Video Dataset for Group Action Quality Assessment","S. Zhang; W. Dai; S. Wang; X. Shen; J. Lu; J. Zhou; Y. Tang","Shenzhen International Graduate School, Tsinghua University; Shenzhen International Graduate School, Tsinghua University; Shenzhen International Graduate School, Tsinghua University; Shenzhen International Graduate School, Tsinghua University; Department of Automation, Tsinghua University; Department of Automation, Tsinghua University; Shenzhen International Graduate School, Tsinghua University",2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"22 Aug 2023","2023","","","2405","2414","Action quality assessment (AQA) has become an emerging topic since it can be extensively applied in numerous scenarios. However, most existing methods and datasets focus on single-person short-sequence scenes, hindering the application of AQA in more complex situations. To address this issue, we construct a new multi-person long-form video dataset for action quality assessment named LOGO. Distinguished in scenario complexity, our dataset contains 200 videos from 26 artistic swimming events with 8 athletes in each sample along with an average duration of 204.2 seconds. As for richness in annotations, LOGO includes formation labels to depict group information of multiple athletes and detailed annotations on action procedures. Furthermore, we propose a simple yet effective method to model relations among athletes and reason about the potential temporal logic in long-form videos. Specifically, we design a group-aware attention module, which can be easily plugged into existing AQA methods, to enrich the clip-wise representations based on contextual group information. To benchmark LOGO, we systematically conduct investigations on the performance of several popular methods in AQA and action segmentation. The results reveal the challenges our dataset brings. Extensive experiments also show that our approach achieves state-of-the-art on the LOGO dataset. The dataset and code will be released at https://github.com/shiyi-zh0408/LOGO.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00238","National Natural Science Foundation of China(grant numbers:62206153,62125603); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204722","Video: Action and event understanding","Computer vision;Codes;Annotations;Fuses;Benchmark testing;Quality assessment;Pattern recognition","","32","","58","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Action Quality Assessment for ASD Behaviour Evaluation","D. Zhang; D. Zhou; H. Liu","School of Computing, University of Portsmouth, Portsmouth, UK; School of Computing, University of Portsmouth, Portsmouth, UK; School of Computing, University of Portsmouth, Portsmouth, UK",2023 International Conference on Machine Learning and Cybernetics (ICMLC),"28 Nov 2023","2023","","","483","488","Given the current increasing prevalence of autism, expensive and time-consuming manual diagnosis is highly detrimental to the management of the condition. With the development of computer-based methods of human behavioural analysis, these methods are expected to provide more accurate, objective and reproducible methods of early screening and diagnosis of autism. To advance the field of behavioural quantification in autism research, this study utilises human skeletal behavioural data from publicly available autism datasets and ADOS scores from clinical professionals in a first attempt to build deep neural networks that can predict ADOS scores from behavioural data using the AQA approach. This paper finds a moderately correlated between the ground truth ADOS score and the predicted ADOS score, it reveals the potential use of the AQA method in ASD diagnoses.","2160-1348","979-8-3503-0378-0","10.1109/ICMLC58545.2023.10327994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10327994","AQA;ASD;Behaviour Evaluation;CNN-LSTM;Skeleton","Autism;Manuals;Machine learning;Artificial neural networks;Quality assessment;Cybernetics","","2","","14","IEEE","28 Nov 2023","","","IEEE","IEEE Conferences"
"PE3DNet: A “Pull-Up” Action Quality Assessment Network Based on Fusion of RGB Image Data and Key Point Optical Flow","R. Yang; X. Xiu; J. Wang; R. Wang","School of Electronic and Information Engineering, Shandong University of Science and Technology, Qingdao, China; School of Electronic and Information Engineering, Shandong University of Science and Technology, Qingdao, China; School of Electronic and Information Engineering, Shandong University of Science and Technology, Qingdao, China; School of Physical Education, Shandong University of Science and Technology, Qingdao, China",2024 7th International Conference on Pattern Recognition and Artificial Intelligence (PRAI),"14 Jan 2025","2024","","","742","747","“Pull-up” is a regular physical fitness test in high school and college physical education, and the traditional manual supervised assessment method has the problems of strong subjectivity, large error and low efficiency in the assessment and training of “Pull-up”. In order to solve these problems, this paper proposes an action quality assessment network PE3DNet for “Pull-up” video streams, which uses OpenPifPaf to generate key points, constructs key points optical flow data using key points, and adopts P3D residual blocks to construct a dual-stream network that fuses RGB image data and key point optical flow to achieve the action quality assessment of “Pull-up”. Through the validation experiments on the self-made dataset SKD-PULL, the results show that PE3DNet achieves 92.8% Accuracy, 92.9% Precision, 91.3% Recall, and 92.1% F1 score in the standardized action of “Pull-up”, which effectively improves the accuracy of the action quality assessment of “Pull-up” program and brings higher efficiency and fairness to the sports testing process.","","979-8-3503-5089-0","10.1109/PRAI62207.2024.10827404","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10827404","pull-up;action quality assessment;video analytics;deep learning;dual-stream networks","Training;Accuracy;Streaming media;Quality assessment;Pattern recognition;Optical flow;Streams;Standards;Sports;Testing","","","","28","IEEE","14 Jan 2025","","","IEEE","IEEE Conferences"
"PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment","A. Dadashzadeh; S. Duan; A. Whone; M. Mirmehdi","School of Computer Science, University of Bristol, UK; School of Computer Science, University of Bristol, UK; Translational Health Sciences, University of Bristol, UK; School of Computer Science, University of Bristol, UK",2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),"9 Apr 2024","2024","","","42","52","The limited availability of labelled data in Action Quality Assessment (AQA), has forced previous works to fine-tune their models pretrained on large-scale domain-general datasets. This common approach results in weak generalisation, particularly when there is a significant domain shift. We propose a novel, parameter efficient, continual pretraining framework, PECoP, to reduce such domain shift via an additional pretraining stage. In PECoP, we introduce 3D-Adapters, inserted into the pretrained model, to learn spatiotemporal, in-domain information via self-supervised learning where only the adapter modules’ parameters are updated. We demonstrate PECoP’s ability to enhance the performance of recent state-of-the-art methods (MUSDL, CoRe, and TSA) applied to AQA, leading to considerable improvements on benchmark datasets, JIGSAWS (↑ 6.0%), MTL-AQA (↑ 0.99%), and FineDiving (↑ 2.54%). We also present a new Parkinson’s Disease dataset, PD4T, of real patients performing four various actions, where we surpass (↑ 3.56%) the state-of-the-art in comparison. Our code, pretrained models, and the PD4T dataset are available at https://github.com/Plrbear/PECoP.","2642-9381","979-8-3503-1892-0","10.1109/WACV57701.2024.00012","University of Bristol; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484297","Algorithms;Machine learning architectures;formulations;and algorithms;Algorithms;Biometrics;face;gesture;body pose;Algorithms;Video recognition and understanding","Training;Computer vision;Costs;Self-supervised learning;Benchmark testing;Data models;Quality assessment","","17","","51","IEEE","9 Apr 2024","","","IEEE","IEEE Conferences"
"Tai Chi Action Quality Assessment and Visual Analysis with a Consumer RGB-D Camera","J. Li; H. Hu; Q. Xing; X. Wang; J. Li; Y. Shen","School of Sports Engineering, Beijing Sports University, Beijing, China; School of Sports Engineering, Beijing Sports University, Beijing, China; School of Sports Engineering, Beijing Sports University, Beijing, China; School of Sports Engineering, Beijing Sports University, Beijing, China; School of Sports Engineering, Beijing Sports University, Beijing, China; School of Sports Engineering, Beijing Sports University, Beijing, China",2022 IEEE 24th International Workshop on Multimedia Signal Processing (MMSP),"22 Nov 2022","2022","","","1","6","Visual-based human action analysis is an important research topic in the field of computer vision, and has great application prospect in intelligent sports. Home-based fitness is increasingly common in recent years, however lacking of accurate feedback and scientific guidance main easily lead to problems such as exercise injuries. In this paper, we propose an analysis system for Tai Chi action quality assessment and visual analysis with a consumer RGB-D camera. The main innovative work is as follows: 1) for home-based fitness action evaluation, we design a real-time intelligent analysis system combined with expert rules through a consumer RGB-D camera; 2) we transform the evaluation of 24-form Tai Chi Chuan into an artificial intelligence (AI) model, and realize action recognition and assessment through computer vision; 2) to train the AI model, we build a new dataset named TaiChi-24, which contains 1,408 samples with RGB-D images and 3D skeletons. We carry out evaluation experiments and analyses, and the experimental results have shown the advantage of applying our evaluation method on the proposed TaiChi-24 dataset.","2473-3628","978-1-6654-7189-3","10.1109/MMSP55362.2022.9949464","National Natural Science Foundation of China(grant numbers:72071018); Open Projects Program of National Laboratory of Pattern Recognition(grant numbers:202100009); Fundamental Research Funds for Central Universities(grant numbers:2021Td006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9949464","Sports performance analysis;Action quality evaluation;Action recognition;Visual analysis","Training;Visualization;Computer vision;Analytical models;Solid modeling;Computational modeling;Transforms","","6","","26","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"A Survey of Video-based Action Quality Assessment","S. Wang; D. Yang; P. Zhai; Q. Yu; T. Suo; Z. Sun; K. Li; L. Zhang","Institute of AI & Robotics, Fudan University, Shanghai, China; Institute of AI & Robotics, Fudan University, Shanghai, China; Institute of AI & Robotics, Fudan University, Shanghai, China; ZhongShan Hospital, Shanghai, China; ZhongShan Hospital, Shanghai, China; ZhongShan Hospital, Shanghai, China; ZhongShan Hospital, Shanghai, China; Institute of AI & Robotics, Fudan University, Shanghai, China",2021 International Conference on Networking Systems of AI (INSAI),"19 Apr 2022","2021","","","1","9","Human action recognition and analysis have great demand and important application significance in video surveillance, video retrieval, and human-computer interaction. The task of human action quality evaluation requires the intelligent system to automatically and objectively evaluate the action completed by the human. The action quality assessment model can reduce the human and material resources spent in action evaluation and reduce subjectivity. In this paper, we provide a comprehensive survey of existing papers on video-based action quality assessment. Different from human action recognition, the application scenario of action quality assessment is relatively narrow. Most of the existing work focuses on sports and medical care. We first introduce the definition and challenges of human action quality assessment. Then we present the existing datasets and evaluation metrics. In addition, we summarized the methods of sports and medical care according to the model categories and publishing institutions according to the characteristics of the two fields. At the end, combined with recent work, the promising development direction in action quality assessment is discussed.","","978-1-6654-0859-2","10.1109/INSAI54028.2021.00029","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9757944","action quality assessment;human behavior analysis;deep learning;computer vision","Training;Measurement;Human computer interaction;Publishing;Medical services;Video surveillance;Quality assessment","","17","","61","IEEE","19 Apr 2022","","","IEEE","IEEE Conferences"
"FineParser: A Fine-Grained Spatio-Temporal Action Parser for Human-Centric Action Quality Assessment","J. Xu; S. Yin; G. Zhao; Z. Wang; Y. Peng","School of Intelligence Science and Technology, University of Science and Technology Beijing; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","14628","14637","Existing action quality assessment (AQA) methods mainly learn deep representations at the video level for scoring diverse actions. Due to the lack of a fine-grained understanding of actions in videos, they harshly suffer from low credibility and interpretability, thus insufficient for stringent applications, such as Olympic diving events. We argue that a fine-grained understanding of actions requires the model to perceive and parse actions in both time and space, which is also the key to the credibility and inter-pretability of the AQA technique. Based on this insight, we propose a new fine-grained spatial-temporal action parser named FineParser. It learns human-centric foreground action representations by focusing on target action regions within each frame and exploiting their fine-grained alignments in time and space to minimize the impact of in-valid backgrounds during the assessment. In addition, we construct fine-grained annotations of human-centric fore-ground action masks for the FineDiving dataset, called FineDiving-HM. With refined annotations on diverse target action procedures, FineDiving-HM can promote the development of real-world AQA systems. Through extensive experiments, we demonstrate the effectiveness of FineParser, which outperforms state-of-the-art methods while supporting more tasks of fine-grained action understanding. Data and code are available at https://github.com/PKU-ICST-MIPL/FineParser_CVPR2024.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.01386","National Natural Science Foundation of China(grant numbers:61925201,62132001,62373043); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10658132","Action Quality Assessment;Fine-grained Action Understanding;Human-centric Foreground Action Masks","Computer vision;Codes;Annotations;Focusing;Quality assessment;Pattern recognition","","17","","42","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"FineRehab: A Multi-modality and Multi-task Dataset for Rehabilitation Analysis","J. Li; J. Xue; R. Cao; X. Du; S. Mo; K. Ran; Z. Zhang","School of Sports Engineering, Beijing Sport University, China; School of Sports Engineering, Beijing Sport University, China; School of Sports Engineering, Beijing Sport University, China; Department of Neurorehabilitation, Rehabilitation Research Center, China; School of Sports Engineering, Beijing Sport University, China; School of Sports Engineering, Beijing Sport University, China; Department of Neurorehabilitation, Rehabilitation Research Center, China",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),"27 Sep 2024","2024","","","3184","3193","The assessment of rehabilitation exercises for neurological and musculoskeletal disorders are crucial for recovery. Traditionally, assessment methods have been subjective, with inherent uncertainty and limitations. This paper introduces a novel multi-modality dataset named FineRehab§ to prompt the study of rehabilitation movement analysis, leveraging advancements in sensor technology and artificial intelligence. FineRehab collects 16 actions from 50 participants, including both patients with musculoskeletal disorders and healthy individuals, and consists of 4,215 action samples captured by two Kinect cameras and 17 IMUs. To benchmark FineRehab, we present a reliable approach to analyze rehabilitation exercises, and make experiments to evaluate the comprehensive movement quality from across multi-dimensions. Comparative experimental analyses have verified the validity of our dataset in distinguishing between the movement of the normal population and patients, which can offer a quantifiable basis for personalized rehabilitation feedback. The introduction of FineRehab will encourage researchers to apply, develop and adapt various methods for rehabilitation exercise analysis.","2160-7516","979-8-3503-6547-4","10.1109/CVPRW63382.2024.00324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10678182","Action recognition;Action quality assessment;Rehabilitation exercise;Dataset","Deep learning;Musculoskeletal system;Computer vision;Uncertainty;Conferences;Multitasking;Cameras","","8","","32","IEEE","27 Sep 2024","","","IEEE","IEEE Conferences"
"Uncertainty-Aware Score Distribution Learning for Action Quality Assessment","Y. Tang; Z. Ni; J. Zhou; D. Zhang; J. Lu; Y. Wu; J. Zhou","Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Electrical and Computer Engineering Department, Northwestern University; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Electrical and Computer Engineering Department, Northwestern University; Department of Automation, Tsinghua University, China",2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"5 Aug 2020","2020","","","9836","9845","Assessing action quality from videos has attracted growing attention in recent years. Most existing approaches usually tackle this problem based on regression algorithms, which ignore the intrinsic ambiguity in the score labels caused by multiple judges or their subjective appraisals. To address this issue, we propose an uncertainty-aware score distribution learning (USDL) approach for action quality assessment (AQA). Specifically, we regard an action as an instance associated with a score distribution, which describes the probability of different evaluated scores. Moreover, under the circumstance where finer-grained score labels are available (e.g., difficulty degree of an action or multiple scores from different judges), we further devise a multi-path uncertainty-aware score distribution learning (MUSDL) method to explore the disentangled components of a score. In order to demonstrate the effectiveness of our proposed methods, We conduct experiments on two AQA datasets containing various Olympic actions. Our approaches set new state-of-the-arts under the Spearman's Rank Correlation (i.e., 0.8102 on AQA-7 and 0.9273 on MTL-AQA).","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.00986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9157684","","Videos;Uncertainty;Games;Gaussian distribution;Task analysis;Quality assessment;Training","","121","","44","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"Group-aware Contrastive Regression for Action Quality Assessment","X. Yu; Y. Rao; W. Zhao; J. Lu; J. Zhou","Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China",2021 IEEE/CVF International Conference on Computer Vision (ICCV),"28 Feb 2022","2021","","","7899","7908","Assessing action quality is challenging due to the subtle differences between videos and large variations in scores. Most existing approaches tackle this problem by regressing a quality score from a single video, suffering a lot from the large inter-video score variations. In this paper, we show that the relations among videos can provide important clues for more accurate action quality assessment during both training and inference. Specifically, we reformulate the problem of action quality assessment as regressing the relative scores with reference to another video that has shared attributes (e.g., category and difficulty), instead of learning unreferenced scores. Following this formulation, we propose a new Contrastive Regression (CoRe) framework to learn the relative scores by pair-wise comparison, which highlights the differences between videos and guides the models to learn the key hints for assessment. In order to further exploit the relative information between two videos, we devise a group-aware regression tree to convert the conventional score regression into two easier sub-problems: coarse-to-fine classification and regression in small intervals. To demonstrate the effectiveness of CoRe, we conduct extensive experiments on three mainstream AQA datasets including AQA-7, MTL-AQA and JIGSAWS. Our approach outperforms previous methods by a large margin and establishes new state-of-the-art on all three benchmarks.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00782","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711028","Video analysis and understanding","Training;Computer vision;Benchmark testing;Quality assessment;Task analysis;Regression tree analysis;Videos","","86","","42","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"ResFNN: Residual Structure-Based Feedforward Neural Network for Action Quality Assessment in Sports Consumer Electronics","H. Gao; S. Yu; M. Iqbal; M. Guizani","School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer Engineering and Science, Shanghai University, Shanghai, China; Smart System Lab, College of Engineering, Prince Sultan University, Riyadh, Saudi Arabia; Machine Learning Department, Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE",IEEE Transactions on Consumer Electronics,"3 Jan 2025","2024","70","4","6653","6663","With the development of artificial intelligence (AI) and sports consumer electronics, AI-empowered Olympic sport technologies are being implemented more extensively. Action quality assessment (AQA), a sport action recognition and video refereeing technology, aims to automatically score action performance in videos obtained from sports consumer electronics deployed in arenas. It has gained much attention for its wide range of applications, such as sports event scoring, specific skill assessment, and rehabilitation medicine. General methods score action performance by directly regressing the initial video features to score, which neglects the possibility that the initial features are insufficiently effective. To address this issue, we propose a residual structure-based feedforward neural network (ResFNN) that enables efficient action feature learning to attain improved score assessment performance. First, the input videos are downsampled to clips and passed through inflated 3D convolutional networks (ConvNets) to obtain initial action video features. These features contain spatiotemporal information about the human actions occurring in the videos. Second, these features are aggregated and learned through our ResFNN. The ResFNN is composed of feedforward neural network residual blocks, which have strong function fitting and feature conversion capabilities. Therefore, the network learns features well and obtains more effective features. Third, a score distribution regression method is applied to obtain the underlying score distribution. This step establishes a more accurate mapping between the videos and scores. Finally, our method is demonstrated to outperform the majority of the existing methods through experiments conducted on the AQA-7, MTL-AQA, and JIGSAWS datasets.","1558-4127","","10.1109/TCE.2024.3482560","National Natural Science Foundation of China(grant numbers:92367103); National Key Research and Development Program of China(grant numbers:2022YFF0902500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10720818","Action quality assessment;ResFNN;uncertainty learning;distribution regression;sports consumer electronics","Feature extraction;Sports;Feedforward neural networks;Consumer electronics;Artificial intelligence;Uncertainty;Quality assessment;Training;Degradation;Spatiotemporal phenomena","","6","","40","IEEE","17 Oct 2024","","","IEEE","IEEE Journals"
"Action Quality Assessment Across Multiple Actions","P. Parmar; B. Morris","University of Nevada, Las Vegas; University of Nevada, Las Vegas",2019 IEEE Winter Conference on Applications of Computer Vision (WACV),"7 Mar 2019","2019","","","1468","1476","Can learning to measure the quality of an action help in measuring the quality of other actions? If so, can consolidated samples from multiple actions help improve the performance of current approaches? In this paper, we carry out experiments to see if knowledge transfer is possible in the action quality assessment (AQA) setting. Experiments are carried out on our newly released AQA dataset (http://rtis.oit.unlv.edu/datasets.html) consisting of 1106 action samples from seven actions with quality as measured by expert human judges. Our experimental results show that there is utility in learning a single model across multiple actions.","1550-5790","978-1-7281-1975-5","10.1109/WACV.2019.00161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658709","","Sports;Synchronization;Current measurement;Task analysis;Quality assessment;Cameras;Training","","110","","28","IEEE","7 Mar 2019","","","IEEE","IEEE Conferences"
"What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment","P. Parmar; B. T. Morris","University of Nevada, Las Vegas; University of Nevada, Las Vegas",2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"9 Jan 2020","2019","","","304","313","Can performance on the task of action quality assessment (AQA) be improved by exploiting a description of the action and its quality? Current AQA and skills assessment approaches propose to learn features that serve only one task - estimating the final score. In this paper, we propose to learn spatio-temporal features that explain three related tasks - fine-grained action recognition, commentary generation, and estimating the AQA score. A new multitask-AQA dataset, the largest to date, comprising of 1412 diving samples was collected to evaluate our approach (http://rtis.oit.unlv.edu/datasets.html). We show that our MTL approach outperforms STL approach using two different kinds of architectures: C3D-AVG and MSCADC. The C3D-AVG-MTL approach achieves the new state-of-the-art performance with a rank correlation of 90.44%. Detailed experiments were performed to show that MTL offers better generalization than STL, and representations from action recognition models are not sufficient for the AQA task and instead should be learned.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954464","Face;Gesture;and Body Pose;Action Recognition;Datasets and Evaluation;Video Analytics","Computer vision;Correlation;Computational modeling;Computer architecture;Quality assessment;Spatiotemporal phenomena;Pattern recognition","","147","","33","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Piano Skills Assessment","P. Parmar; J. Reddy; B. Morris","Dept. of Computer Science, University of British Columbia, Canada; Dept. of Computer Science, Stanford University, USA; Dept. of Electrical Engineering, University of Nevada, Las Vegas, USA",2021 IEEE 23rd International Workshop on Multimedia Signal Processing (MMSP),"16 Mar 2022","2021","","","1","5","Can a computer determine a piano player’s skill level? Is it preferable to base this assessment on visual analysis of the player’s performance or should we trust our ears over our eyes? Since current convolutional neural networks (CNNs) have difficulty processing long video videos, how can shorter clips be sampled to best reflect the players skill level? In this work, we collect and release a first-of-its-kind dataset for multimodal skill assessment focusing on assessing piano player’s skill level, answer the asked questions, initiate work in automated evaluation of piano playing skills and provide baselines for future work. Dataset can be accessed from: https://github.com/ParitoshParmar/Piano-Skills-Assessment.","2473-3628","978-1-6654-3288-7","10.1109/MMSP53017.2021.9733638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733638","Skills Assessment;Automated Piano Skills Assessment;Action Quality Assessment;Video Understanding;Audio Classification;Video Classification;Computer Vision","Visualization;Computer vision;Conferences;Computational modeling;Focusing;Ear;Signal processing","","26","","31","IEEE","16 Mar 2022","","","IEEE","IEEE Conferences"
"A Contrastive Learning Network for Performance Metric and Assessment of Physical Rehabilitation Exercises","L. Yao; Q. Lei; H. Zhang; J. Du; S. Gao","Xiamen Key Laboratory of Computer Vision and Pattern Recognition and the Department of Computer Science and Technology, Huaqiao University, Xiamen, China; Xiamen Key Laboratory of Computer Vision and Pattern Recognition and the Department of Computer Science and Technology, Huaqiao University, Xiamen, China; Fujian Key Laboratory of Big Data Intelligence and Security, Huaqiao University, Xiamen, China; Fujian Key Laboratory of Big Data Intelligence and Security, Huaqiao University, Xiamen, China; Faculty of Engineering, University of Toyama, Toyama-shi, Japan",IEEE Transactions on Neural Systems and Rehabilitation Engineering,"28 Sep 2023","2023","31","","3790","3802","Human activity analysis in the legal monitoring environment plays an important role in the physical rehabilitation field, as it helps patients with physical injuries improve their postoperative conditions and reduce their medical costs. Recently, several deep learning-based action quality assessment (AQA) frameworks have been proposed to evaluate physical rehabilitation exercises. However, most of them treat this problem as a simple regression task, which requires both the action instance and its score label as input. This approach is limited by the fact that the annotations in this field usually consist of healthy or unhealthy labels rather than quality scores provided by professional physicians. Additionally, most of these methods cannot provide informative feedback on a patient’s motion defects, which weakens their practical application. To address these problems, we propose a multi-task contrastive learning framework to learn subtle and critical differences from skeleton sequences to deal with the performance metric and AQA problems of physical rehabilitation exercises. Specifically, we propose a performance metric network that takes triplets of training samples as input for score generation. For the AQA task, the same contrast learning strategy is used, but pairwise training samples are fed into the action quality assessment network for score prediction. Notably, we propose quantifying the deviation of the joint attention matrix between different skeleton sequences and introducing it into the loss function of our learning network. It is proven that considering both score prediction loss and joint attention deviation loss improves physical exercises AQA performance. Furthermore, it helps to obtain informative feedback for patients to improve their motion defects by visualizing the joint attention matrix’s difference. The proposed method is verified on the UI-PRMD and KIMORE datasets. Experimental results show that the proposed method achieves state-of-the-art performance.","1558-0210","","10.1109/TNSRE.2023.3317411","National Natural Science Foundation of China(grant numbers:62001176,61871196); Natural Science Foundation of Fujian Province, China(grant numbers:2020J01085,2019J01082); National Key Research and Development Program of China(grant numbers:2019YFC1604700); Promotion Program for Young and Middle-Aged Teacher in Science and Technology Research of Huaqiao University(grant numbers:ZQN-YX601); Japan Society for the Promotion of Science (JSPS) KAKENHI(grant numbers:JP22H03643); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256137","Physical rehabilitation exercise;action quality assessment;performance metric quantification;contrastive learning;informative feedback","Feature extraction;Skeleton;Training;Measurement;Task analysis;Quality assessment;Sports","Humans;Exercise Therapy;Exercise;Motion","13","","58","CCBY","20 Sep 2023","","","IEEE","IEEE Journals"
"AI Trainer: Autoencoder Based Approach for Squat Analysis and Correction","M. Chariar; S. Rao; A. Irani; S. Suresh; C. S. Asha","Department of Mechatronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, India; Department of Mechatronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, India; Department of Mechatronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, India; Department of Mechatronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, India; Department of Mechatronics, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, India",IEEE Access,"6 Oct 2023","2023","11","","107135","107149","Artificial intelligence and computer vision have widespread applications in workout analysis. It has been extensively used in sports and the athlete industry to identify errors and improve performance. Furthermore, these methods prevent injuries caused by a lack of instructors or costly infrastructure. One such exercise is the squat, which is a movement in which a standing person descends to a posture with their torso vertical and their knees firmly bent, then returns to their original upright position. Each person’s squat is distinct, with varying limb lengths causing their form to change when observed. It has been observed that the mobility of various joints and muscular strength have a role in this. A squat improves the user by increasing overall leg strength, strengthening knee and hip joints, and lowering the risk of heart disease due to cardiovascular development. This paper presents a method for classifying squat types and recommending the right squat version. This study uses MediaPipe and a deep learning-based technique to decide if squatting is good or bad. A stacked Bidirectional Gated Recurrent Unit (Bi-GRU) model with an attention layer is proposed to consistently and fairly assess each user, categorizing squats into seven classes. This stacked Bi-GRU model with an attention unit is then compared to other cutting-edge models, both with and without the attention layer. The model outperforms other models by attaining an accuracy of 94% and is demonstrated to work the best and most consistently for our dataset. Furthermore, the individual executing the incorrect squat is corrected to the best of their ability, depending on their performance and body proportions, by providing the correct form.","2169-3536","","10.1109/ACCESS.2023.3316009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254650","Action quality assessment;attention;computer vision;curve fitting;gated recurrent unit;pose estimation;squat","Pose estimation;Long short term memory;Hidden Markov models;Three-dimensional displays;Videos;Human activity recognition;Artificial intelligence","","18","","73","CCBYNCND","18 Sep 2023","","","IEEE","IEEE Journals"
"A Video-Based Augmented Reality System for Human-in-the-Loop Muscle Strength Assessment of Juvenile Dermatomyositis","K. Zhou; R. Cai; Y. Ma; Q. Tan; X. Wang; J. Li; H. P. H. Shum; F. W. B. Li; S. Jin; X. Liang","Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Children's Hospital of Capital Institute of Pediatrics, Beijing, China; Children's Hospital of Capital Institute of Pediatrics, Beijing, China; Children's Hospital of Capital Institute of Pediatrics, Beijing, China; Durham University, Durham, United Kingdom; Durham University, Durham, United Kingdom; Beijing Diannaite Medical Technology Co., Ltd., China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China",IEEE Transactions on Visualization and Computer Graphics,"29 Mar 2023","2023","29","5","2456","2466","As the most common idiopathic inflammatory myopathy in children, juvenile dermatomyositis (JDM) is characterized by skin rashes and muscle weakness. The childhood myositis assessment scale (CMAS) is commonly used to measure the degree of muscle involvement for diagnosis or rehabilitation monitoring. On the one hand, human diagnosis is not scalable and may be subject to personal bias. On the other hand, automatic action quality assessment (AQA) algorithms cannot guarantee 100% accuracy, making them not suitable for biomedical applications. As a solution, we propose a video-based augmented reality system for human-in-the-loop muscle strength assessment of children with JDM. We first propose an AQA algorithm for muscle strength assessment of JDM using contrastive regression trained by a JDM dataset. Our core insight is to visualize the AQA results as a virtual character facilitated by a 3D animation dataset, so that users can compare the real-world patient and the virtual character to understand and verify the AQA results. To allow effective comparisons, we propose a video-based augmented reality system. Given a feed, we adapt computer vision algorithms for scene understanding, evaluate the optimal way of augmenting the virtual character into the scene, and highlight important parts for effective human verification. The experimental results confirm the effectiveness of our AQA algorithm, and the results of the user study demonstrate that humans can more accurately and quickly assess the muscle strength of children using our system.","1941-0506","","10.1109/TVCG.2023.3247092","National Natural Science Foundation of China(grant numbers:6227201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049714","Action Quality Assessment;Augmented Reality;Human-in-the-Loop System;Juvenile Dermatomyositis","Muscles;Medical services;Animation;Visualization;Pediatrics;Medical diagnostic imaging;Human in the loop","","17","","50","IEEE","22 Feb 2023","","","IEEE","IEEE Journals"
"Narrative Action Evaluation with Prompt-Guided Multimodal Interaction","S. Zhang; S. Bai; G. Chen; L. Chen; J. Lu; J. Wang; Y. Tang","Shenzhen International Graduate School, Tsinghua University; Shenzhen International Graduate School, Tsinghua University; Carnegie Mellon University, Pittsburgh, PA, USA; Department of Automation, Tsinghua University; Department of Automation, Tsinghua University; Tencent; Carnegie Mellon University, Pittsburgh, PA, USA",2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"16 Sep 2024","2024","","","18430","18439","In this paper, we investigate a new problem called narrative action evaluation (NAE). NAE aims to generate professional commentary that evaluates the execution of an action. Unlike traditional tasks such as score-based action qual-ity assessment and video captioning involving superficial sentences, NAE focuses on creating detailed narratives in natural language. These narratives provide intricate descriptions of actions along with objective evaluations. NAE is a more challenging task because it requires both narrative flex-ibility and evaluation rigor. One existing possible solution is to use multi-task learning, where narrative language and evaluative information are predicted separately. However, this approach results in reduced performance for individual tasks because of variations between tasks and differences in modality between language information and evaluation information. To address this, we propose a prompt-guided multimodal interaction framework. This framework utilizes a pair of transformers to facilitate the interaction between different modalities of information. It also uses prompts to transform the score regression task into a video-text matching task, thus enabling task interactivity. To support further research in this field, we re-annotate the MTL-AQA and FineGym datasets with high-quality and comprehensive action narration. Additionally, we establish benchmarks for NAE. Extensive experiment results prove that our method outperforms separate learning methods and naive multi-task learning methods. Data and code are released at here.","2575-7075","979-8-3503-5300-6","10.1109/CVPR52733.2024.01744","National Natural Science Foundation of China(grant numbers:62125603,62321005,62336004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10655159","","Learning systems;Computer vision;Codes;Computational modeling;Natural languages;Transforms;Benchmark testing","","4","","45","IEEE","16 Sep 2024","","","IEEE","IEEE Conferences"
"Likert Scoring with Grade Decoupling for Long-term Action Assessment","A. Xu; L. -A. Zeng; W. -S. Zheng","School of Computer Science and Engineering, Sun Yat-sen University, China; School of Artificial Intelligence, Sun Yat-sen University, China; School of Computer Science and Engineering, Sun Yat-sen University, China",2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"27 Sep 2022","2022","","","3222","3231","Long-term action quality assessment is a task of evaluating how well an action is performed, namely, estimating a quality score from a long video. Intuitively, long-term actions generally involve parts exhibiting different levels of skill, and we call the levels of skill as performance grades. For example, technical highlights and faults may appear in the same long-term action. Hence, the final score should be determined by the comprehensive effect of different grades exhibited in the video. To explore this latent relationship, we design a novel Likert scoring paradigm in-spired by the Likert scale in psychometrics, in which we quantify the grades explicitly and generate the final quality score by combining the quantitative values and the corresponding responses estimated from the video, instead of performing direct regression. Moreover, we extract grade-specific features, which will be used to estimate the responses of each grade, through a Transformer decoder architecture with diverse learnable queries. The whole model is named as Grade-decoupling Likert Transformer (GDLT), and we achieve state-of-the-art results on two long-term action assessment datasets.11Project page https://isee-ai.cn/-angchi/CVPR22_GDLT.html","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880394","Video analysis and understanding","Computer vision;Computational modeling;Estimation;Computer architecture;Transformers;Feature extraction;Decoding","","29","","48","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"A Multi-dimensional Aesthetic Quality Assessment Model for Mobile Game Images","T. Wang; W. Sun; X. Min; W. Lu; Z. Zhang; G. Zhai","Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China",2021 International Conference on Visual Communications and Image Processing (VCIP),"19 Jan 2022","2021","","","1","5","With the development of the game industry and the popularization of mobile devices, mobile games have played an important role in people's entertainment life. The aesthetic quality of mobile game images determines the users' Quality of Experience (QoE) to a certain extent. In this paper, we propose a multi-task deep learning based method to evaluate the aesthetic quality of mobile game images in multiple dimensions (i.e. the fineness, color harmony, colorfulness, and overall quality). Specifically, we first extract the quality-aware feature representation through integrating the features from all intermediate layers of the convolution neural network (CNN) and then map these quality-aware features into the quality score space in each dimension via the quality regressor module, which consists of three fully connected (FC) layers. The proposed model is trained through a multi-task learning manner, where the quality-aware features are shared by different quality dimension prediction tasks, and the multi-dimensional quality scores of each image are regressed by multiple quality regression modules respectively. We further introduce an uncertainty principle to balance the loss of each task in the training stage. The experimental results show that our proposed model achieves the best performance on the Multi-dimensional Aesthetic assessment for Mobile Game image database (MAMG) among state-of-the-art image quality assessment (IQA) algorithms and aesthetic quality assessment (AQA) algorithms.","2642-9357","978-1-7281-8551-4","10.1109/VCIP53242.2021.9675430","National Natural Science Foundation of China(grant numbers:61901260,61831015,61771305,U1908210,62101326); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9675430","mobile game image;aesthetic quality assessment;multi-task learning;deep learning","Deep learning;Uncertainty;Games;Feature extraction;Multitasking;Quality assessment;Quality of experience","","15","","25","IEEE","19 Jan 2022","","","IEEE","IEEE Conferences"
"Learning to Score Olympic Events","P. Parmar; B. T. Morris","University of Nevada, Las Vegas; University of Nevada, Las Vegas",2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),"24 Aug 2017","2017","","","76","84","Estimating action quality, the process of assigning a ""score"" to the execution of an action, is crucial in areas such as sports and health care. Unlike action recognition, which has millions of examples to learn from, the action quality datasets that are currently available are small-typically comprised of only a few hundred samples. This work presents three frameworks for evaluating Olympic sports which utilize spatiotemporal features learned using 3D convolutional neural networks (C3D) and perform score regression with i) SVR ii) LSTM and iii) LSTM followed by SVR. An efficient training mechanism for the limited data scenarios is presented for clip-based training with LSTM. The proposed systems show significant improvement over existing quality assessment approaches on the task of predicting scores of diving, vault, figure skating. SVR-based frameworks yield better results, LSTM-based frameworks are more natural for describing an action and can be used for improvement feedback.","2160-7516","978-1-5386-0733-6","10.1109/CVPRW.2017.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014750","","Training;Quality assessment;Feature extraction;Visualization;Spatiotemporal phenomena;Three-dimensional displays;Support vector machines","","178","","17","IEEE","24 Aug 2017","","","IEEE","IEEE Conferences"
"Learning Semantics-Guided Representations for Scoring Figure Skating","Z. Du; D. He; X. Wang; Q. Wang","School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China",IEEE Transactions on Multimedia,"25 Mar 2024","2024","26","","4987","4997","This paper explores semantic-aware representations for scoring figure skating videos. Most existing approaches to sports video analysis only focus on reasoning action scores based on visual input, limiting their ability to depict high-level semantic representations. Here, we propose a teacher-student-based network with an attention mechanism to realize an adaptive knowledge transfer from the semantic domain to the visual domain, which is termed semantics-guided network (SGN). Specifically, we use a set of learnable atomic queries in the student branch to mimic the semantic-aware distribution in the teacher branch, which is represented by the visual and semantic inputs. In addition, we propose three auxiliary losses to align features in different domains. With aligned feature representations, the adapted teacher is capable of transferring the semantic knowledge to the student. To verify the effectiveness of our method, we collect a new dataset OlympicFS for scoring figure skating. Besides action scores, OlympicFS also provides professional comments on actions for learning semantic representations. By evaluating four challenging datasets, our method achieves state-of-the-art performance.","1941-0077","","10.1109/TMM.2023.3328180","National Natural Science Foundation of China(grant numbers:62031023,61801396); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10301591","Figure skating videos;sports video analysis;multi-modality representation learning;teacher-student network;action quality assessment","Videos;Semantics;Visualization;Sports;Training;Feature extraction;Games","","12","","52","IEEE","30 Oct 2023","","","IEEE","IEEE Journals"
