"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Affiliations","Authors with affiliations","Abstract","Author Keywords","Index Keywords","Molecular Sequence Numbers","Chemicals/CAS","Funding Texts","References","Editors","Publisher","Sponsors","Conference name","Conference date","Conference location","Conference code","ISSN","ISBN","CODEN","PubMed ID","Language of Original Document","Document Type","Publication Stage","Open Access","Source","EID"
"A., Dadashzadeh, Amirhossein; S., Duan, Shuchao; A.L., Whone, Alan L.; M., Mirmehdi, Majid","Dadashzadeh, Amirhossein (57213067843); Duan, Shuchao (58722692300); Whone, Alan L. (6602648827); Mirmehdi, Majid (7004105162)","57213067843; 58722692300; 6602648827; 7004105162","PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment","2024","","","","","42","52","0","15","10.1109/WACV57701.2024.00012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191968585&doi=10.1109%2FWACV57701.2024.00012&partnerID=40&md5=9638c3c064348a70f57b59d9de50ac16","University of Bristol, Bristol, United Kingdom; University of Bristol, Bristol, United Kingdom","Dadashzadeh, Amirhossein, School of Computer Science, University of Bristol, Bristol, United Kingdom; Duan, Shuchao, School of Computer Science, University of Bristol, Bristol, United Kingdom; Whone, Alan L., Translational Health Sciences, University of Bristol, Bristol, United Kingdom; Mirmehdi, Majid, School of Computer Science, University of Bristol, Bristol, United Kingdom","The limited availability of labelled data in Action Quality Assessment (AQA), has forced previous works to fine-tune their models pretrained on large-scale domain-general datasets. This common approach results in weak generalisation, particularly when there is a significant domain shift. We propose a novel, parameter efficient, continual pretraining framework, PECoP, to reduce such domain shift via an additional pretraining stage. In PECoP, we introduce 3D-Adapters, inserted into the pretrained model, to learn spatiotemporal, in-domain information via self-supervised learning where only the adapter modules' parameters are updated. We demonstrate PECoP's ability to enhance the performance of recent state-of-the-art methods (MUSDL, CoRe, and TSA) applied to AQA, leading to considerable improvements on benchmark datasets, JIGSAWS (↑ 6.0%), MTL-AQA (↑ 0.99%), and FineDiving (↑ 2.54%). We also present a new Parkinson's Disease dataset, PD4T, of real patients performing four various actions, where we surpass (↑ 3.56%) the state-of-the-art in comparison. Our code, pretrained models, and the PD4T dataset are available at https://github.com/Plrbear/PECoP. © 2024 Elsevier B.V., All rights reserved.","Algorithms; Algorithms; Algorithms; And Algorithms; Biometrics; Body Pose; Face; Formulations; Gesture; Machine Learning Architectures; Video Recognition And Understanding; Benchmarking; Computer Vision; Large Datasets; And Algorithm; Body Pose; Face; Formulation; Gesture; Learning Architectures; Machine Learning Architecture; Machine-learning; Video Recognition; Video Understanding; Machine Learning","Benchmarking; Computer vision; Large datasets; And algorithm; Body pose; Face; Formulation; Gesture; Learning architectures; Machine learning architecture; Machine-learning; Video recognition; Video understanding; Machine learning","","","","Robust and Efficient Medical Imaging with Self Supervision, (2022); Benaim, Sagie, SpeedNet: Learning the Speediness in Videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 9919-9928, (2020); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Conv Adapter Exploring Parameter Efficient Transfer Learning for Convnets, (2022); Chen, Peihao, RSPNet: Relative Speed Perception for Unsupervised Video Representation Learning, 2A, pp. 1045-1053, (2021); Chen, Shoufa, AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition, Advances in Neural Information Processing Systems, 35, (2022); Chen, Ting, A simple framework for contrastive learning of visual representations, PartF168147-3, pp. 1575-1585, (2020); Dadashzadeh, Amirhossein, Auxiliary Learning for Self-Supervised Video Representation via Similarity-based Knowledge Distillation, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2022-June, pp. 4230-4239, (2022); Dadashzadeh, Amirhossein, Exploring Motion Boundaries in an End-to-End Network for Vision-based Parkinson’s Severity Assessment, International Conference on Pattern Recognition Applications and Methods, 1, pp. 89-97, (2021); Ding, Ning, Parameter-efficient fine-tuning of large-scale pre-trained language models, Nature Machine Intelligence, 5, 3, pp. 220-235, (2023)","","Institute of Electrical and Electronics Engineers Inc.","CVF; IEEE Computer Society","2024 IEEE Winter Conference on Applications of Computer Vision, WACV 2024","","Waikoloa; HI","198735","","9798350318920","","","English","Conference paper","Final","","Scopus","2-s2.0-85191968585"
"R., Yang, Rui; X., Xiu, Xiaona; J., Wang, Jian; R., Wang, Ruyao","Yang, Rui (58591741000); Xiu, Xiaona (59011186000); Wang, Jian (59863130600); Wang, Ruyao (59548087000)","58591741000; 59011186000; 59863130600; 59548087000","PE3DNet: A 'Pull-Up' Action Quality Assessment Network Based on Fusion of RGB Image Data and Key Point Optical Flow","2024","","","","","742","747","0","0","10.1109/PRAI62207.2024.10827404","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217223529&doi=10.1109%2FPRAI62207.2024.10827404&partnerID=40&md5=fdc518ac3e41ce91e27db3e5ef879ae5","Shandong University of Science and Technology, Qingdao, China; Shandong University of Science and Technology, Qingdao, China","Yang, Rui, School of Electronic and Information Engineering, Shandong University of Science and Technology, Qingdao, China; Xiu, Xiaona, School of Electronic and Information Engineering, Shandong University of Science and Technology, Qingdao, China; Wang, Jian, School of Electronic and Information Engineering, Shandong University of Science and Technology, Qingdao, China; Wang, Ruyao, School of Physics, Shandong University of Science and Technology, Qingdao, China","'Pull-up' is a regular physical fitness test in high school and college physical education, and the traditional manual supervised assessment method has the problems of strong subjectivity, large error and low efficiency in the assessment and training of 'Pull-up'. In order to solve these problems, this paper proposes an action quality assessment network PE3DNet for 'Pull-up' video streams, which uses OpenPifPaf to generate key points, constructs key points optical flow data using key points, and adopts P3D residual blocks to construct a dual-stream network that fuses RGB image data and key point optical flow to achieve the action quality assessment of 'Pull-up'. Through the validation experiments on the self-made dataset SKD-PULL, the results show that PE3DNet achieves 92.8% Accuracy, 92.9% Precision, 91.3% Recall, and 92.1% F1 score in the standardized action of 'Pull-up', which effectively improves the accuracy of the action quality assessment of 'Pull-up' program and brings higher efficiency and fairness to the sports testing process. © 2025 Elsevier B.V., All rights reserved.","Action Quality Assessment; Deep Learning; Dual-stream Networks; Pull-up; Video Analytics; Deep Learning; Electronic Assessment; Image Fusion; Optical Data Processing; Video Streaming; Action Quality Assessment; Dual-stream Network; Keypoints; Optical-; Pull Up; Quality Assessment; Rgb Images; Stream Networks; Video Analytics; Optical Flows","Deep learning; Electronic assessment; Image fusion; Optical data processing; Video streaming; Action quality assessment; Dual-stream network; Keypoints; Optical-; Pull up; Quality assessment; RGB images; Stream networks; Video analytics; Optical flows","","","","Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, (2022); Kreiss, Sven, OpenPifPaf: Composite Fields for Semantic Keypoint Detection and Spatio-Temporal Association, IEEE Transactions on Intelligent Transportation Systems, 23, 8, pp. 13498-13511, (2022); Qiu, Zhaofan, Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 5534-5542, (2017); Research on Auxiliary Diagnosis of Developmental Coordination Disorder Based on Skeletal Sequence D, (2023); Hu, Mingxuan, Segmentation and Evaluation of Continuous Rehabilitation Exercises, Shanghai Jiaotong Daxue Xuebao/Journal of Shanghai Jiaotong University, 57, 5, pp. 533-544, (2023); Sardari, Faegheh, Vi-Net—view-invariant quality of human movement assessment, Sensors, 20, 18, pp. 1-15, (2020); Research on the Quality Assessment of Taijiquan Movements Based on Spatio Temporal Map Convolution D, (2023); Journal of Hunan University of Science and Technology Natural Science Edition, (2022); Artificial Intelligence, (2022); Hubei Sports Science and Technology, (2019)","","Institute of Electrical and Electronics Engineers Inc.","CASC; et al.; Hangzhou Dianzi University; IEEE; Nanjing University of Posts and Telecommunications; Shenzhen University","7th International Conference on Pattern Recognition and Artificial Intelligence, PRAI 2024","","Hangzhou","206064","","9798350350890","","","English","Conference paper","Final","","Scopus","2-s2.0-85217223529"
"W., Wang, Wei; H., Wang, Hongyu; Y., Hao, Yinggguang; Q., Wang, Qiong","Wang, Wei (57192615575); Wang, Hongyu (22037060600); Hao, Yinggguang (57199703948); Wang, Qiong (59210585000)","57192615575; 22037060600; 57199703948; 59210585000","Action Quality Assessment with Multi-scale Temporal Attention Mechanism","2024","","","","","247","251","0","0","10.1109/ICAACE61206.2024.10548995","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197889824&doi=10.1109%2FICAACE61206.2024.10548995&partnerID=40&md5=3cc32727d9d01da1f5d728ac58496d14","Dalian University of Technology, Dalian, China; 81550 Troops, Dalian, China","Wang, Wei, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Wang, Hongyu, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Hao, Yinggguang, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Wang, Qiong, 42 Unit, 81550 Troops, Dalian, China","Action quality assessment is a more challenging topic than action recognition because of its requirement for models to assess quality through fine-grained differences in actions. Current mainstream approaches formalize the problem as a regression task based on video spatio-temporal features. However, most previous methods ignore that motion performance at different stages or time points may have different importance in action quality assessment. In this regard, we propose an action quality assessment method using a multi-scale temporal attention mechanism to assign appropriate weights to different time steps through the temporal attention mechanism. In addition, to address the issues of video feature fusion and subjective noise in the AQA dataset, LSTM-like MLP structures and smooth labeling strategies were applied respectively. Compared to the current state-of-the-art method, CoRe, we improved 1.84% on the AQA-7 dataset and 0.91% on the MTL-AQA dataset. © 2024 Elsevier B.V., All rights reserved.","Action Quality Assessment; Deep Learning; Temporal Attention; Video Understanding; Computer Vision; 'current; Action Quality Assessment; Action Recognition; Attention Mechanisms; Deep Learning; Fine Grained; Multi-scales; Quality Assessment; Temporal Attention; Video Understanding; Long Short-term Memory","Computer vision; 'current; Action quality assessment; Action recognition; Attention mechanisms; Deep learning; Fine grained; Multi-scales; Quality assessment; Temporal attention; Video understanding; Long short-term memory","","","Supported by Dalian Science and Technology Innovation Fund Project 2022JJ11CG002.","Proceedings of AI ED, (1995); Pirsiavash, Hamed, Assessing the quality of actions, Lecture Notes in Computer Science, 8694 LNCS, PART 6, pp. 556-571, (2014); Parmar, Paritosh, Learning to Score Olympic Events, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2017-July, pp. 76-84, (2017); Tran, Du, Learning spatiotemporal features with 3D convolutional networks, Proceedings of the IEEE International Conference on Computer Vision, 2015 International Conference on Computer Vision, ICCV 2015, pp. 4489-4497, (2015); Hochreiter, Sepp, Long Short-Term Memory, Neural Computation, 9, 8, pp. 1735-1780, (1997); Xu, Chengming, Learning to Score Figure Skating Sport Videos, IEEE Transactions on Circuits and Systems for Video Technology, 30, 12, pp. 4578-4590, (2020); Pan, Jiahui, Action assessment by joint relation graphs, Proceedings of the IEEE International Conference on Computer Vision, pp. 6330-6339, (2019); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Parmar, Paritosh, What and how well you performed? a multitask learning approach to action quality assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 304-313, (2019); Tang, Yansong, Uncertainty-Aware Score Distribution Learning for Action Quality Assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 9836-9845, (2020)","","Institute of Electrical and Electronics Engineers Inc.","IEEE","7th International Conference on Advanced Algorithms and Control Engineering, ICAACE 2024","","Hybrid, Shanghai","200297","","9798350361445","","","English","Conference paper","Final","","Scopus","2-s2.0-85197889824"
"T., Nagai, Takasuke; S., Takeda, Shoichiro; S., Suzuki, Satoshi; H., Seshimo, Hitoshi","Nagai, Takasuke (57203634284); Takeda, Shoichiro (57207765972); Suzuki, Satoshi (57033228400); Seshimo, Hitoshi (57190427174)","57203634284; 57207765972; 57033228400; 57190427174","MMW-AQA: Multimodal In-the-Wild Dataset for Action Quality Assessment","2024","IEEE Access","12","","","92062","92072","0","2","10.1109/ACCESS.2024.3423462","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197516873&doi=10.1109%2FACCESS.2024.3423462&partnerID=40&md5=487b6370ae982a8a6299c3b677aa38d4","Nippon Telegraph and Telephone Corporation, Tokyo, Japan","Nagai, Takasuke, Nippon Telegraph and Telephone Corporation, Tokyo, Japan; Takeda, Shoichiro, Nippon Telegraph and Telephone Corporation, Tokyo, Japan; Suzuki, Satoshi, Nippon Telegraph and Telephone Corporation, Tokyo, Japan; Seshimo, Hitoshi, Nippon Telegraph and Telephone Corporation, Tokyo, Japan","Action quality assessment (AQA) is a task for assessing a specific action quality in videos. Since existing AQA datasets provide only two-dimensional (2D) video data captured from fewer viewpoints, existing AQA methods based on deep neural networks (DNNs) often struggle to assess complex three-dimensional (3D) actions accurately, and their robustness against diversified viewpoints remains unknown. We created a dataset called multimodal in-the-wild (MMW)-AQA in freestyle windsurfing that addresses these concerns. In addition to video data, MMW-AQA provides inertial measurement unit (IMU) and global positioning system (GPS) data. The 3D information of IMU data helps DNNs accurately assess complex 3D actions. Moreover, MMW-AQA provides wild video data captured by a single unmanned aerial vehicle (UAV). These wild video data enable us to evaluate whether AQA methods can work well on diversified viewpoints. Furthermore, we also present the baseline multimodalization framework with a transformer-based fusion module. These frameworks multimodalize existing unimodal DNN models easily to assess action quality using multimodal data. Our experimental results demonstrate that multimodal data improves the AQA accuracy compared with unimodal video data. © 2024 Elsevier B.V., All rights reserved.","Action Quality Assessment; Deep Learning; Multimodal Dataset; Multimodal Learning; Antennas; Complex Networks; Global Positioning System; Three Dimensional Displays; Unmanned Aerial Vehicles (uav); Video Recording; Action Quality Assessment; Deep Learning; Multi-modal Dataset; Multi-modal Learning; Multisensory Integration; Quality Assessment; Three-dimensional Display; Transformer; Video; Deep Neural Networks","Antennas; Complex networks; Global positioning system; Three dimensional displays; Unmanned aerial vehicles (UAV); Video recording; Action quality assessment; Deep learning; Multi-modal dataset; Multi-modal learning; Multisensory integration; Quality assessment; Three-dimensional display; Transformer; Video; Deep neural networks","","","","Pirsiavash, Hamed, Assessing the quality of actions, Lecture Notes in Computer Science, 8694 LNCS, PART 6, pp. 556-571, (2014); Parmar, Paritosh, Learning to Score Olympic Events, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2017-July, pp. 76-84, (2017); Parmar, Paritosh, Action quality assessment across multiple actions, pp. 1468-1476, (2019); Parmar, Paritosh, What and how well you performed? a multitask learning approach to action quality assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 304-313, (2019); Pan, Jiahui, Action assessment by joint relation graphs, Proceedings of the IEEE International Conference on Computer Vision, pp. 6330-6339, (2019); Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics, (2023); Tang, Yansong, Uncertainty-Aware Score Distribution Learning for Action Quality Assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 9836-9845, (2020); Yu, Xumin, Group-aware Contrastive Regression for Action Quality Assessment, Proceedings of the IEEE International Conference on Computer Vision, pp. 7899-7908, (2021); Xu, Jinglin, FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2022-June, pp. 2939-2948, (2022); Bai, Yang, Action Quality Assessment with Temporal Parsing Transformer, Lecture Notes in Computer Science, 13664 LNCS, pp. 422-438, (2022)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85197516873"
"S., Sardari, Sara; B., Nakisa, Bahareh; S., Sharifzadeh, Sara; A., Daneshkhah, Alireza; S.W., Loke, Seng W.; M.J., Duncan, Michael J.; M., Crotti, Matteo; V., Palade, Vasile","Sardari, Sara (57509554600); Nakisa, Bahareh (56156792800); Sharifzadeh, Sara (53985366000); Daneshkhah, Alireza (35519340400); Loke, Seng W. (60003381900); Duncan, Michael J. (56654336600); Crotti, Matteo (57195755419); Palade, Vasile (6701857025)","57509554600; 56156792800; 53985366000; 35519340400; 60003381900; 56654336600; 57195755419; 6701857025","Online Deep Squat Evaluation: Leveraging Subject-Specific Adaptation and Information Retention","2024","","","","","","","0","0","10.1109/ICLT63507.2024.11038644","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010815473&doi=10.1109%2FICLT63507.2024.11038644&partnerID=40&md5=d31668c290d7cd10f0cfd8c2ddb2ee30","Coventry University, Coventry, United Kingdom; Faculty of Science, Engineering and Built Environment, Geelong, Australia; Swansea University, Swansea, United Kingdom; Emirates Aviation University, Dubai, United Arab Emirates; Coventry University, Coventry, United Kingdom; Università degli Studi di Bergamo, Bergamo, Italy","Sardari, Sara, Centre for Computational Science and Mathematical Modelling, Coventry University, Coventry, United Kingdom, School of Information Technology, Faculty of Science, Engineering and Built Environment, Geelong, Australia; Nakisa, Bahareh, School of Information Technology, Faculty of Science, Engineering and Built Environment, Geelong, Australia; Sharifzadeh, Sara, Department of Computer Science, Swansea University, Swansea, United Kingdom; Daneshkhah, Alireza, Faculty of Mathematics and Data Science, Emirates Aviation University, Dubai, United Arab Emirates; Loke, Seng W., School of Information Technology, Faculty of Science, Engineering and Built Environment, Geelong, Australia; Duncan, Michael J., Exercise and Life Sciences, Coventry University, Coventry, United Kingdom; Crotti, Matteo, Department of Human and Social Sciences, Università degli Studi di Bergamo, Bergamo, Italy; Palade, Vasile, Centre for Computational Science and Mathematical Modelling, Coventry University, Coventry, United Kingdom","Evaluating deep squats accurately during automatic physical rehabilitation monitoring across different subjects remains challenging due to inter-subject variability and limited labelled data. The challenges include: 1) conventional methods presuppose that a ""one-model-fits-all""approach works for activity evaluation, ignoring that subject-specific differences can lead to suboptimal results if these differences are not considered. 2) Previous studies focus on offline learning, where models are trained on the entire dataset, which can be updated later through retraining. This approach neglects the need for continual learning, where models adapt sequentially to new subjects while retaining past knowledge to prevent catastrophic forgetting. This study addresses these challenges by proposing a novel continual meta-learning approach and a memory buffer to provide personalized deep squat evaluations. Using Azure Kinect sensors, we collected RGB-D videos and 3D skeletal data from 33 participants performing deep squats, annotated with Functional Movement Screen (FMS) scores. Our model dynamically adapts to new participants while retaining knowledge from previous ones, preventing performance degradation over time. Experimental results demonstrate that our approach outperforms a model without a buffer memory technique by retaining learned knowledge across participants and adapting to new individuals with minimal data. © 2025 Elsevier B.V., All rights reserved.","Action Quality Assessment; Continual Learning; Few-shot Learning; Meta-learning; Skeleton Data; Buffer Storage; Deep Learning; Learning Systems; Patient Monitoring; Action Quality Assessment; Continual Learning; Few-shot Learning; Information Retention; Metalearning; Quality Assessment; Skeleton Data; Specific Adaptations; Specific Information; Subject-specific; Musculoskeletal System","Buffer storage; Deep learning; Learning systems; Patient monitoring; Action quality assessment; Continual learning; Few-shot learning; Information retention; Metalearning; Quality assessment; Skeleton data; Specific adaptations; Specific information; Subject-specific; Musculoskeletal system","","","The content of this paper is part of a PhD project funded by Coventry University.","Hoogenboom, Barbara J., Three-Dimensional Kinematics and Kinetics of the Overhead Deep Squat in Healthy Adults: A Descriptive Study, Applied Sciences (Switzerland), 13, 12, (2023); Yoshiko, Akito, Impact of home-based squat training with two-depths on lower limb muscle parameters and physical functional tests in older adults, Scientific Reports, 11, 1, (2021); Xing, Qingjun, Functional movement screen dataset collected with two Azure Kinect depth sensors, Scientific Data, 9, 1, (2022); Lee, Jaehyun, Automatic classification of squat posture using inertial sensors: Deep learning approach, Sensors, 20, 2, (2020); Luna, Alessandro, Artificial intelligence application versus physical therapist for squat evaluation: a randomized controlled trial, Scientific Reports, 11, 1, (2021); Lei, Qing, A survey of vision-based human action evaluation methods, Sensors, 19, 19, (2019); Sardari, Sara, Artificial Intelligence for skeleton-based physical rehabilitation action evaluation: A systematic review, Computers in Biology and Medicine, 158, (2023); Hospedales, Timothy M., Meta-Learning in Neural Networks: A Survey, IEEE Transactions on Pattern Analysis and Machine Intelligence, 44, 9, pp. 5149-5169, (2022); Chen, Yinbo, Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning, Proceedings of the IEEE International Conference on Computer Vision, pp. 9042-9051, (2021); Gharoun, Hassan, Meta-learning Approaches for Few-Shot Learning: A Survey of Recent Advances, ACM Computing Surveys, 56, 12, (2024)","","Institute of Electrical and Electronics Engineers Inc.","","2024 IEEE Consumer Life Tech, ICLT 2024","","Sydney; NSW","209763","","9798331519339","","","English","Conference paper","Final","","Scopus","2-s2.0-105010815473"
"P., Lian, Puxiang; Z., Shao, Zhigang","Lian, Puxiang (58696312500); Shao, Zhigang (35574802700)","58696312500; 35574802700","Improving action quality assessment with across-staged temporal reasoning on imbalanced data","2023","Applied Intelligence","53","24","","30443","30454","0","4","10.1007/s10489-023-05166-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176769140&doi=10.1007%2Fs10489-023-05166-3&partnerID=40&md5=0aa3db8d5aca04f518f54878d9c9f284","South China Normal University, Guangzhou, China; South China Normal University, Guangzhou, China; South China Normal University, Guangzhou, China; South China Normal University, Guangzhou, China","Lian, Puxiang, School of Electronics and Information Engineering, South China Normal University, Guangzhou, China; Shao, Zhigang, School of Physics and Telecommunication Engineering, South China Normal University, Guangzhou, China, Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, South China Normal University, Guangzhou, China, Frontier Research Institute for Physics, South China Normal University, Guangzhou, China","Action quality assessment is a significant research domain in computer vision, aimed at evaluating the accuracy of human movement and providing feedback and guidance for training and rehabilitation. However, the uneven nature of the data, which has a significant impact on the labels with less samples, is not taken into consideration by the generally used approaches in this field. To address this issue, we propose using kernel density estimation (KDE) to recalculate the label density and weight the loss function by the reciprocal of the square root of each label density. Additionally, we divide the entire motion into three sub-stages, including the takeoff, aerial movement, and entry for diving, and connect the three stages using an across-staged temporal reasoning module (ASTRM). Our approach achieves a performance of 0.9222 Spearman correlation coefficient (ρ) and 0.3304 (× 100) Relative ℓ<inf>2</inf> -distance (R - ℓ<inf>2</inf>) on the FineDiving dataset, demonstrating competitiveness compared to other methods. Furthermore, numerous comprehensive ablation experiments validate the effectiveness of the methods and modules we adopted. © 2024 Elsevier B.V., All rights reserved.","Computer Vision; Diving; Human Movement; Label Density; Multi-substage Aqa Model; Spearman Correlation Coefficient; Antennas; Diving; Human Movements; Imbalanced Data; Kernel Density Estimation; Label Density; Multi-substage Aqa Model; Quality Assessment; Research Domains; Spearman Correlation Coefficients; Temporal Reasoning; Computer Vision","Antennas; Diving; Human movements; Imbalanced data; Kernel Density Estimation; Label density; Multi-substage AQA model; Quality assessment; Research domains; Spearman correlation coefficients; Temporal reasoning; Computer vision","","","This work was supported by the National Natural Science Foundation of China (Grant No.52072132).","Srivastava, Abhishek, Analytical evaluation of agile success factors influencing quality in software industry, International Journal of System Assurance Engineering and Management, 11, pp. 247-257, (2020); Singh, Dipti, Integrated municipal solid waste management in Faridabad City, Haryana State (India), International Journal of System Assurance Engineering and Management, 11, 2, pp. 411-425, (2020); Vadivel, S. M., Sustainable postal service design: integrating quality function deployment from the customers perspective, International Journal of System Assurance Engineering and Management, 11, 2, pp. 494-505, (2020); Amanbek, Nurlan, Results of a comprehensive assessment of the quality of services to the population with the use of statistical methods, International Journal of System Assurance Engineering and Management, 12, 6, pp. 1322-1333, (2021); Singh, Ajit Kumar, Industry oriented quality management of engineering education: an integrated QFD-TOPSIS approach, International Journal of System Assurance Engineering and Management, 13, 2, pp. 904-922, (2022); Gupta, Shikha, ANFIS-Based Control of Multi-objective Grid Connected Inverter and Energy Management, Journal of The Institution of Engineers (India): Series B, 101, 1, pp. 1-14, (2020); Xu, Chengming, Learning to Score Figure Skating Sport Videos, IEEE Transactions on Circuits and Systems for Video Technology, 30, 12, pp. 4578-4590, (2020); Parmar, Paritosh, Domain Knowledge-Informed Self-supervised Representations for Workout Form Assessment, Lecture Notes in Computer Science, 13698 LNCS, pp. 105-123, (2022); Doughty, Hazel, The pros and cons: Rank-aware temporal attention for skill determination in long videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7854-7863, (2019); Nayak, Jyoti Ranjan, A fuzzy adaptive symbiotic organism search based hybrid wavelet transform-extreme learning machine model for load forecasting of power system: a case study, Journal of Ambient Intelligence and Humanized Computing, 14, 8, pp. 10833-10847, (2023)","","Springer","","","","","","15737497; 0924669X","9780511611445; 9780521884280","APITE","","English","Article","Final","","Scopus","2-s2.0-85176769140"
"K., Zhou, Kanglei; Y., Ma, Yue; H.P., Shum, Hubert P.H.; X., Liang, Xiaohui","Zhou, Kanglei (57205674291); Ma, Yue (57218566782); Shum, Hubert P.H. (25032239300); Liang, Xiaohui (7401735847)","57205674291; 57218566782; 25032239300; 7401735847","Hierarchical Graph Convolutional Networks for Action Quality Assessment","2023","IEEE Transactions on Circuits and Systems for Video Technology","33","12","3281413","7749","7763","0","42","10.1109/TCSVT.2023.3281413","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161002529&doi=10.1109%2FTCSVT.2023.3281413&partnerID=40&md5=77b3bf67795d19b159fa94bdc0f05aa7","Durham University, Durham, United Kingdom; Beihang University, Beijing, China; Zhongguancun Laboratory, Beijing, China","Zhou, Kanglei, Beihang University, Beijing, China; Ma, Yue, Beihang University, Beijing, China; Shum, Hubert P.H., Department of Computer Science, Durham University, Durham, United Kingdom; Liang, Xiaohui, Beihang University, Beijing, China, Zhongguancun Laboratory, Beijing, China","Action quality assessment (AQA) automatically evaluates how well humans perform actions in a given video, a technique widely used in fields such as rehabilitation medicine, athletic competitions, and specific skills assessment. However, existing works that uniformly divide the video sequence into small clips of equal length suffer from intra-clip confusion and inter-clip incoherence, hindering the further development of AQA. To address this issue, we propose a hierarchical graph convolutional network (GCN). First, semantic information confusion is corrected through clip refinement, generating the 'shot' as the basic action unit. We then construct a scene graph by combining several consecutive shots into meaningful scenes to capture local dynamics. These scenes can be viewed as different procedures of a given action, providing valuable assessment cues. The video-level representation is finally extracted via sequential action aggregation among scenes to regress the predicted score distribution, enhancing discriminative features and improving assessment performance. Experiments on the AQA-7, MTL-AQA, and JIGSAWS datasets demonstrate the superiority of the proposed hierarchical GCN over state-of-the-art methods. © 2023 Elsevier B.V., All rights reserved.","Action Quality Assessment; Graph Convolutional Neural Networks; Human Action Understanding; Computer Vision; Convolution; Feature Extraction; Neural Networks; Semantic Web; Video Recording; Action Quality Assessment; Convolutional Neural Network; Features Extraction; Graph Convolutional Neural Network; Human Action Understanding; Human Actions; Quality Assessment; Representation Learning; Video Sequences; Semantics","Computer vision; Convolution; Feature extraction; Neural networks; Semantic Web; Video recording; Action quality assessment; Convolutional neural network; Features extraction; Graph convolutional neural network; Human action understanding; Human actions; Quality assessment; Representation learning; Video sequences; Semantics","","","This work was supported by the National Natural Science Foundation of China under Project 62272019.","Yang, Zhengyuan, Action Recognition With Spatio–Temporal Visual Attention on Skeleton Image Sequences, IEEE Transactions on Circuits and Systems for Video Technology, 29, 8, pp. 2405-2415, (2019); Kong, Jun, Symmetrical Enhanced Fusion Network for Skeleton-Based Action Recognition, IEEE Transactions on Circuits and Systems for Video Technology, 31, 11, pp. 4394-4408, (2021); Jain, Hiteshi, Action Quality Assessment Using Siamese Network-Based Deep Metric Learning, IEEE Transactions on Circuits and Systems for Video Technology, 31, 6, pp. 2260-2273, (2021); Pirsiavash, Hamed, Assessing the quality of actions, Lecture Notes in Computer Science, 8694 LNCS, PART 6, pp. 556-571, (2014); Wang, Limin, Action recognition with trajectory-pooled deep-convolutional descriptors, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 07-12-June-2015, pp. 4305-4314, (2015); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Finediving A Fine Grained Dataset for Procedure Aware Action Quality Assessment; Paiement, Adeline, Online quality assessment of human movement from skeleton data, (2014); Antunes, Michel, Visual and human-interpretable feedback for assisting physical activity, Lecture Notes in Computer Science, 9914 LNCS, pp. 115-129, (2016); Zhou, Kanglei, A Video-Based Augmented Reality System for Human-in-the-Loop Muscle Strength Assessment of Juvenile Dermatomyositis, IEEE Transactions on Visualization and Computer Graphics, 29, 5, pp. 2456-2466, (2023)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","10518215","","ITCTE","","English","Article","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85161002529"
"N., Hao, Ning; S., Ruan, Sihan; Y., Song, Yiheng; J., Chen, Jiashun; L., Tian, Longgang","Hao, Ning (57207816180); Ruan, Sihan (57226663703); Song, Yiheng (57221792710); Chen, Jiashun (57316417700); Tian, Longgang (35293831100)","57207816180; 57226663703; 57221792710; 57316417700; 35293831100","The Establishment of a precise intelligent evaluation system for sports events: Diving","2023","Heliyon","9","11","e21361","","","0","6","10.1016/j.heliyon.2023.e21361","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174462280&doi=10.1016%2Fj.heliyon.2023.e21361&partnerID=40&md5=8633b7700b35f9c001a3a4331b5fddc2","Southeast University, Nanjing, China; RMIT University, Melbourne, Australia; The University of Tokyo, Tokyo, Japan; Southeast University, Nanjing, China","Hao, Ning, School of Civil Engineering, Southeast University, Nanjing, China; Ruan, Sihan, School of Civil Engineering, Southeast University, Nanjing, China, School of Engineering, RMIT University, Melbourne, Australia; Song, Yiheng, School of Civil Engineering, Southeast University, Nanjing, China, The University of Tokyo, Tokyo, Japan; Chen, Jiashun, School of Computer Science and Engineering, Southeast University, Nanjing, China; Tian, Longgang, School of Civil Engineering, Southeast University, Nanjing, China","The introduction of action quality assessment technology in sports events to achieve precise intelligent evaluation can greatly enhance the objectivity and effectiveness of competition results. Taking diving as the specific application background, this study proposes a novel Multi-granularity Extraction Approach for Temporal-spatial features in judge scoring prediction (MEAT) under the conditions of action quality assessment. On the one hand, it uses dual-modal inflated 3D ConvNet to extract the temporal and spatial features of each modal diving video at the video granularity parallelly and to merge them to form a global feature. On the other hand, the human body pose is modeled, and the simulated athlete's three-dimensional splash state is taken as local characteristics at the object granularity. Finally, the global and local features are concatenated into the fully connected layer, and heuristic method inspired by competition rules using labeled distribution learning are employed to output the probability distribution of the average score of all referees. The maximum probability score is selected and multiplied by the difficulty coefficient to obtain the final diving score. Through comprehensive experiments, comparing the Spearman's rank correlation (SRC) evaluation results of existing methods on the UNIV-Dive dataset, this framework reflects the greater accuracy advantage and further lays the foundation for the actual implementation of the technology. © 2023 Elsevier B.V., All rights reserved.","Action Recognition And Assessment; Computer Vision; Diving; Engineering Mechanics","","","","","Liu, Hai, EHPE: Skeleton Cues-Based Gaussian Coordinate Encoding for Efficient Human Pose Estimation, IEEE Transactions on Multimedia, 26, pp. 8464-8475, (2024); Liu, Tingting, LDCNet: Limb Direction Cues-Aware Network for Flexible HPE in Industrial Behavioral Biometrics Systems, IEEE Transactions on Industrial Informatics, 20, 6, pp. 8068-8078, (2024); Liu, Hai, ARHPE: Asymmetric Relation-Aware Representation Learning for Head Pose Estimation in Industrial Human-Computer Interaction, IEEE Transactions on Industrial Informatics, 18, 10, pp. 7107-7117, (2022); Automatic Evaluation of Organized Basketball Activity Using Bayesian Networks, (2007); Pirsiavash, Hamed, Detecting activities of daily living in first-person camera views, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 2847-2854, (2012); undefined, (2011); undefined; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, (2017); Pirsiavash, Hamed, Assessing the quality of actions, Lecture Notes in Computer Science, 8694 LNCS, PART 6, pp. 556-571, (2014); Parmar, Paritosh, What and how well you performed? a multitask learning approach to action quality assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 304-313, (2019)","","Elsevier Ltd","","","","","","24058440","","","","English","Article","Final","All Open Access; Gold Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85174462280"
"C.K., Ingwersen, Christian Keilstrup; A., Xarles, Artur; A., Clapés, Albert; M., Madadi, Meysam; J.N., Jensen, Janus Nortoft; M.R., Hannemose, Morten Rieger; A.B., Dahl, Anders Bjørholm; S., Escalera, Sergio","Ingwersen, Christian Keilstrup (57205532216); Xarles, Artur (57955669900); Clapés, Albert (55329424200); Madadi, Meysam (56524375700); Jensen, Janus Nortoft (57207063003); Hannemose, Morten Rieger (57193571739); Dahl, Anders Bjørholm (23388569300); Escalera, Sergio (22634035000)","57205532216; 57955669900; 55329424200; 56524375700; 57207063003; 57193571739; 23388569300; 22634035000","Video-based Skill Assessment for Golf: Estimating Golf Handicap","2023","","","","","31","39","0","4","10.1145/3606038.3616150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178297880&doi=10.1145%2F3606038.3616150&partnerID=40&md5=6ef23b8235ce2410d4d8ec1d2fd2da3e","Technical University of Denmark, Lyngby, Denmark; Universitat de Barcelona, Barcelona, Spain; Technical University of Denmark, Lyngby, Denmark; Universitat de Barcelona, Barcelona, Spain","Ingwersen, Christian Keilstrup, Technical University of Denmark, Lyngby, Denmark; Xarles, Artur, Universitat de Barcelona, Barcelona, Spain; Clapés, Albert, Universitat de Barcelona, Barcelona, Spain; Madadi, Meysam, Universitat de Barcelona, Barcelona, Spain; Jensen, Janus Nortoft, Technical University of Denmark, Lyngby, Denmark; Hannemose, Morten Rieger, Technical University of Denmark, Lyngby, Denmark; Dahl, Anders Bjørholm, Technical University of Denmark, Lyngby, Denmark; Escalera, Sergio, Universitat de Barcelona, Barcelona, Spain","Automated skill assessment in sports using video-based analysis holds great potential for revolutionizing coaching methodologies. This paper focuses on the problem of skill determination in golfers by leveraging deep learning models applied to a large database of video recordings of golf swings. We investigate different regression, ranking and classification based methods and compare to a simple baseline approach. The performance is evaluated using mean squared error (MSE) as well as computing the percentages of correctly ranked pairs based on the Kendall correlation. Our results demonstrate an improvement over the baseline, with a 35% lower mean squared error and 68% correctly ranked pairs. However, achieving fine-grained skill assessment remains challenging. This work contributes to the development of AI-driven coaching systems and advances the understanding of video-based skill determination in the context of golf. © 2023 Elsevier B.V., All rights reserved.","Action Quality Assessment; Action Understanding; Datasets; Golf; Neural Networks; Deep Learning; Mean Square Error; Video Recording; Action Quality Assessment; Action Understanding; Dataset; Golf Swing; Large Database; Learning Models; Mean Squared Error; Neural-networks; Quality Assessment; Skill Assessment; Golf","Deep learning; Mean square error; Video recording; Action quality assessment; Action understanding; Dataset; Golf swing; Large database; Learning models; Mean squared error; Neural-networks; Quality assessment; Skill assessment; Golf","","","This work has been partially supported by the Spanish project PID2022-136436NB-I00 and by ICREA under the ICREA Academia programme.","Trackman A S N D Trackman; Encyclopedia of Measurement and Statistics, (2007); Capecci, M., The KIMORE Dataset: KInematic Assessment of MOvement and Clinical Scores for Remote Monitoring of Physical REhabilitation, IEEE Transactions on Neural Systems and Rehabilitation Engineering, 27, 7, pp. 1436-1448, (2019); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Doughty, Hazel, Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 6057-6066, (2018); Doughty, Hazel, The pros and cons: Rank-aware temporal attention for skill determination in long videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7854-7863, (2019); Farabi, Shafkat, Improving Action Quality Assessment Using Weighted Aggregation, Lecture Notes in Computer Science, 13256 LNCS, pp. 576-587, (2022); Feichtenhofer, Christoph, Slowfast networks for video recognition, Proceedings of the IEEE International Conference on Computer Vision, pp. 6201-6210, (2019); 2022 Thor Threshold Based Ranking Loss for Ordinal Regression; Miccai Workshop M2cai, (2014)","","Association for Computing Machinery, Inc","ACM SIGMM","6th ACM International Workshop on Multimedia Content Analysis in Sports, MMSports 2023, co-located with ACM Multimedia 2023","","Ottawa; ON","193957","","9798400702693","","","English","Conference paper","Final","All Open Access; Green Final Open Access; Green Open Access","Scopus","2-s2.0-85178297880"
"Y., Liu, Yanchao; X., Cheng, Xina; T., Ikenaga, Takeshi","Liu, Yanchao (58262316100); Cheng, Xina (56621799800); Ikenaga, Takeshi (8882572600)","58262316100; 56621799800; 8882572600","A Figure Skating Jumping Dataset for Replay-Guided Action Quality Assessment","2023","","","","","2437","2445","0","8","10.1145/3581783.3613774","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179548737&doi=10.1145%2F3581783.3613774&partnerID=40&md5=b55139c2ca04ce6c0f7039e73f11c012","Waseda University, Tokyo, Japan; Xidian University, Xi'an, China","Liu, Yanchao, Waseda University, Tokyo, Japan; Cheng, Xina, Xidian University, Xi'an, China; Ikenaga, Takeshi, Waseda University, Tokyo, Japan","In competitive sports, judges often scrutinize replay videos from multiple views to adjudicate uncertain or contentious actions, and ultimately ascertain the definitive score. Most existing action quality assessment methods regress from a single video or a pairwise exemplar and input videos, which are limited by the viewpoint and zoom scale of videos. To end this, we construct a Replay Figure Skating Jumping dataset (RFSJ), containing additional view information provided by the post-match replay video and fine-grained annotations. We also propose a Replay-Guided approach for action quality assessment, learned by a Triple-Stream Contrastive Transformer and a Temporal Concentration Module. Specifically, besides the pairwise input and exemplar, we contrast the input and its replay by an extra contrastive module. Then the consistency of scores guides the model to learn features of the same action under different views and zoom scales. In addition, based on the fact that errors or highlight moments of athletes are crucial factors affecting scoring, these moments are concentrated in parts of the video rather than a uniform distribution. The proposed temporal concentration module encourages the model to concentrate on these features, then cooperates with the contrastive regression module to obtain an effective scoring mechanism. Extensive experiments demonstrate that our method achieves Spearman's Rank Correlation of 0.9346 on the proposed RFSJ dataset, improving over the existing state-of-the-art methods. © 2025 Elsevier B.V., All rights reserved.","Action Quality Assessment; Sports Video Dataset; Triple-stream Contrastive Learning; Video Action Analysis; Computer Vision; Action Analysis; Action Quality Assessment; Figure Skating; Quality Assessment; Sport Video; Sport Video Dataset; Temporal Concentrations; Triple-stream Contrastive Learning; Video Action Analyse; Video Dataset; Sports","Computer vision; Action analysis; Action quality assessment; Figure skating; Quality assessment; Sport video; Sport video dataset; Temporal concentrations; Triple-stream contrastive learning; Video action analyse; Video dataset; Sports","","","This work was supported by KAKENHI (21K11816).","Bai, Yang, Action Quality Assessment with Temporal Parsing Transformer, Lecture Notes in Computer Science, 13664 LNCS, pp. 422-438, (2022); Proceedings of the IEEE International Conference on Computer Vision, (2017); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Chen, Shoufa, Watch Only Once: An End-to-End Video Action Detection Framework, Proceedings of the IEEE International Conference on Computer Vision, pp. 8158-8167, (2021); Cheng, Ke, Skeleton-based action recognition with shift graph convolutional network, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 180-189, (2020); Doughty, Hazel, Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 6057-6066, (2018); Doughty, Hazel, The pros and cons: Rank-aware temporal attention for skill determination in long videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7854-7863, (2019); Eun, Hyunjun, Learning to discriminate information for online action detection, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 806-815, (2020); Funke, Isabel, Video-based surgical skill assessment using 3D convolutional neural networks, International Journal of Computer Assisted Radiology and Surgery, 14, 7, pp. 1217-1225, (2019); Gao, Ruohan, Listen to look: Action recognition by previewing audio, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 10454-10464, (2020)","","Association for Computing Machinery, Inc","ACM SIGMM","31st ACM International Conference on Multimedia, MM 2023","","Ottawa; ON","194105","","9798400701085","","","English","Conference paper","Final","","Scopus","2-s2.0-85179548737"
"J., Li, Jicheng; V., Chheang, Vuthea; P., Kullu, Pinar; E., Brignac, Eli; Z., Guo, Zhang; A.N., Bhat, Anjana Narayan; K.E., Barner, Kenneth E.; R.L., Barmaki, Roghayeh Leila","Li, Jicheng (57225221973); Chheang, Vuthea (57203963677); Kullu, Pinar (57202829609); Brignac, Eli (58452122900); Guo, Zhang (57217061612); Bhat, Anjana Narayan (7102056594); Barner, Kenneth E. (7006447386); Barmaki, Roghayeh Leila (57079124100)","57225221973; 57203963677; 57202829609; 58452122900; 57217061612; 7102056594; 7006447386; 57079124100","MMASD: A Multimodal Dataset for Autism Intervention Analysis","2023","","","","","397","405","0","8","10.1145/3577190.3614117","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175845393&doi=10.1145%2F3577190.3614117&partnerID=40&md5=bcb7589d4c85f0e99b7d0001607f3c08","University of Delaware, Newark, United States","Li, Jicheng, University of Delaware, Newark, United States; Chheang, Vuthea, University of Delaware, Newark, United States; Kullu, Pinar, University of Delaware, Newark, United States; Brignac, Eli, University of Delaware, Newark, United States; Guo, Zhang, University of Delaware, Newark, United States; Bhat, Anjana Narayan, University of Delaware, Newark, United States; Barner, Kenneth E., University of Delaware, Newark, United States; Barmaki, Roghayeh Leila, University of Delaware, Newark, United States","Autism spectrum disorder (ASD) is a developmental disorder characterized by significant impairments in social communication and difficulties perceiving and presenting communication signals. Machine learning techniques have been widely used to facilitate autism studies and assessments. However, computational models are primarily concentrated on very specific analysis and validated on private, non-public datasets in the autism community, which limits comparisons across models due to privacy-preserving data-sharing complications. This work presents a novel open source privacy-preserving dataset, MMASD as a MultiModal ASD benchmark dataset, collected from play therapy interventions for children with autism. The MMASD includes data from 32 children with ASD, and 1,315 data samples segmented from more than 100 hours of intervention recordings. To promote the privacy of children while offering public access, each sample consists of four privacy-preserving modalities, some of which are derived from original videos: (1) optical flow, (2) 2D skeleton, (3) 3D skeleton, and (4) clinician ASD evaluation scores of children. MMASD aims to assist researchers and therapists in understanding children's cognitive status, monitoring their progress during therapy, and customizing the treatment plan accordingly. It also inspires downstream social tasks such as action quality assessment and interpersonal synchrony estimation. The dataset is publicly accessible via the MMASD project website. © 2023 Elsevier B.V., All rights reserved.","2d/ 3d Skeleton; Autism Spectrum Disorder; Deep Learning; Human Activity Recognition; Machine Learning; Multimodal Dataset; Privacy-preserving Data Sharing.; Diseases; Learning Systems; Musculoskeletal System; Privacy-preserving Techniques; 2d/ 3d Skeleton; 3d Skeleton; Autism Spectrum Disorders; Data Sharing; Deep Learning; Human Activity Recognition; Machine-learning; Multi-modal Dataset; Privacy Preserving; Privacy-preserving Data Sharing.","Diseases; Learning systems; Musculoskeletal system; Privacy-preserving techniques; 2d/ 3d skeleton; 3D skeleton; Autism spectrum disorders; Data Sharing; Deep learning; Human activity recognition; Machine-learning; Multi-modal dataset; Privacy preserving; Privacy-preserving data sharing.","","","We would like to express our sincere gratitude for our research team, therapists and students who conducted the therapy sessions (Sudha Srinivasan, Maninderjit Kaur, and Isabel Park). We also thank therapy participants and their caregivers for contributing to the MMASD dataset collection. This work is partly supported by our sponsors, Amazon Research Awards Program, University of Delaware AI Center of Excellence, the National Institute of General Medical Sciences (NIGMS, P20 GM103446E), the National Institutes of Mental Health (NIMH, 5R21MH089441-02, 4R33MH089441-03), and Autism Speaks (Grant #8137). Any opinions, fndings, conclusions, or recommendations expressed in this paper are those of the authors and do not necessarily refect the views of the sponsors.","Baio, Jon, Prevalence of autism spectrum disorder among children aged 8 Years - Autism and developmental disabilities monitoring network, 11 Sites, United States, 2014, MMWR Surveillance Summaries, 67, 6, pp. 1-23, (2018); Automatic Classifcation of Autistic Child Vocalisations A Novel Database and Results, (2017); Baltrušaitis, Tadas, Constrained local neural fields for robust facial landmark detection in the wild, Proceedings of the IEEE International Conference on Computer Vision, pp. 354-361, (2013); Billing, Erik A., The DREAM Dataset: Supporting a data-driven study of autism spectrum disorder and robot enhanced therapy, PLOS ONE, 15, 8 August 2020, (2020); Cao, Zhe, OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields, IEEE Transactions on Pattern Analysis and Machine Intelligence, 43, 1, pp. 172-186, (2021); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Chen, Shi, Attention-based autism spectrum disorder screening with privileged modality, Proceedings of the IEEE International Conference on Computer Vision, pp. 1181-1190, (2019); Towards Anatomy Education with Generative AI Based Virtual Assistants in Immersive Virtual Reality Environments, (2023); Coco, Moreno I., Cross-recurrence quantification analysis of categorical and continuous time series: An R package, Frontiers in Psychology, 5, JUN, (2014); Dawson, Geraldine, Atypical postural control can be detected via computer vision analysis in toddlers with autism spectrum disorder, Scientific Reports, 8, 1, (2018)","","Association for Computing Machinery","ACM SIGCHI; AFIHM; CCC Computing Community Consortium; et al.; Openstreams.ai and Living and Learning Lab Neurodevelopment (LiLLab); Persyval Lab","25th International Conference on Multimodal Interaction, ICMI 2023","","Paris","193303","","9798400700552; 9798400708442; 9798400707612; 9798400703218; 9798400708190; 9798400707520","","","English","Conference paper","Final","","Scopus","2-s2.0-85175845393"
"K., Zhou, Kanglei; R., Cai, Ruizhi; Y., Ma, Yue; Q., Tan, Qingqing; X., Wang, Xinning; J., Li, Jianguo; H.P., Shum, Hubert P.H.; F.W., Li, Frederick W.B.; S., Jin, Song; X., Liang, Xiaohui","Zhou, Kanglei (57205674291); Cai, Ruizhi (58128062700); Ma, Yue (57218566782); Tan, Qingqing (58123647200); Wang, Xinning (57193015804); Li, Jianguo (56540149100); Shum, Hubert P.H. (25032239300); Li, Frederick W.B. (7406057098); Jin, Song (58127881700); Liang, Xiaohui (7401735847)","57205674291; 58128062700; 57218566782; 58123647200; 57193015804; 56540149100; 25032239300; 7406057098; 58127881700; 7401735847","A Video-Based Augmented Reality System for Human-in-the-Loop Muscle Strength Assessment of Juvenile Dermatomyositis","2023","IEEE Transactions on Visualization and Computer Graphics","29","5","","2456","2466","0","16","10.1109/TVCG.2023.3247092","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149403925&doi=10.1109%2FTVCG.2023.3247092&partnerID=40&md5=cf5ea67ee8d3259f0a278575c3ac104e","Beihang University, Beijing, China; Zhongguancun Laboratory, Beijing, China; Beihang University, Beijing, China; Capital Institute of Pediatrics, Beijing, China; Durham University, Durham, United Kingdom; Ltd., Beijing, China","Zhou, Kanglei, Beihang University, Beijing, China; Cai, Ruizhi, Beihang University, Beijing, China; Ma, Yue, Beihang University, Beijing, China; Tan, Qingqing, Capital Institute of Pediatrics, Beijing, China; Wang, Xinning, Capital Institute of Pediatrics, Beijing, China; Li, Jianguo, Capital Institute of Pediatrics, Beijing, China; Shum, Hubert P.H., Durham University, Durham, United Kingdom; Li, Frederick W.B., Durham University, Durham, United Kingdom; Jin, Song, Ltd., Beijing, China; Liang, Xiaohui, Beihang University, Beijing, China, Zhongguancun Laboratory, Beijing, China","As the most common idiopathic inflammatory myopathy in children, juvenile dermatomyositis (JDM) is characterized by skin rashes and muscle weakness. The childhood myositis assessment scale (CMAS) is commonly used to measure the degree of muscle involvement for diagnosis or rehabilitation monitoring. On the one hand, human diagnosis is not scalable and may be subject to personal bias. On the other hand, automatic action quality assessment (AQA) algorithms cannot guarantee 100% accuracy, making them not suitable for biomedical applications. As a solution, we propose a video-based augmented reality system for human-in-the-loop muscle strength assessment of children with JDM. We first propose an AQA algorithm for muscle strength assessment of JDM using contrastive regression trained by a JDM dataset. Our core insight is to visualize the AQA results as a virtual character facilitated by a 3D animation dataset, so that users can compare the real-world patient and the virtual character to understand and verify the AQA results. To allow effective comparisons, we propose a video-based augmented reality system. Given a feed, we adapt computer vision algorithms for scene understanding, evaluate the optimal way of augmenting the virtual character into the scene, and highlight important parts for effective human verification. The experimental results confirm the effectiveness of our AQA algorithm, and the results of the user study demonstrate that humans can more accurately and quickly assess the muscle strength of children using our system. © 2023 Elsevier B.V., All rights reserved.","Action Quality Assessment; Augmented Reality; Human-in-the-loop System; Juvenile Dermatomyositis; Animation; Augmented Reality; Bioinformatics; Diagnosis; Medical Applications; Medical Imaging; Virtual Reality; Action Quality Assessment; Human-in-the-loop; Human-in-the-loop System; Juvenile Dermatomyositis; Loop Systems; Medical Diagnostic Imaging; Medical Services; Muscle Strength; Quality Assessment; Muscle","Animation; Augmented reality; Bioinformatics; Diagnosis; Medical applications; Medical imaging; Virtual reality; Action quality assessment; Human-in-the-loop; Human-in-the-loop system; Juvenile dermatomyositis; Loop systems; Medical diagnostic imaging; Medical services; Muscle strength; Quality assessment; Muscle","","","This work was supported by the National Natural Science Foundation of China (Project Number: 62272019).","Antunes, Michel, Visual and human-interpretable feedback for assisting physical activity, Lecture Notes in Computer Science, 9914 LNCS, pp. 115-129, (2016); Action Quality Assessment with Temporal Parsing Transformer, (2022); Batthish, Michelle, Juvenile dermatomyositis, Current Rheumatology Reports, 13, 3, pp. 216-224, (2011); Buchanan, Emma, On the Effectiveness of Conveying BIM Metadata in VR Design Reviews for Healthcare Architecture, pp. 806-807, (2022); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Chen, Liuyuan, Solution path algorithm for twin multi-class support vector machine, Expert Systems with Applications, 210, (2022); Chen, Ting, A simple framework for contrastive learning of visual representations, PartF168147-3, pp. 1575-1585, (2020); Cidotã, Marina Anca, Assessing upper extremity motor dysfunction using an augmented reality game, pp. 144-154, (2017); Dhar, Poshmaal, Augmented reality in medical education: students’ experiences and learning outcomes, Medical Education Online, 26, 1, (2021); Doughty, Hazel, Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 6057-6066, (2018)","","IEEE Computer Society","","","","","","10772626","","ITVGE","37027743","English","Article","Final","","Scopus","2-s2.0-85149403925"
"H., Zhou, Haoyang; T., Hou, Teng; J., Li, Jitao","Zhou, Haoyang (58542159800); Hou, Teng (58759809500); Li, Jitao (58759781700)","58542159800; 58759809500; 58759781700","Prior Knowledge-guided Hierarchical Action Quality Assessment with 3D Convolution and Attention Mechanism","2023","Journal of Physics: Conference Series","2632","1","012027","","","0","1","10.1088/1742-6596/2632/1/012027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179582633&doi=10.1088%2F1742-6596%2F2632%2F1%2F012027&partnerID=40&md5=e5805c48bf2d75c86af8fe5d9e90e1e8","International Campus, Zhejiang University, Haining, China","Zhou, Haoyang, International Campus, Zhejiang University, Haining, China; Hou, Teng, International Campus, Zhejiang University, Haining, China; Li, Jitao, International Campus, Zhejiang University, Haining, China","Recently, there has been a growing interest in the field of computer vision and deep learning regarding a newly emerging problem known as action quality assessment (AQA). However, most researchers still rely on the traditional approach of using models from the video action recognition field. Unfortunately, this approach overlooks crucial features in AQA, such as movement fluency and degree of completion. Alternatively, some researchers have employed the transformer paradigm to capture action details and overall action integrity, but the high computational cost associated with transformers makes them impractical for real-time tasks. Due to the diversity of action types, it is challenging to rely solely on a shared model for quality assessment of various types of actions. To address these issues, we propose a novel network structure for AQA, which is the first to integrate multi-model capabilities through a classification model. Specifically, we utilize a pre-trained I3D model equipped with a self-attention block for classification. This allows us to evaluate various categories of actions using just one model. Furthermore, we introduce self-attention mechanisms and multi-head attention into the traditional convolutional neural network. By systematically replacing the last few layers of the conventional convolutional network, our model gains a greater ability to sense the global coordination of different actions. We have verified the effectiveness of our approach on the AQA-7 dataset. In comparison to other popular models, our model achieves satisfactory performance while maintaining a low computational cost. © 2023 Elsevier B.V., All rights reserved.","Computer Vision; Deep Learning; Neural Networks; Action Recognition; Attention Mechanisms; Computational Costs; Multi-modelling; Network Structures; Prior-knowledge; Quality Assessment; Real-time Tasks; Shared Model; Traditional Approaches; Convolution","Computer vision; Deep learning; Neural networks; Action recognition; Attention mechanisms; Computational costs; Multi-modelling; Network structures; Prior-knowledge; Quality assessment; Real-time tasks; Shared model; Traditional approaches; Convolution","","","","Attention is all You Need, (2017); Qiu, Zhaofan, Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 5534-5542, (2017); Zia, Aneeq, Temporal clustering of surgical activities in robot-assisted surgery, International Journal of Computer Assisted Radiology and Surgery, 12, 7, pp. 1171-1178, (2017); Parmar, Paritosh, What and how well you performed? a multitask learning approach to action quality assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 304-313, (2019); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Hu, Jie, Squeeze-and-Excitation Networks, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 7132-7141, (2018); Szegedy, Christian, Going deeper with convolutions, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 07-12-June-2015, pp. 1-9, (2015); He, Kaiming, Deep residual learning for image recognition, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December, pp. 770-778, (2016); Devlin, Jacob, BERT: Pre-training of deep bidirectional transformers for language understanding, 1, pp. 4171-4186, (2019); An Image is Worth 16x16 Words Transformers for Image Recognition at Scale, (2020)","","Institute of Physics","","2023 4th International Conference on Internet of Things, Artificial Intelligence and Mechanical Automation, IoTAIMA 2023","","Hybrid, Guangzhou","194443","17426588; 17426596","9788394593742; 9781628905861","","","English","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85179582633"
"G.A., Kumie, Gedamu Alemu; Y., Ji, Yanli; Y., Yang, Yang; J., Shao, Jie; H., Shen, Hengtao","Kumie, Gedamu Alemu (57221749042); Ji, Yanli (36677523000); Yang, Yang (57222954946); Shao, Jie (57002035900); Shen, Hengtao (7404523209)","57221749042; 36677523000; 57222954946; 57002035900; 7404523209","Fine-Grained Spatio-Temporal Parsing Network for Action Quality Assessment","2023","IEEE Transactions on Image Processing","32","","","6386","6400","0","14","10.1109/TIP.2023.3331212","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177080222&doi=10.1109%2FTIP.2023.3331212&partnerID=40&md5=ee6e1df9060f2af5b2444d678db02d9b","Sichuan Artificial Intelligence Research Institute, Yibin, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China","Kumie, Gedamu Alemu, Sichuan Artificial Intelligence Research Institute, Yibin, China, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Ji, Yanli, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China, University of Electronic Science and Technology of China, Chengdu, China; Yang, Yang, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Shao, Jie, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China, University of Electronic Science and Technology of China, Chengdu, China; Shen, Hengtao, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China","Action Quality Assessment (AQA) plays an important role in video analysis, which is applied to evaluate the quality of specific actions, i.e., sports activities. However, it is still challenging because there are lots of small action discrepancies with similar backgrounds, but current approaches mostly adopt holistic video representations. So that fine-grained intra-class variations are unable to be captured. To address the aforementioned challenge, we propose a Fine-grained Spatio-temporal Parsing Network (FSPN) which is composed of the intra-sequence action parsing module and spatiotemporal multiscale transformer module to learn fine-grained spatiotemporal sub-action representations for more reliable AQA. The intra-sequence action parsing module performs semantical sub-action parsing by mining sub-actions at fine-grained levels. It enables a correct description of the subtle differences between action sequences. The spatiotemporal multiscale transformer module learns motion-oriented action features and obtains their long-range dependencies among sub-actions at different scales. Furthermore, we design a group contrastive loss to train the model and learn more discriminative feature representations for sub-actions without explicit supervision. We exhaustively evaluate our proposed approach in the FineDiving, AQA-7, and MTL-AQA datasets. Extensive experiment results demonstrate the effectiveness and feasibility of our proposed approach, which outperforms the state-of-the-art methods by a significant margin. © 2023 Elsevier B.V., All rights reserved.","Action Parsing; Action Quality Assessment; Fine-grained Representation; Multiscale Transformer; Computer Vision; Job Analysis; Quality Control; Sports; Action Parsing; Action Quality Assessment; Decoding; Fine Grained; Fine-grained Representation; Multiscale Transformer; Quality Assessment; Spatiotemporal Phenomenon; Task Analysis; Transformer; Semantics; Article; Diagnosis; Human; Mining; Motion; Quality Control; Videorecording","Computer vision; Job analysis; Quality control; Sports; Action parsing; Action quality assessment; Decoding; Fine grained; Fine-grained representation; Multiscale transformer; Quality assessment; Spatiotemporal phenomenon; Task analysis; Transformer; Semantics; article; diagnosis; human; mining; motion; quality control; videorecording","","","This work was supported in part by the Ministry of Science and Technology of China, Foreign Young Talents Program under Grant QN2023036003L","Zhang, Yu M., Learning time-aware features for action quality assessment, Pattern Recognition Letters, 158, pp. 104-110, (2022); Pan, Jiahui, Action assessment by joint relation graphs, Proceedings of the IEEE International Conference on Computer Vision, pp. 6330-6339, (2019); Parmar, Paritosh, What and how well you performed? a multitask learning approach to action quality assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 304-313, (2019); Parmar, Paritosh, Learning to Score Olympic Events, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2017-July, pp. 76-84, (2017); Tang, Yansong, Uncertainty-Aware Score Distribution Learning for Action Quality Assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 9836-9845, (2020); Shao, Dian, Intra- and inter-action understanding via temporal action parsing, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 727-736, (2020); Li, Zhenqiang, Manipulation-skill assessment from videos with spatial attention network, pp. 4385-4395, (2019); Doughty, Hazel, Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 6057-6066, (2018); Wang, Shunli, TSA-Net: Tube Self-Attention Network for Action Quality Assessment, pp. 4902-4910, (2021); Doughty, Hazel, The pros and cons: Rank-aware temporal attention for skill determination in long videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7854-7863, (2019)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","10577149","","IIPRE","37963006","English","Article","Final","","Scopus","2-s2.0-85177080222"
"M., Chariar, Mukundan; S., Rao, Shreyas; A., Irani, Aryan; S., Suresh, Shilpa; C.S., Asha, C. S.","Chariar, Mukundan (58629585600); Rao, Shreyas (58629795200); Irani, Aryan (58628759000); Suresh, Shilpa (57213645921); Asha, C. S. (57191091621)","58629585600; 58629795200; 58628759000; 57213645921; 57191091621","AI Trainer: Autoencoder Based Approach for Squat Analysis and Correction","2023","IEEE Access","11","","","107135","107149","0","18","10.1109/ACCESS.2023.3316009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173050326&doi=10.1109%2FACCESS.2023.3316009&partnerID=40&md5=b4680374d2626417003b83b82de4300e","Manipal Institute of Technology, Manipal, India","Chariar, Mukundan, Department of Mechatronics, Manipal Institute of Technology, Manipal, India; Rao, Shreyas, Department of Mechatronics, Manipal Institute of Technology, Manipal, India; Irani, Aryan, Department of Mechatronics, Manipal Institute of Technology, Manipal, India; Suresh, Shilpa, Department of Mechatronics, Manipal Institute of Technology, Manipal, India; Asha, C. S., Department of Mechatronics, Manipal Institute of Technology, Manipal, India","Artificial intelligence and computer vision have widespread applications in workout analysis. It has been extensively used in sports and the athlete industry to identify errors and improve performance. Furthermore, these methods prevent injuries caused by a lack of instructors or costly infrastructure. One such exercise is the squat, which is a movement in which a standing person descends to a posture with their torso vertical and their knees firmly bent, then returns to their original upright position. Each person's squat is distinct, with varying limb lengths causing their form to change when observed. It has been observed that the mobility of various joints and muscular strength have a role in this. A squat improves the user by increasing overall leg strength, strengthening knee and hip joints, and lowering the risk of heart disease due to cardiovascular development. This paper presents a method for classifying squat types and recommending the right squat version. This study uses MediaPipe and a deep learning-based technique to decide if squatting is good or bad. A stacked Bidirectional Gated Recurrent Unit (Bi-GRU) model with an attention layer is proposed to consistently and fairly assess each user, categorizing squats into seven classes. This stacked Bi-GRU model with an attention unit is then compared to other cutting-edge models, both with and without the attention layer. The model outperforms other models by attaining an accuracy of 94% and is demonstrated to work the best and most consistently for our dataset. Furthermore, the individual executing the incorrect squat is corrected to the best of their ability, depending on their performance and body proportions, by providing the correct form. © 2023 Elsevier B.V., All rights reserved.","Action Quality Assessment; Attention; Computer Vision; Curve Fitting; Gated Recurrent Unit; Pose Estimation; Squat; Computer Vision; Curve Fitting; Deep Learning; Intelligent Robots; Joints (anatomy); Motion Analysis; Three Dimensional Computer Graphics; Three Dimensional Displays; Action Quality Assessment; Attention; Curves Fittings; Gated Recurrent Unit; Hidden-markov Models; Human Activity Recognition; Pose-estimation; Quality Assessment; Squat; Three-dimensional Display; Video; Hidden Markov Models","Computer vision; Curve fitting; Deep learning; Intelligent robots; Joints (anatomy); Motion analysis; Three dimensional computer graphics; Three dimensional displays; Action quality assessment; Attention; Curves fittings; Gated recurrent unit; Hidden-Markov models; Human activity recognition; Pose-estimation; Quality assessment; Squat; Three-dimensional display; Video; Hidden Markov models","","","","Usquat Adetective and Corrective Exercise Assistant Using Computer Vision and Machine Learning, (2021); AI Fitness Trainer Using Mediapipe; Squat Depth; Squat Evaluation; Cao, Zhe, Realtime multi-person 2D pose estimation using part affinity fields, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 1302-1310, (2017); Squat Classification and Counting; Sun, Ke, Deep high-resolution representation learning for human pose estimation, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 5686-5696, (2019); Standard Squat Posture Classifier; Cypherics Ivu; Ogata, Ryoji, Temporal distance matrices for squat classification, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2019-June, pp. 2533-2542, (2019)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85173050326"
"L., Yao, Long; Q., Lei, Qing; H., Zhang, Hongbo; J., Du, Jixiang; S., Gao, Shangce","Yao, Long (58624245800); Lei, Qing (23091470800); Zhang, Hongbo (56945874300); Du, Jixiang (14017783700); Gao, Shangce (23491827300)","58624245800; 23091470800; 56945874300; 14017783700; 23491827300","A Contrastive Learning Network for Performance Metric and Assessment of Physical Rehabilitation Exercises","2023","IEEE Transactions on Neural Systems and Rehabilitation Engineering","31","","","3790","3802","0","13","10.1109/TNSRE.2023.3317411","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172740083&doi=10.1109%2FTNSRE.2023.3317411&partnerID=40&md5=3e5a186b3ce17df906fdf410cf949741","Huaqiao University, Quanzhou, China; Huaqiao University, Quanzhou, China; University of Toyama, Toyama, Japan","Yao, Long, Department of Computer Science and Technology, Huaqiao University, Quanzhou, China; Lei, Qing, Department of Computer Science and Technology, Huaqiao University, Quanzhou, China; Zhang, Hongbo, Huaqiao University, Quanzhou, China; Du, Jixiang, Huaqiao University, Quanzhou, China; Gao, Shangce, University of Toyama, Toyama, Japan","Human activity analysis in the legal monitoring environment plays an important role in the physical rehabilitation field, as it helps patients with physical injuries improve their postoperative conditions and reduce their medical costs. Recently, several deep learning-based action quality assessment (AQA) frameworks have been proposed to evaluate physical rehabilitation exercises. However, most of them treat this problem as a simple regression task, which requires both the action instance and its score label as input. This approach is limited by the fact that the annotations in this field usually consist of healthy or unhealthy labels rather than quality scores provided by professional physicians. Additionally, most of these methods cannot provide informative feedback on a patient's motion defects, which weakens their practical application. To address these problems, we propose a multi-task contrastive learning framework to learn subtle and critical differences from skeleton sequences to deal with the performance metric and AQA problems of physical rehabilitation exercises. Specifically, we propose a performance metric network that takes triplets of training samples as input for score generation. For the AQA task, the same contrast learning strategy is used, but pairwise training samples are fed into the action quality assessment network for score prediction. Notably, we propose quantifying the deviation of the joint attention matrix between different skeleton sequences and introducing it into the loss function of our learning network. It is proven that considering both score prediction loss and joint attention deviation loss improves physical exercises AQA performance. Furthermore, it helps to obtain informative feedback for patients to improve their motion defects by visualizing the joint attention matrix's difference. The proposed method is verified on the UI-PRMD and KIMORE datasets. Experimental results show that the proposed method achieves state-of-the-art performance. © 2023 Elsevier B.V., All rights reserved.","Action Quality Assessment; Contrastive Learning; Informative Feedback; Performance Metric Quantification; Physical Rehabilitation Exercise; Deep Learning; Defects; Feature Extraction; Job Analysis; Patient Rehabilitation; Quality Control; Sampling; Sports; Action Quality Assessment; Contrastive Learning; Features Extraction; Informative Feedback; Performance Metric Quantification; Performance Metrices; Physical Rehabilitation; Physical Rehabilitation Exercise; Quality Assessment; Rehabilitation Exercise; Skeleton; Task Analysis; Musculoskeletal System; Adult; Article; Attention; Exercise; Feature Extraction; Human; Learning; Loss Of Function Mutation; Motion; Performance Indicator; Physician; Prediction; Quality Control; Rehabilitation; Skeleton; Sport; Kinesiotherapy; Exercise; Exercise Therapy; Humans; Motion","Deep learning; Defects; Feature extraction; Job analysis; Patient rehabilitation; Quality control; Sampling; Sports; Action quality assessment; Contrastive learning; Features extraction; Informative feedback; Performance metric quantification; Performance metrices; Physical rehabilitation; Physical rehabilitation exercise; Quality assessment; Rehabilitation exercise; Skeleton; Task analysis; Musculoskeletal system; adult; article; attention; exercise; feature extraction; human; learning; loss of function mutation; motion; performance indicator; physician; prediction; quality control; rehabilitation; skeleton; sport; kinesiotherapy; Exercise; Exercise Therapy; Humans; Motion","","","This work was supported in part by the National Natural Science Foundation of China under Grant 62001176 and Grant 61871196; in part by the Natural Science Foundation of Fujian Province, China, under Grant 2020J01085 and Grant 2019J01082; in part by the National Key Research and Development Program of China under Grant 2019YFC1604700; in part by the Promotion Program for Young and Middle-Aged Teacher in Science and Technology Research of Huaqiao University under Grant ZQN-YX601; and in part by the Japan Society for the Promotion of Science (JSPS) KAKENHI under Grant JP22H03643.","Yu, Bruce X.B., Skeleton-based human action evaluation using graph convolutional network for monitoring Alzheimer's progression, Pattern Recognition, 119, (2021); Parmar, Paritosh, Learning to Score Olympic Events, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2017-July, pp. 76-84, (2017); Roditakis, Konstantinos, Towards Improved and Interpretable Action Quality Assessment with Self-Supervised Alignment, ACM International Conference Proceeding Series, pp. 507-513, (2021); Xu, Angchi, Likert Scoring with Grade Decoupling for Long-term Action Assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2022-June, pp. 3222-3231, (2022); Pirsiavash, Hamed, Assessing the quality of actions, Lecture Notes in Computer Science, 8694 LNCS, PART 6, pp. 556-571, (2014); Pan, Jiahui, Action assessment by joint relation graphs, Proceedings of the IEEE International Conference on Computer Vision, pp. 6330-6339, (2019); Tran, Du, Learning spatiotemporal features with 3D convolutional networks, Proceedings of the IEEE International Conference on Computer Vision, 2015 International Conference on Computer Vision, ICCV 2015, pp. 4489-4497, (2015); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Tang, Yansong, Uncertainty-Aware Score Distribution Learning for Action Quality Assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 9836-9845, (2020); Pan, Jiahui, Adaptive Action Assessment, IEEE Transactions on Pattern Analysis and Machine Intelligence, 44, 12, pp. 8779-8795, (2022)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","15580210; 15344320","","ITNSB","37729572","English","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85172740083"
"W., Sun, Wenhao; Y., Hu, Yanxiang; B., Zhang, Bo; X., Chen, Xinran; C., Hao, Caixia; Y., Gao, Yaru","Sun, Wenhao (57201426452); Hu, Yanxiang (57207076458); Zhang, Bo (57191472154); Chen, Xinran (58577577400); Hao, Caixia (58577105700); Gao, Yaru (58577577500)","57201426452; 57207076458; 57191472154; 58577577400; 58577105700; 58577577500","A novel blind action quality assessment based on Multi-headed GRU network and attention mechanism","2023","Proceedings of SPIE - The International Society for Optical Engineering","12717","","1271737","","","0","0","10.1117/12.2685368","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171288635&doi=10.1117%2F12.2685368&partnerID=40&md5=04429bad7430bc8d7f9b333dbcaf7f05","Tianjin Normal University, Tianjin, China","Sun, Wenhao, College of Computer and Information Engineering, Tianjin Normal University, Tianjin, China; Hu, Yanxiang, College of Computer and Information Engineering, Tianjin Normal University, Tianjin, China; Zhang, Bo, College of Computer and Information Engineering, Tianjin Normal University, Tianjin, China; Chen, Xinran, College of Computer and Information Engineering, Tianjin Normal University, Tianjin, China; Hao, Caixia, College of Computer and Information Engineering, Tianjin Normal University, Tianjin, China; Gao, Yaru, College of Computer and Information Engineering, Tianjin Normal University, Tianjin, China","Objective action quality assessment (AQA) is a complex machine vision task because existing AQA assessment models can’t effectively fit the subjective assessment. To address this issue, we propose a novel blind action quality assessment method. By processing the video data with spatial and temporal features, the performance of the model is effectively improved. In addition, we also proposed a new loss function to better train the model, which combines the information entropy of the data. Finally, the experimental results show that on the existing datasets AQA-7 and JIGSAWS are significantly improved, reaching 0.63 and 0.57, respectively. © 2023 Elsevier B.V., All rights reserved.","Action Quality Assessment; Attention Mechanism; Deep Learning; Recurrent Neural Network; Data Handling; Action Quality Assessment; Assessment Models; Attention Mechanisms; Complex Machines; Deep Learning; Machine-vision; Network Mechanism; Quality Assessment; Subjective Assessments; Video Data; Recurrent Neural Networks","Data handling; Action quality assessment; Assessment models; Attention mechanisms; Complex machines; Deep learning; Machine-vision; Network mechanism; Quality assessment; Subjective assessments; Video data; Recurrent neural networks","","","","Feichtenhofer, Christoph, Convolutional Two-Stream Network Fusion for Video Action Recognition, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December, pp. 1933-1941, (2016); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Xiang, Xiang, S3D: Stacking Segmental P3D for Action Quality Assessment, Proceedings - International Conference on Image Processing, ICIP, pp. 928-932, (2018); Qiu, Zhaofan, Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 5534-5542, (2017); Parmar, Paritosh, What and how well you performed? a multitask learning approach to action quality assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 304-313, (2019); Parmar, Paritosh, Action quality assessment across multiple actions, pp. 1468-1476, (2019); Pan, Jiahui, Action assessment by joint relation graphs, Proceedings of the IEEE International Conference on Computer Vision, pp. 6330-6339, (2019); Seshadrinathan, Kalpana, Temporal hysteresis model of time varying subjective video quality, Proceedings - ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 1153-1156, (2011); Proceedings of AI ED, (1995); Tang, Yansong, Uncertainty-Aware Score Distribution Learning for Action Quality Assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 9836-9845, (2020)","Yang, S.X.; Karras, D.A.","SPIE","Academic Exchange Information Centre (AEIC); National and Kapodistrian University of Athens","3rd International Conference on Artificial Intelligence, Automation, and High-Performance Computing, AIAHPC 2023","","Wuhan","191090","0277786X; 1996756X","9781510692657; 9781510690561; 9781510693302; 9781510692251; 9781510692275; 9781510693081; 9781510688728; 9781510688629; 9781510692671; 9781510693326","PSISD","","English","Conference paper","Final","","Scopus","2-s2.0-85171288635"
"G., Liu, Guozhong; J., Wang, Jian; Z., Zhang, Zhibo; Q., Liu, Qingyi; Y., Ren, Yande; M., Zhang, Mengjiao; S., Chen, Shan; P., Bai, Peirui","Liu, Guozhong (58025237700); Wang, Jian (59863130600); Zhang, Zhibo (58025237600); Liu, Qingyi (55860259300); Ren, Yande (56643622500); Zhang, Mengjiao (58093157600); Chen, Shan (58092736400); Bai, Peirui (15065167200)","58025237700; 59863130600; 58025237600; 55860259300; 56643622500; 58093157600; 58092736400; 15065167200","A Novel Model for Intelligent Pull-Ups Test Based on Key Point Estimation of Human Body and Equipment","2023","Mobile Information Systems","2023","","3649217","","","0","8","10.1155/2023/3649217","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147509643&doi=10.1155%2F2023%2F3649217&partnerID=40&md5=c81170f31300e528801fc6088733132b","Shandong University of Science and Technology, Qingdao, China; Shandong University of Science and Technology, Qingdao, China; Qingdao University, Qingdao, China","Liu, Guozhong, Department of Physical Education, Shandong University of Science and Technology, Qingdao, China; Wang, Jian, College of Electronic and Information Engineering, Shandong University of Science and Technology, Qingdao, China; Zhang, Zhibo, College of Electronic and Information Engineering, Shandong University of Science and Technology, Qingdao, China; Liu, Qingyi, College of Electronic and Information Engineering, Shandong University of Science and Technology, Qingdao, China; Ren, Yande, Department of Radiology, Qingdao University, Qingdao, China; Zhang, Mengjiao, College of Electronic and Information Engineering, Shandong University of Science and Technology, Qingdao, China; Chen, Shan, College of Electronic and Information Engineering, Shandong University of Science and Technology, Qingdao, China; Bai, Peirui, College of Electronic and Information Engineering, Shandong University of Science and Technology, Qingdao, China","Applying computer vision and machine learning techniques into sport tests is an effective way to realize ""intelligent sports.""Facing practical application, we design a real-time and lightweight deep learning network to realize intelligent pull-ups test in this study. The main contributions are as follows: (1) a new self-produced pull-ups dataset is established under the requirement of including a human body and horizontal bar. In addition, a semiautomatic annotating software is developed to enhance annotation efficiency and increase labeling accuracy. (2) A novel lightweight deep network named PEPoseNet is designed to estimate and analyze a human pose in real time. The backbone of the network is made up of the heatmap network and key point network, which conduct human pose estimation based on the key points extracted from a human body and horizontal bar. The depth-wise separable convolution is adopted to speed up the training and convergence. (3) An evaluation criterion of intelligent pull-ups test is defined based on action quality assessment (AQA). The action quality of five states, i.e., ready or end, hang, pull, achieved, and resume in one pull-ups test cycle is automatically graded using a random forest classifier. A mobile application is developed to realize intelligent pull-ups test in real time. The performance of the proposed model and software is confirmed by verification and ablation experiments. The experimental results demonstrated that the proposed PEPoseNet has competitive performance to the state of the art. Its PCK @ 0.2 and frames per second (FPS) achieved were 83.8 and 30 fps, respectively. The mobile application has promising application prospects in pull-ups test under complex scenarios. © 2023 Elsevier B.V., All rights reserved.","Computer Vision; Deep Learning; Forestry; Learning Systems; Mobile Computing; Verification; Horizontal Bars; Human Bodies; Keypoints; Labeling Accuracies; Learning Network; Machine Learning Techniques; Mobile Applications; Point Estimation; Real- Time; Vision Learning; Sports","Computer vision; Deep learning; Forestry; Learning systems; Mobile computing; Verification; Horizontal bars; Human bodies; Keypoints; Labeling accuracies; Learning network; Machine learning techniques; Mobile applications; Point estimation; Real- time; Vision learning; Sports","","","","Zhou, Donghua, Research on the development situation and countermeasures of Intelligent Equestrian Sport in China, pp. 592-595, (2021); Xiao, Ningning, Wearable heart rate monitoring intelligent sports bracelet based on Internet of things, Measurement: Journal of the International Measurement Confederation, 164, (2020); Zhang, Wei, Blockchain-based decentralized federated transfer learning methodology for collaborative machinery fault diagnosis, Reliability Engineering and System Safety, 229, (2023); Li, Xiang, Remaining Useful Life Prediction With Partial Sensor Malfunctions Using Deep Adversarial Networks, IEEE/CAA Journal of Automatica Sinica, 10, 1, pp. 121-134, (2023); Bai, Peirui, A 3D Multi-domain U-Net Model for Intracranial Aneurysms Detecting, pp. 529-533, (2022); Badiola, Aritz, A systematic review of the application of camera-based human pose estimation in the field of sport and physical exercise, Sensors, 21, 18, (2021); Lin, Huei Yung, Augmented reality with human body interaction based on monocular 3D pose estimation, Lecture Notes in Computer Science, 6474 LNCS, PART 1, pp. 321-331, (2010); Stenum, Jan, Applications of pose estimation in human health and performance across the lifespan, Sensors, 21, 21, (2021); Li, Ying, Human pose estimation based in-home lower body rehabilitation system, Proceedings of the International Joint Conference on Neural Networks, (2020); Li, Miaopeng, Cross Refinement Techniques for Markerless Human<?brk?> Motion Capture, ACM Transactions on Multimedia Computing, Communications and Applications, 16, 1, (2020)","","Hindawi Limited","","","","","","1875905X; 1574017X","","","","English","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85147509643"
"Y., Ren, Yuhao; B., Zhang, Bochao; J., Chen, Jing; L., Guo, Liquan; J., Wang, Jiping","Ren, Yuhao (57803927200); Zhang, Bochao (57432311300); Chen, Jing (57203335122); Guo, Liquan (36701256000); Wang, Jiping (55603246700)","57803927200; 57432311300; 57203335122; 36701256000; 55603246700","An Efficient Motion Registration Method Based on Self-Coordination and Self-Referential Normalization","2022","Electronics (Switzerland)","11","19","3051","","","0","0","10.3390/electronics11193051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139926869&doi=10.3390%2Felectronics11193051&partnerID=40&md5=0cd90a83da2657bbbdbab3d3eaf9ddb8","University of Science and Technology of China, Hefei, China; Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, Suzhou, China","Ren, Yuhao, Division of Life Sciences and Medicine, University of Science and Technology of China, Hefei, China, Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, Suzhou, China; Zhang, Bochao, Division of Life Sciences and Medicine, University of Science and Technology of China, Hefei, China, Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, Suzhou, China; Chen, Jing, Division of Life Sciences and Medicine, University of Science and Technology of China, Hefei, China, Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, Suzhou, China; Guo, Liquan, Division of Life Sciences and Medicine, University of Science and Technology of China, Hefei, China, Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, Suzhou, China; Wang, Jiping, Division of Life Sciences and Medicine, University of Science and Technology of China, Hefei, China","Action quality assessment (AQA) is an important problem in computer vision applications. During human AQA, differences in body size or changes in position relative to the sensor may cause unwanted effects. We propose a motion registration method based on self-coordination (SC) and self-referential normalization (SRN). By establishing a coordinate system on the human body and using a part of the human body as a normalized reference standard to process the raw data, the standardization and distinguishability of the raw data are improved. To demonstrate the effectiveness of our method, we conducted experiments on KTH datasets. The experimental results show that the method improved the classification accuracy of the KNN-DTW network for KTH-5 from 82.46% to 87.72% and for KTH-4 from 89.47% to 94.74%, and it improved the classification accuracy of the tsai-MiniRocket network for KTH-5 from 91.29% to 93.86% and for KTH-4 from 94.74% to 97.90%. The results show that our method can reduce the above effects and improve the action classification accuracy of the action classification network. This study provides a new method and idea for improving the accuracy of AQA-related algorithms. © 2022 Elsevier B.V., All rights reserved.","Action Feature Fusion Method; Aqa; Computer Vision; Motion Registration; Sc; Srn","","","","This research was funded by SUZHOU SCIENCE AND TECHNOLOGY PLAN, grant number SS201851 and Suzhou Special Technical Project for Diagnosis and Treatment of Key Clinical Diseases, grant number LCZX201931.","Lei, Qing, A survey of vision-based human action evaluation methods, Sensors, 19, 19, (2019); Parmar, Paritosh, Action quality assessment across multiple actions, pp. 1468-1476, (2019); Lei, Qing, Temporal attention learning for action quality assessment in sports video, Signal, Image and Video Processing, 15, 7, pp. 1575-1583, (2021); Sahoo, Suraj Prakash, On an algorithm for human action recognition, Expert Systems with Applications, 115, pp. 524-534, (2019); Tran, Du, Learning spatiotemporal features with 3D convolutional networks, Proceedings of the IEEE International Conference on Computer Vision, 2015 International Conference on Computer Vision, ICCV 2015, pp. 4489-4497, (2015); Batchuluun, Ganbayar, Action recognition from thermal videos, IEEE Access, 7, pp. 103893-103917, (2019); Wang, Jiaming, Spatial–temporal pooling for action recognition in videos, Neurocomputing, 451, pp. 265-278, (2021); Zhang, Shumei, Real-time human posture recognition using an adaptive hybrid classifier, International Journal of Machine Learning and Cybernetics, 12, 2, pp. 489-499, (2021); Tang, Xianlun, Selective spatiotemporal features learning for dynamic gesture recognition, Expert Systems with Applications, 169, (2021); Zhong, Fujin, DSPNet: A low computational-cost network for human pose estimation, Neurocomputing, 423, pp. 327-335, (2021)","","MDPI","","","","","","20799292","","","","English","Article","Final","All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85139926869"
"S., Zhang, Shaojie; J., Pan, Jiahui; J., Gao, Jibin; W., Zheng, Wei-Shi","Zhang, Shaojie (57423048500); Pan, Jiahui (55355956000); Gao, Jibin (57214861263); Zheng, Wei-Shi (25928152800)","57423048500; 55355956000; 57214861263; 25928152800","Semi-Supervised Action Quality Assessment With Self-Supervised Segment Feature Recovery","2022","IEEE Transactions on Circuits and Systems for Video Technology","32","9","","6017","6028","0","29","10.1109/TCSVT.2022.3143549","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123295419&doi=10.1109%2FTCSVT.2022.3143549&partnerID=40&md5=25a35c92d504028d75827bc50509a778","Sun Yat-Sen University, Guangzhou, China; Sun Yat-Sen University, Guangzhou, China; Peng Cheng Laboratory, Shenzhen, China","Zhang, Shaojie, School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China; Pan, Jiahui, School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China; Gao, Jibin, School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China; Zheng, Wei-Shi, Key Laboratory of Machine Intelligence and Advanced Computing, Sun Yat-Sen University, Guangzhou, China, Peng Cheng Laboratory, Shenzhen, China","Action Quality Assessment aims to evaluate how well an action performs. Existing methods have achieved remarkable progress on fully-supervised action assessment. However, in real-world applications, with expert's experience, it is not always feasible to manually label all samples. Therefore, it is important to study the problem of semi-supervised action assessment with only a small amount of samples annotated. A major challenge for semi-supervised action assessment is how to exploit the temporal pattern from unlabeled videos. Inspired by the temporal dependencies of the action execution, we propose a self-supervised learning on the unlabeled videos by recovering the feature of a masked segment of an unlabeled video. Furthermore, we leverage adversarial learning to align the representation distribution of the labeled and the unlabeled samples to close their gap in the sample space since unlabeled samples always come from unseen actions. Finally, we propose an adversarial self-supervised framework for semi-supervised action quality assessment. The extensive experimental results on the MTL-AQA and the Rhythmic Gymnastics datasets will demonstrate the effectiveness of our framework, achieving the state-of-the-art performances of semi-supervised action quality assessment. © 2024 Elsevier B.V., All rights reserved.","Action Quality Assessment; Semi-supervised Learning; Computer Vision; Action Assessment; Action Execution; Action Quality Assessment; Feature Recovery; Quality Assessment; Real-world; Semi-supervised; Semi-supervised Learning; Temporal Pattern; Unlabeled Samples; Supervised Learning","Computer vision; Action assessment; Action execution; Action quality assessment; Feature recovery; Quality assessment; Real-world; Semi-supervised; Semi-supervised learning; Temporal pattern; Unlabeled samples; Supervised learning","","","This work was supported in part by the NSFC under Grant U21A20471, Grant U1911401, and Grant U1811461; in part by the Guangdong NSF Project under Grant 2020B1515120085 and Grant 2018B030312002; in part by the Guangzhou Research Project under Grant 201902010037; and in part by the KeyArea Research and Development Program of Guangzhou under Grant 202007030004.","Capecci, M., The KIMORE Dataset: KInematic Assessment of MOvement and Clinical Scores for Remote Monitoring of Physical REhabilitation, IEEE Transactions on Neural Systems and Rehabilitation Engineering, 27, 7, pp. 1436-1448, (2019); Liao, Yalin, A Deep Learning Framework for Assessing Physical Rehabilitation Exercises, IEEE Transactions on Neural Systems and Rehabilitation Engineering, 28, 2, pp. 468-477, (2020); Parmar, Paritosh, Learning to Score Olympic Events, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2017-July, pp. 76-84, (2017); Pan, Jiahui, Action assessment by joint relation graphs, Proceedings of the IEEE International Conference on Computer Vision, pp. 6330-6339, (2019); Gao, Jibin, An Asymmetric Modeling for Action Assessment, Lecture Notes in Computer Science, 12375 LNCS, pp. 222-238, (2020); Zeng, Lingan, Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos, pp. 2526-2534, (2020); Parmar, Paritosh, Action quality assessment across multiple actions, pp. 1468-1476, (2019); Tang, Yansong, Uncertainty-Aware Score Distribution Learning for Action Quality Assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 9836-9845, (2020); Pirsiavash, Hamed, Assessing the quality of actions, Lecture Notes in Computer Science, 8694 LNCS, PART 6, pp. 556-571, (2014); Doughty, Hazel, The pros and cons: Rank-aware temporal attention for skill determination in long videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7854-7863, (2019)","","Institute of Electrical and Electronics Engineers Inc.","","","","","","10518215","","ITCTE","","English","Article","Final","","Scopus","2-s2.0-85123295419"
"J., Li, Jianwei; H., Hu, Haiqing; Q., Xing, Qingjun; X., Wang, Xinyu; J., Li, Jinyang; Y., Shen, Yanfei","Li, Jianwei (57219228353); Hu, Haiqing (57984246100); Xing, Qingjun (57554389700); Wang, Xinyu (58393277000); Li, Jinyang (57998606700); Shen, Yanfei (57219224314)","57219228353; 57984246100; 57554389700; 58393277000; 57998606700; 57219224314","Tai Chi Action Quality Assessment and Visual Analysis with a Consumer RGB-D Camera","2022","","","","","","","0","4","10.1109/MMSP55362.2022.9949464","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143603875&doi=10.1109%2FMMSP55362.2022.9949464&partnerID=40&md5=3013070bc5a98e911d127c1a38067aa2","Beijing Sport University, Beijing, China","Li, Jianwei, School of Sports Engineering, Beijing Sport University, Beijing, China; Hu, Haiqing, School of Sports Engineering, Beijing Sport University, Beijing, China; Xing, Qingjun, School of Sports Engineering, Beijing Sport University, Beijing, China; Wang, Xinyu, School of Sports Engineering, Beijing Sport University, Beijing, China; Li, Jinyang, School of Sports Engineering, Beijing Sport University, Beijing, China; Shen, Yanfei, School of Sports Engineering, Beijing Sport University, Beijing, China","Visual-based human action analysis is an important research topic in the field of computer vision, and has great application prospect in intelligent sports. Home-based fitness is increasingly common in recent years, however lacking of accurate feedback and scientific guidance main easily lead to problems such as exercise injuries. In this paper, we propose an analysis system for Tai Chi action quality assessment and visual analysis with a consumer RGB-D camera. The main innovative work is as follows: 1) for home-based fitness action evaluation, we design a real-time intelligent analysis system combined with expert rules through a consumer RGB-D camera; 2) we transform the evaluation of 24-form Tai Chi Chuan into an artificial intelligence (AI) model, and realize action recognition and assessment through computer vision; 2) to train the AI model, we build a new dataset named TaiChi-24, which contains 1,408 samples with RGB-D images and 3D skeletons. We carry out evaluation experiments and analyses, and the experimental results have shown the advantage of applying our evaluation method on the proposed TaiChi-24 dataset. © 2022 Elsevier B.V., All rights reserved.","Action Quality Evaluation; Action Recognition; Sports Performance Analysis; Visual Analysis; Computer Vision; Quality Control; Sports; Action Quality Evaluation; Action Recognition; Home-based; Human Action Analysis; Intelligence Models; Quality Assessment; Quality Evaluation; Sports Performance Analysis; Tai Chis; Visual Analysis; Cameras","Computer vision; Quality control; Sports; Action quality evaluation; Action recognition; Home-based; Human action analysis; Intelligence models; Quality assessment; Quality evaluation; Sports performance analysis; Tai chis; Visual analysis; Cameras","","","This work is supported by the National Natural Science Foundation of China under Grant No.72071018, the Open Projects Program of National Laboratory of Pattern Recognition under Grant No.202100009, and the Fundamental Research Funds for Central Universities No.2021TD006.","Antunes, Michel, Visual and human-interpretable feedback for assisting physical activity, Lecture Notes in Computer Science, 9914 LNCS, pp. 115-129, (2016); Tao, Lili, A comparative study of pose representation and dynamics modelling for online motion quality assessment, Computer Vision and Image Understanding, 148, pp. 136-152, (2016); Baptista, Renato, Video-based feedback for assisting physical activity, 5, pp. 274-280, (2017); Pirsiavash, Hamed, Assessing the quality of actions, Lecture Notes in Computer Science, 8694 LNCS, PART 6, pp. 556-571, (2014); Patrona, Fotini, Motion analysis: Action detection, recognition and evaluation based on motion capture data, Pattern Recognition, 76, pp. 612-622, (2018); Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, (2017); Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, (2017); Li, Yongjun, ScoringNet: Learning Key Fragment for Action Quality Assessment with Ranking Loss in Skilled Sports, Lecture Notes in Computer Science, 11366 LNCS, pp. 149-164, (2019); Parmar, Paritosh, What and how well you performed? a multitask learning approach to action quality assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 304-313, (2019); McNally, William, GolfDB: A video database for golf swing sequencing, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2019-June, pp. 2553-2562, (2019)","","Institute of Electrical and Electronics Engineers Inc.","","24th IEEE International Workshop on Multimedia Signal Processing, MMSP 2022","","Shanghai","184573","","9781665471893","","","English","Conference paper","Final","","Scopus","2-s2.0-85143603875"
"X., Wang, Xinyu; J., Li, Jianwei; H., Hu, Haiqing","Wang, Xinyu (58393277000); Li, Jianwei (57219228353); Hu, Haiqing (57984246100)","58393277000; 57219228353; 57984246100","Skeleton-Based Action Quality Assessment via Partially Connected LSTM with Triplet Losses","2022","Lecture Notes in Computer Science","13536 LNCS","","","220","232","0","5","10.1007/978-3-031-18913-5_17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142791826&doi=10.1007%2F978-3-031-18913-5_17&partnerID=40&md5=fd2bd1cae62f30e2b2a650b156b72483","Beijing Sport University, Beijing, China","Wang, Xinyu, School of Sports Engineering, Beijing Sport University, Beijing, China; Li, Jianwei, School of Sports Engineering, Beijing Sport University, Beijing, China; Hu, Haiqing, School of Sports Engineering, Beijing Sport University, Beijing, China","Human action quality assessment (AQA) recently has attracted increasing attentions in computer vision for its practical applications, such as skill training, physical rehabilitation and scoring sports events. In this paper, we propose a partially connected LSTM with triplet losses to evaluate different skill levels. Compared to human action recognition (HAR), we explain and discuss two characteristics and countermeasures of AQA. To ignore the negative influence of complex joint movements in actions, the skeleton is not regarded as a single graph. The fully connected layer in the LSTM model is replaced by the partially connected layer, using a diagonal matrix which activates the corresponding weights, to explore hierarchical relations in the skeleton graph. Furthermore, to improve the generalization ability of models, we introduce additional functions of triplet loss to the loss function, which make samples with similar skill levels close to each other. We carry out experiments to test our model and compare it with seven LSTM architectures and three GNN architectures on the UMONS-TAICHI dataset and walking gait dataset. Experimental results demonstrate that our model achieves outstanding performance. © 2022 Elsevier B.V., All rights reserved.","Action Quality Assessment; Lstm; Skeleton Sequence; Triplet Loss; Long Short-term Memory; Statistical Tests; Action Quality Assessment; Human Actions; Lstm; Physical Rehabilitation; Quality Assessment; Skeleton Sequence; Skill Levels; Skill Training; Sports Events; Triplet Loss; Musculoskeletal System","Long short-term memory; Statistical tests; Action quality assessment; Human actions; LSTM; Physical rehabilitation; Quality assessment; Skeleton sequence; Skill levels; Skill training; Sports events; Triplet loss; Musculoskeletal system","","","Supported by the Open Projects Program of National Laboratory of Pattern Recognition under Grant No. 202100009, and the Fundamental Research Funds for Central Universities No. 2021TD006.","McNally, William, GolfDB: A video database for golf swing sequencing, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2019-June, pp. 2553-2562, (2019); Szczesna, Agnieszka, Optical motion capture dataset of selected techniques in beginner and advanced Kyokushin karate athletes, Scientific Data, 8, 1, (2021); Tits, Mickaël, UMONS-TAICHI: A multimodal motion capture dataset of expertise in Taijiquan gestures, Data in Brief, 19, pp. 1214-1221, (2018); Liao, Yalin, A Deep Learning Framework for Assessing Physical Rehabilitation Exercises, IEEE Transactions on Neural Systems and Rehabilitation Engineering, 28, 2, pp. 468-477, (2020); Capecci, M., The KIMORE Dataset: KInematic Assessment of MOvement and Clinical Scores for Remote Monitoring of Physical REhabilitation, IEEE Transactions on Neural Systems and Rehabilitation Engineering, 27, 7, pp. 1436-1448, (2019); Xu, Chengming, Learning to Score Figure Skating Sport Videos, IEEE Transactions on Circuits and Systems for Video Technology, 30, 12, pp. 4578-4590, (2020); Parmar, Paritosh, What and how well you performed? a multitask learning approach to action quality assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 304-313, (2019); Parmar, Paritosh, Learning to Score Olympic Events, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2017-July, pp. 76-84, (2017); Parmar, Paritosh, Action quality assessment across multiple actions, pp. 1468-1476, (2019); Pan, Jiahui, Action assessment by joint relation graphs, Proceedings of the IEEE International Conference on Computer Vision, pp. 6330-6339, (2019)","Yu, S.; Zhang, J.; Zhang, Z.; Tan, T.; Yuen, P.C.; Guo, Y.; Han, J.; Lai, J.","Springer Science and Business Media Deutschland GmbH","","5th Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2022","","Shenzhen","285979","16113349; 03029743","9789819698936; 9789819698042; 9789819698110; 9789819698905; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141; 9783031984136","","","English","Conference paper","Final","","Scopus","2-s2.0-85142791826"
"Y., Bai, Yang; D., Zhou, Desen; S., Zhang, Songyang; J., Wang, Jian; E., DIng, Errui; Y., Guan, Yu; Y., Long, Yang; J., Wang, Jingdong","Bai, Yang (57818545400); Zhou, Desen (57191078761); Zhang, Songyang (57215328161); Wang, Jian (57200617794); DIng, Errui (57194186284); Guan, Yu (55243023100); Long, Yang (57089680300); Wang, Jingdong (56301574900)","57818545400; 57191078761; 57215328161; 57200617794; 57194186284; 55243023100; 57089680300; 56301574900","Action Quality Assessment with Temporal Parsing Transformer","2022","Lecture Notes in Computer Science","13664 LNCS","","","422","438","0","45","10.1007/978-3-031-19772-7_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142743599&doi=10.1007%2F978-3-031-19772-7_25&partnerID=40&md5=ff61ef1cfd7a1e41b8dd1d0c72e108cd","Durham University, Durham, United Kingdom; Baidu, Inc., Beijing, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China; University of Warwick, Coventry, United Kingdom","Bai, Yang, Department of Computer Science, Durham University, Durham, United Kingdom, Baidu, Inc., Beijing, China; Zhou, Desen, Baidu, Inc., Beijing, China; Zhang, Songyang, Shanghai Artificial Intelligence Laboratory, Shanghai, China; Wang, Jian, Baidu, Inc., Beijing, China; DIng, Errui, Baidu, Inc., Beijing, China; Guan, Yu, University of Warwick, Coventry, United Kingdom; Long, Yang, Department of Computer Science, Durham University, Durham, United Kingdom; Wang, Jingdong, Baidu, Inc., Beijing, China","Action Quality Assessment(AQA) is important for action understanding and resolving the task poses unique challenges due to subtle visual differences. Existing state-of-the-art methods typically rely on the holistic video representations for score regression or ranking, which limits the generalization to capture fine-grained intra-class variation. To overcome the above limitation, we propose a temporal parsing transformer to decompose the holistic feature into temporal part-level representations. Specifically, we utilize a set of learnable queries to represent the atomic temporal patterns for a specific action. Our decoding process converts the frame representations to a fixed number of temporally ordered part representations. To obtain the quality score, we adopt the state-of-the-art contrastive regression based on the part representations. Since existing AQA datasets do not provide temporal part-level labels or partitions, we propose two novel loss functions on the cross attention responses of the decoder: a ranking loss to ensure the learnable queries to satisfy the temporal order in cross attention and a sparsity loss to encourage the part representations to be more discriminative. Extensive experiments show that our proposed method outperforms prior work on three public AQA benchmarks by a considerable margin. © 2025 Elsevier B.V., All rights reserved.","Action Quality Assessment; Contrastive Regression; Temporal Parsing Transformer; Temporal Patterns; Computer Vision; Regression Analysis; Action Quality Assessment; Contrastive Regression; Generalisation; Part Levels; Quality Assessment; State-of-the-art Methods; Temporal Parsing Transformer; Temporal Pattern; Video Representations; Visual Differences; Decoding","Computer vision; Regression analysis; Action quality assessment; Contrastive regression; Generalisation; Part levels; Quality assessment; State-of-the-art methods; Temporal parsing transformer; Temporal pattern; Video representations; Visual differences; Decoding","","","","Alayrac, Jean Baptiste, Joint Discovery of Object States and Manipulation Actions, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 2146-2155, (2017); Bertasius, Gedas, Am i a Baller? Basketball Performance Assessment from First-Person Videos, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 2196-2204, (2017); Zur Zuverlassigkeit Der Bemessung Von Biegebeanspruchten Betonbauteilen Mit Textiler Bewehrung, (2018); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Doughty, Hazel, Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 6057-6066, (2018); Doughty, Hazel, The pros and cons: Rank-aware temporal attention for skill determination in long videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7854-7863, (2019); Miccai Workshop M2cai, (2014); Proceedings of AI ED, (1995); Jug, Marko, Trajectory Based assessment of coordinated human activity, Lecture Notes in Computer Science, 2626, pp. 534-543, (2003); Kuehne, Hilde, The language of actions: Recovering the syntax and semantics of goal-directed human activities, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 780-787, (2014)","Avidan, S.; Brostow, G.; Cissé, M.; Farinella, G.M.; Hassner, T.","Springer Science and Business Media Deutschland GmbH","","17th European Conference on Computer Vision, ECCV 2022","","Tel Aviv","285469","16113349; 03029743","9789819698936; 9789819698042; 9789819698110; 9789819698905; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141; 9783031984136","","","English","Conference paper","Final","","Scopus","2-s2.0-85142743599"
"J., Xu, Jinglin; Y., Rao, Yongming; X., Yu, Xumin; G., Chen, Guangyi; J., Zhou, Jie; J., Lu, Jiwen","Xu, Jinglin (55751087900); Rao, Yongming (57200621657); Yu, Xumin (57220749597); Chen, Guangyi (57201580848); Zhou, Jie (59662779500); Lu, Jiwen (14065346100)","55751087900; 57200621657; 57220749597; 57201580848; 59662779500; 14065346100","FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","2939","2948","0","96","10.1109/CVPR52688.2022.00296","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134965134&doi=10.1109%2FCVPR52688.2022.00296&partnerID=40&md5=91ebc64e86912059c84247b59c9d336e","Tsinghua University, Beijing, China; Beijing National Research Center for Information Science and Technology, Beijing, China","Xu, Jinglin, Department of Automation, Tsinghua University, Beijing, China, Beijing National Research Center for Information Science and Technology, Beijing, China; Rao, Yongming, Department of Automation, Tsinghua University, Beijing, China, Beijing National Research Center for Information Science and Technology, Beijing, China; Yu, Xumin, Department of Automation, Tsinghua University, Beijing, China, Beijing National Research Center for Information Science and Technology, Beijing, China; Chen, Guangyi, Department of Automation, Tsinghua University, Beijing, China, Beijing National Research Center for Information Science and Technology, Beijing, China; Zhou, Jie, Department of Automation, Tsinghua University, Beijing, China, Beijing National Research Center for Information Science and Technology, Beijing, China; Lu, Jiwen, Department of Automation, Tsinghua University, Beijing, China, Beijing National Research Center for Information Science and Technology, Beijing, China","Most existing action quality assessment methods rely on the deep features of an entire video to predict the score, which is less reliable due to the non-transparent inference process and poor interpretability. We argue that understanding both high-level semantics and internal temporal structures of actions in competitive sports videos is the key to making predictions accurate and interpretable. Towards this goal, we construct a new fine-grained dataset, called FineDiving, developed on diverse diving events with detailed annotations on action procedures. We also propose a procedure-aware approach for action quality assessment, learned by a new Temporal Segmentation Attention module. Specifically, we propose to parse pairwise query and exemplar action instances into consecutive steps with diverse semantic and temporal correspondences. The procedure-aware cross-attention is proposed to learn embeddings between query and exemplar steps to discover their semantic, spatial, and temporal correspondences, and further serve for fine-grained contrastive regression to derive a reliable scoring mechanism. Extensive experiments demonstrate that our approach achieves substantial improvements over the state-of-the-art methods with better interpretability. The dataset and code are available at https://github.com/xujinglin/FineDiving. © 2022 Elsevier B.V., All rights reserved.","Action And Event Recognition; Video Analysis And Understanding; Computer Vision; Query Processing; Action Recognition; Event Recognition; Fine Grained; High Level Semantics; Inference Process; Interpretability; Quality Assessment; Temporal Structures; Video Analysis; Video Understanding; Semantics","Computer vision; Query processing; Action recognition; Event recognition; Fine grained; High level semantics; Inference process; Interpretability; Quality assessment; Temporal structures; Video analysis; Video understanding; Semantics","","","This work was supported in part by the National Natural Science Foundation of China under Grant 62125603, Grant 62106124, Grant U1813218, in part by China Postdoctoral Science Foundation under Grant 2020M680564, and in part by a grant from the Beijing Academy of Artificial Intelligence (BAAI).","Proceedings of the IEEE International Conference on Computer Vision, (2017); Cvpr, (2017); Sportscap Monocular 3d Human Motion Capture and Fine Grained Understanding in Challenging Sports Videos, (2025); An Image is Worth 16x16 Words Transformers for Image Recognition at Scale, (2020); Doughty, Hazel, Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 6057-6066, (2018); Doughty, Hazel, The pros and cons: Rank-aware temporal attention for skill determination in long videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7854-7863, (2019); Caba-Heilbron, Fabian, ActivityNet: A large-scale video benchmark for human activity understanding, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 07-12-June-2015, pp. 961-970, (2015); Feichtenhofer, Christoph, Convolutional Two-Stream Network Fusion for Video Action Recognition, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December, pp. 1933-1941, (2016); Gattupalli, Srujana, CogniLearn: A deep learning-based interface for cognitive behavior assessment, pp. 577-587, (2017); Thumos Challenge Action Recognition with A Large Number of Classes, (2015)","","IEEE Computer Society","","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022","","New Orleans; LA","183275","10636919","0769526462; 9798350301298; 9781665445092; 9798350353006; 9781467388511; 9781457703942; 9781728171685; 0818684976; 9781665469463; 1424411807","PIVRE","","English","Conference paper","Final","","Scopus","2-s2.0-85134965134"
"S., Farabi, Shafkat; H., Himel, Hasibul; F., Gazzali, Fakhruddin; M.B., Hasan, Md Bakhtiar; M.H., Kabir, Md Hasanul; M.R., Farazi, Moshiur R.","Farabi, Shafkat (57222311722); Himel, Hasibul (58345966300); Gazzali, Fakhruddin (57222312633); Hasan, Md Bakhtiar (57247978600); Kabir, Md Hasanul (15122132000); Farazi, Moshiur R. (56904118600)","57222311722; 58345966300; 57222312633; 57247978600; 15122132000; 56904118600","Improving Action Quality Assessment Using Weighted Aggregation","2022","Lecture Notes in Computer Science","13256 LNCS","","","576","587","0","8","10.1007/978-3-031-04881-4_46","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129869360&doi=10.1007%2F978-3-031-04881-4_46&partnerID=40&md5=c63cb1a356d8ab73fab436c8eef23038","Islamic University of Technology, Gazipur, Bangladesh; Commonwealth Scientific and Industrial Research Organisation, Canberra, Australia","Farabi, Shafkat, Islamic University of Technology, Gazipur, Bangladesh; Himel, Hasibul, Islamic University of Technology, Gazipur, Bangladesh; Gazzali, Fakhruddin, Islamic University of Technology, Gazipur, Bangladesh; Hasan, Md Bakhtiar, Islamic University of Technology, Gazipur, Bangladesh; Kabir, Md Hasanul, Islamic University of Technology, Gazipur, Bangladesh; Farazi, Moshiur R., Commonwealth Scientific and Industrial Research Organisation, Canberra, Australia","Action quality assessment (AQA) aims at automatically judging human action based on a video of the said action and assigning a performance score to it. The majority of works in the existing literature on AQA divide RGB videos into short clips, transform these clips to higher-level representations using Convolutional 3D (C3D) networks, and aggregate them through averaging. These higher-level representations are used to perform AQA. We find that the current clip level feature aggregation technique of averaging is insufficient to capture the relative importance of clip level features. In this work, we propose a learning-based weighted-averaging technique. Using this technique, better performance can be obtained without sacrificing too much computational resources. We call this technique Weight-Decider(WD). We also experiment with ResNets for learning better representations for action quality assessment. We assess the effects of the depth and input clip size of the convolutional neural network on the quality of action score predictions. We achieve a new state-of-the-art Spearman’s rank correlation of 0.9315 (an increase of 0.45%) on the MTL-AQA dataset using a 34 layer (2+1)D ResNet with the capability of processing 32 frame clips, with WD aggregation. © 2022 Elsevier B.V., All rights reserved.","Action Quality Assessment; Aggregation; Mtl-aqa; Computer Vision; Data Link Layer; Linearization; Neural Networks; 'current; 3d Networks; Action Quality Assessment; Clip Level; Feature Aggregation; Human Actions; Mtl-action Quality Assessment; Performance; Quality Assessment; Convolution","Computer vision; Data Link Layer; Linearization; Neural networks; 'current; 3D networks; Action quality assessment; Clip level; Feature aggregation; Human actions; MTL-action quality assessment; Performance; Quality assessment; Convolution","","","","Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); 600 V Power Device Technologies for Highly Efficient Power Supplies º in 2021 23rd European Conference on Power Electronics and Applications; Funke, Isabel, Video-based surgical skill assessment using 3D convolutional neural networks, International Journal of Computer Assisted Radiology and Surgery, 14, 7, pp. 1217-1225, (2019); undefined; Hara, Kensho, Learning spatio-Temporal features with 3D residual networks for action recognition, 2018-January, pp. 3154-3160, (2017); He, Kaiming, Deep residual learning for image recognition, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December, pp. 770-778, (2016); Hochreiter, Sepp, Long Short-Term Memory, Neural Computation, 9, 8, pp. 1735-1780, (1997); Karpathy, Andrej, Large-scale video classification with convolutional neural networks, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1725-1732, (2014); Corr, (2017); Kingma, Diederik P., Adam: A method for stochastic optimization, (2015)","Pinho, A.J.; Georgieva, P.; Teixeira, L.F.; Sánchez, J.A.","Springer Science and Business Media Deutschland GmbH","","10th Iberian Conference on Pattern Recognition and Image Analysis, IbPRIA 2022","","Aveiro","277289","16113349; 03029743","9789819698936; 9789819698042; 9789819698110; 9789819698905; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141; 9783031984136","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85129869360"
"J., Li, Jicheng; A.N., Bhat, Anjana Narayan; R.L., Barmaki, Roghayeh Leila","Li, Jicheng (57225221973); Bhat, Anjana Narayan (7102056594); Barmaki, Roghayeh Leila (57079124100)","57225221973; 7102056594; 57079124100","Improving the Movement Synchrony Estimation with Action Quality Assessment in Children Play Therapy","2021","","","","","397","406","0","17","10.1145/3462244.3479891","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119005184&doi=10.1145%2F3462244.3479891&partnerID=40&md5=8f9842fbe24daf24adf99ba8d4f30ee4","University of Delaware, Newark, United States","Li, Jicheng, University of Delaware, Newark, United States; Bhat, Anjana Narayan, University of Delaware, Newark, United States; Barmaki, Roghayeh Leila, University of Delaware, Newark, United States","Movement synchrony refers to the dynamic temporal connection between the motions of interacting people. The automatic measurement of movement synchrony is worth studying for social behavior analysis applications, for instance, in play therapy of children in the autism spectrum. Existing approaches based on motion energy analysis are strongly reliant on the region of interest, and thus limit the interaction between individuals, especially for highly engaging activities like play therapy. Inspired by action quality assessment, a task to assess how well an action has been performed, in this paper, we propose an end-to-end deep learning method to integrate the following major tasks: (1) the automatic assessment of children's performance in play therapy, and (2) the automatic estimation of movement synchrony between children and therapists, facilitated by an auxiliary task of intervention activity recognition. This multi-task paradigm generally improves the performance of our model across all tasks. Furthermore, when annotations are subjective, the typical exclusive annotation strategy may reduce tagging quality. As a result, we explored applying distribution learning to mitigate human bias in movement synchrony estimation. We allowed the second and third labels for each instance, namely the uncertainty-preserved annotation approach. We tested our method on Play Therapy 13 (PT13), a dataset collected from video recordings of play therapy interventions. The findings of the experiments indicated that our framework can accurately quantify movement synchronization and assess the quality of children's actions in play therapy. Moreover, the uncertainty-preserved annotation approach produced a comparable outcome to standard methods at a far reduced cost, demonstrating its efficacy in mitigating biases. © 2021 Elsevier B.V., All rights reserved.","Action Quality Assessment; Autism; Deep Learning; Movement Synchrony; Multi-task Learning; Play Therapy; Visual Behavior Analysis; Deep Learning; Image Segmentation; Quality Control; Social Behavior; Video Recording; Action Quality Assessment; Behavior Analysis; Movement Synchrony; Performance; Play Therapy; Quality Assessment; Uncertainty; Visual Behavior; Visual Behavior Analyse; Diseases","Deep learning; Image segmentation; Quality control; Social behavior; Video recording; Action quality assessment; Behavior analysis; Movement synchrony; Performance; Play therapy; Quality assessment; Uncertainty; Visual behavior; Visual behavior analyse; Diseases","","","This work is supported by the Amazon Research Awards Program, the National Institutes of Mental Health (NIMH, 5R21MH089441-02, 4R33MH089441-03), and Autism Speaks (Grant #8137). We wish to acknowledge efforts of Dr. Kangsoo Kim for his insightful suggestions throughout the project. We also thank the entire research team, study participants, and caregivers for their contribution to data collection and annotation. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.","Altmann, Uwe, Movement Synchrony and Facial Synchrony as Diagnostic Features of Depression: A Pilot Study, Journal of Nervous and Mental Disease, 209, 2, pp. 128-136, (2021); Altmann, Uwe, Associations between movement synchrony and outcome in patients with social anxiety disorder: Evidence for treatment specific effects, Psychotherapy Research, 30, 5, pp. 574-590, (2020); Diagnostic and Statistical Manual of Mental Disorders, (2000); Barmaki, Roghayeh Leila, Multimodal assessment of teaching behavior in immersive rehearsal environment - TeachLivE™, pp. 651-655, (2015); Barmaki, Roghayeh Leila, Gesturing and embodiment in teaching: Investigating the nonverbal behavior of teachers in a virtual rehearsal environment, pp. 7893-7899, (2018); Barmaki, Roghayeh Leila, Providing real-time feedback for student teachers in a virtual rehearsal environment, pp. 531-537, (2015); Barsoum, Emad, Training deep networks for facial expression recognition with crowd-sourced label distribution, pp. 279-283, (2016); 2020 Combined Sections Meeting Csm, (2020); Yolov4 Optimal Speed and Accuracy of Object Detection, (2020); Boker, Steven M., Windowed cross-correlation and peak picking for the analysis of variability in the association between behavioral time series, Psychological Methods, 7, 3, pp. 338-355, (2002)","","Association for Computing Machinery, Inc","ACM SIGCHI","23rd ACM International Conference on Multimodal Interaction, ICMI 2021","","Virtual, Online","172814","","9781450384810","","","English","Conference paper","Final","","Scopus","2-s2.0-85119005184"
"S., Wang, Shunli; D., Yang, Dingkang; P., Zhai, Peng; C., Chen, Chixiao; L., Zhang, Lihua","Wang, Shunli (58038020300); Yang, Dingkang (57345560900); Zhai, Peng (57222341319); Chen, Chixiao (54931930600); Zhang, Lihua (57223101319)","58038020300; 57345560900; 57222341319; 54931930600; 57223101319","TSA-Net: Tube Self-Attention Network for Action Quality Assessment","2021","","","","","4902","4910","0","74","10.1145/3474085.3475438","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119354614&doi=10.1145%2F3474085.3475438&partnerID=40&md5=cc43b6070edf368cdc43361e81c95d85","Fudan University, Shanghai, China; Ji Hua Laboratory, Foshan, China; Shanghai Engineering Research Center of AI & Robotics, Shanghai, China; Jilin University, Changchun, China","Wang, Shunli, Fudan University, Shanghai, China, Shanghai Engineering Research Center of AI & Robotics, Shanghai, China; Yang, Dingkang, Fudan University, Shanghai, China, Ji Hua Laboratory, Foshan, China; Zhai, Peng, Fudan University, Shanghai, China, Jilin University, Changchun, China; Chen, Chixiao, Fudan University, Shanghai, China; Zhang, Lihua, Fudan University, Shanghai, China, Ji Hua Laboratory, Foshan, China, Shanghai Engineering Research Center of AI & Robotics, Shanghai, China, Jilin University, Changchun, China","In recent years, assessing action quality from videos has attracted growing attention in computer vision community and human-computer interaction. Most existing approaches usually tackle this problem by directly migrating the model from action recognition tasks, which ignores the intrinsic differences within the feature map such as foreground and background information. To address this issue, we propose a Tube Self-Attention Network (TSA-Net) for action quality assessment (AQA). Specifically, we introduce a single object tracker into AQA and propose the Tube Self-Attention Module (TSA), which can efficiently generate rich spatio-temporal contextual information by adopting sparse feature interactions. The TSA module is embedded in existing video networks to form TSA-Net. Overall, our TSA-Net is with the following merits: 1) High computational efficiency, 2) High flexibility, and 3) The state-of-the-art performance. Extensive experiments are conducted on popular action quality assessment datasets including AQA-7 and MTL-AQA. Besides, a dataset named Fall Recognition in Figure Skating (FR-FS) is proposed to explore the basic action assessment in the figure skating scene. Our TSA-Net achieves the Spearman's Rank Correlation of 0.8476 and 0.9393 on AQA-7 and MTL-AQA, respectively, which are the new state-of-the-art results. The results on FR-FS also verify the effectiveness of the TSA-Net. The code and FR-FS dataset are publicly available at https://github.com/Shunli-Wang/TSA-Net. © 2021 Elsevier B.V., All rights reserved.","Action Quality Assessment; Self-attention Mechanism; Video Action Analysis; Computer Vision; Human Computer Interaction; Action Analysis; Action Quality Assessment; Action Recognition; Attention Mechanisms; Fall Recognition; Figure Skating; Quality Assessment; Self-attention Mechanism; Video Action Analyse; Vision Communities; Computational Efficiency","Computer vision; Human computer interaction; Action analysis; Action quality assessment; Action recognition; Attention mechanisms; Fall recognition; Figure skating; Quality assessment; Self-attention mechanism; Video action analyse; Vision communities; Computational efficiency","","","This work was supported by Shanghai Municipal Science and Technology Major Project 2021SHZDZX0103 and National Natural Science Foundation of China under Grant 82090052.","Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Chen, Yunpeng, A2-Nets: Double attention networks, Advances in Neural Information Processing Systems, 2018-December, (2018); Devlin, Jacob, BERT: Pre-training of deep bidirectional transformers for language understanding, 1, pp. 4171-4186, (2019); Doughty, Hazel, Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 6057-6066, (2018); Doughty, Hazel, The pros and cons: Rank-aware temporal attention for skill determination in long videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7854-7863, (2019); Fan, Qi, Few-shot object detection with attention-RpN and multi-relation detector, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 4012-4021, (2020); Fang, Haoshu, RMPE: Regional Multi-person Pose Estimation, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 2353-2362, (2017); Feichtenhofer, Christoph, X3D: Expanding Architectures for Efficient Video Recognition, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 200-210, (2020); Feichtenhofer, Christoph, Convolutional Two-Stream Network Fusion for Video Action Recognition, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December, pp. 1933-1941, (2016); Han, Kai, Attribute-aware attention model for fine-grained representation learning, pp. 2040-2048, (2018)","","Association for Computing Machinery, Inc","ACM SIGMM","29th ACM International Conference on Multimedia, MM 2021","","Virtual, Online","173350","","9781450386517","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85119354614"
"K., Roditakis, Konstantinos; A., Makris, Alexandros; A.A., Argyros, Antonis A.","Roditakis, Konstantinos (56989870500); Makris, Alexandros (16305126900); Argyros, Antonis A. (7003791908)","56989870500; 16305126900; 7003791908","Towards Improved and Interpretable Action Quality Assessment with Self-Supervised Alignment","2021","ACM International Conference Proceeding Series","","","","507","513","0","10","10.1145/3453892.3461624","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109249346&doi=10.1145%2F3453892.3461624&partnerID=40&md5=3fd9815a7b87286f86a7aeb8dd22d3b3","University of Crete and Forth, Crete, Greece; Foundation for Research and Technology-Hellas, Heraklion, Greece","Roditakis, Konstantinos, University of Crete and Forth, Crete, Greece; Makris, Alexandros, Foundation for Research and Technology-Hellas, Heraklion, Greece; Argyros, Antonis A., University of Crete and Forth, Crete, Greece","Action Quality Assessment (AQA) is a video understanding task aiming at the quantification of the execution quality of an action. One of the main challenges in relevant, deep learning-based approaches is the collection of training data annotated by experts. Current methods perform fine-tuning on pre-trained backbone models and aim to improve performance by modeling the subjects and the scene. In this work, we consider embeddings extracted using a self-supervised training method based on a differential cycle consistency loss between sequences of actions. These are shown to improve the state-of-the-art without the need for additional annotations or scene modeling. The same embeddings are also used to temporally align the sequences prior to quality assessment which further increases the accuracy, provides robustness to variance in execution speed and enables us to provide fine-grained interpretability of the assessment score. The experimental evaluation of the method on the MTL-AQA dataset demonstrates significant accuracy gain compared to the state-of-the-art baselines, which grows even more when the action execution sequences are not well aligned. © 2021 Elsevier B.V., All rights reserved.","Action Quality Assessment; Computer Vision; Video Alignment; Video Understanding; Embeddings; Petroleum Reservoir Evaluation; Action Execution; Experimental Evaluation; Improve Performance; Interpretability; Learning-based Approach; Quality Assessment; Supervised Trainings; Video Understanding; Deep Learning","Embeddings; Petroleum reservoir evaluation; Action execution; Experimental evaluation; Improve performance; Interpretability; Learning-based approach; Quality assessment; Supervised trainings; Video understanding; Deep learning","","","","Bertasius, Gedas, Am i a Baller? Basketball Performance Assessment from First-Person Videos, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 2196-2204, (2017); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Doughty, Hazel, Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 6057-6066, (2018); Doughty, Hazel, The pros and cons: Rank-aware temporal attention for skill determination in long videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7854-7863, (2019); Dwibedi, Debidatta, Temporal cycle-consistency learning, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 1801-1810, (2019); International Conference on Machine Learning, (2019); Gao, Jibin, An Asymmetric Modeling for Action Assessment, Lecture Notes in Computer Science, 12375 LNCS, pp. 222-238, (2020); Miccai Workshop M2cai, (2014); He, Kaiming, Deep residual learning for image recognition, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December, pp. 770-778, (2016); Jain, Hiteshi, Action Quality Assessment Using Siamese Network-Based Deep Metric Learning, IEEE Transactions on Circuits and Systems for Video Technology, 31, 6, pp. 2260-2273, (2021)","","Association for Computing Machinery","The College of Engineering, University of Texas at Arlington (UTA); The Department of Computer Science and Engineering at UTA; The Human Centered Computing Laboratory (Heracleia) at UTA; The iPerform Industry-University NSF Center at UTA; The National Center for Scientific Research (NCSR)-Demokritos; The National Science Foundation (NSF)","14th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2021","","Virtual, Online","169934","","9781450385855; 9781450314398; 9781450396387; 9781450390019; 9781450390217; 9781450348270; 9781450381963; 9781450322485; 9781450348201; 9781450364454","","","English","Conference paper","Final","","Scopus","2-s2.0-85109249346"
"B., Hirchoua, Badr; B., Ouhbi, Brahim; B., Frikh, Bouchra","Hirchoua, Badr (57200044095); Ouhbi, Brahim (6603412029); Frikh, Bouchra (35092386700)","57200044095; 6603412029; 35092386700","Deep reinforcement learning based trading agents: Risk curiosity driven learning for financial rules-based policy","2021","Expert Systems with Applications","170","","114553","","","0","48","10.1016/j.eswa.2020.114553","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099212691&doi=10.1016%2Fj.eswa.2020.114553&partnerID=40&md5=d769afdbe460f7220e96f0901478dca8","Université Moulay Ismaïl, Meknes, Morocco; Université Sidi Mohamed Ben Abdellah, Fez, Morocco","Hirchoua, Badr, Industrial Engineering and Productivity Department, Université Moulay Ismaïl, Meknes, Morocco; Ouhbi, Brahim, Industrial Engineering and Productivity Department, Université Moulay Ismaïl, Meknes, Morocco; Frikh, Bouchra, Department of Computer Science, Université Sidi Mohamed Ben Abdellah, Fez, Morocco","Financial markets are complex dynamic systems influenced by a high number of active agents, which produce a behavior with high randomness and noise. Trading strategies are well depicted as an online decision-making problem involving imperfect information and aiming to maximize the return while restraining the risk. However, it is challenging to obtain an optimal strategy in the complex and dynamic stock market. Therefore, recent developments in similar environments have pushed researchers towards exciting new horizons. In this paper, a novel rule-based policy approach is proposed to train a deep reinforcement learning agent for automated financial trading. Precisely, a continuous virtual environment has been created, with different versions of agents trading against one another. During this multiplex process, the agents which are trained on 504 risky datasets, use the fundamental concepts of proximal policy optimization to improve their own decision making by adjusting their action choice against the uncertainty of states. Risk curiosity-driven learning acts as an intrinsic reward function and is heavily laden with signals to find salient relationships between actions and market behaviors. The trained agent based on curiosity-driven risk has steadily and progressively improved actions quality. The self-learned rules driven by the agent curiosity push the policy towards actions that yield a high performance over the environment. Experiments on 8 real-world stocks are given to verify the appropriateness and efficiency of the self-learned rules. The proposed system has achieved promising performances, made better trades using fewer transactions, and outperformed the state-of-the-art baselines. © 2021 Elsevier B.V., All rights reserved.","Deep Reinforcement Learning; Financial Data; Financial Engineering; Knowledge Uncertainty; Partially Observable Markov Decision Process; Trading System; Commerce; Decision Making; Electronic Trading; Financial Markets; Reinforcement Learning; Complex Dynamic Systems; Fundamental Concepts; Imperfect Information; On-line Decision Makings; Optimal Strategies; Policy Optimization; Reinforcement Learning Agent; Rule-based Policies; Deep Learning","Commerce; Decision making; Electronic trading; Financial markets; Reinforcement learning; Complex dynamic systems; Fundamental concepts; Imperfect information; On-line decision makings; Optimal strategies; Policy optimization; Reinforcement learning agent; Rule-based policies; Deep learning","","","","Almahdi, Saud, An adaptive portfolio trading system: A risk-return portfolio optimization using recurrent reinforcement learning with expected maximum drawdown, Expert Systems with Applications, 87, pp. 267-279, (2017); Arulkumaran, Kai, Deep reinforcement learning: A brief survey, IEEE Signal Processing Magazine, 34, 6, pp. 26-38, (2017); Azhikodan, Akhil Raj, Stock trading bot using deep reinforcement learning, Lecture Notes in Networks and Systems, 32, pp. 41-49, (2019); Ballings, Michel, Evaluating multiple classifiers for stock price direction prediction, Expert Systems with Applications, 42, 20, pp. 7046-7056, (2015); Buehler, Hans, Deep hedging, Quantitative Finance, 19, 8, pp. 1271-1291, (2019); Carapuço, João, Reinforcement learning applied to Forex trading, Applied Soft Computing, 73, pp. 783-794, (2018); Chaboud, Alain P., Rise of the machines: Algorithmic trading in the foreign exchange market, Journal of Finance, 69, 5, pp. 2045-2084, (2014); Chen, Chiaoting, Cloning strategies from trading records using agent-based reinforcement learning algorithm, pp. 34-37, (2018); Journal of Machine Learning Research, (2017); Dash, Rajashree, An evolutionary hybrid Fuzzy Computationally Efficient EGARCH model for volatility prediction, Applied Soft Computing, 45, pp. 40-60, (2016)","","Elsevier Ltd","","","","","","09574174","","ESAPE","","English","Article","Final","","Scopus","2-s2.0-85099212691"
"S., Wang, Shunli; D., Yang, Dingkang; P., Zhai, Peng; Q., Yu, Qing; T., Suo, Tao; Z., Sun, Zhan; K., Li, Ka; L., Zhang, Lihua","Wang, Shunli (58038020300); Yang, Dingkang (57345560900); Zhai, Peng (57222341319); Yu, Qing (57658324900); Suo, Tao (56379960500); Sun, Zhan (38062096400); Li, Ka (55646954900); Zhang, Lihua (57223101319)","58038020300; 57345560900; 57222341319; 57658324900; 56379960500; 38062096400; 55646954900; 57223101319","A Survey of Video-based Action Quality Assessment","2021","","","","","101","109","0","19","10.1109/INSAI54028.2021.00029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129803885&doi=10.1109%2FINSAI54028.2021.00029&partnerID=40&md5=ced0fe16b38b8451f3e650ba0e8f2b16","Fudan University, Shanghai, China; Zhongshan Hospital, Shanghai, China; Ji Hua Laboratory, Foshan, China","Wang, Shunli, Institute of AI and Robotics, Fudan University, Shanghai, China; Yang, Dingkang, Institute of AI and Robotics, Fudan University, Shanghai, China; Zhai, Peng, Institute of AI and Robotics, Fudan University, Shanghai, China; Yu, Qing, Zhongshan Hospital, Shanghai, China; Suo, Tao, Zhongshan Hospital, Shanghai, China; Sun, Zhan, Zhongshan Hospital, Shanghai, China; Li, Ka, Zhongshan Hospital, Shanghai, China; Zhang, Lihua, Institute of AI and Robotics, Fudan University, Shanghai, China, Ji Hua Laboratory, Foshan, China","Human action recognition and analysis have great demand and important application significance in video surveillance, video retrieval, and human-computer interaction. The task of human action quality evaluation requires the intelligent system to automatically and objectively evaluate the action completed by the human. The action quality assessment model can reduce the human and material resources spent in action evaluation and reduce subjectivity. In this paper, we provide a comprehensive survey of existing papers on video-based action quality assessment. Different from human action recognition, the application scenario of action quality assessment is relatively narrow. Most of the existing work focuses on sports and medical care. We first introduce the definition and challenges of human action quality assessment. Then we present the existing datasets and evaluation metrics. In addition, we summarized the methods of sports and medical care according to the model categories and publishing institutions according to the characteristics of the two fields. At the end, combined with recent work, the promising development direction in action quality assessment is discussed. © 2022 Elsevier B.V., All rights reserved.","Action Quality Assessment; Computer Vision; Deep Learning; Human Behavior Analysis; Behavioral Research; Computer Vision; Deep Learning; Human Computer Interaction; Intelligent Systems; Quality Control; Security Systems; Sports; Action Quality Assessment; Human Action Analysis; Human Actions; Human Behavior Analysis; Human-action Recognition; Quality Assessment; Surveillance Video; Video Retrieval; Video Surveillance; Surveys","Behavioral research; Computer vision; Deep learning; Human computer interaction; Intelligent systems; Quality control; Security systems; Sports; Action quality assessment; Human action analysis; Human actions; Human behavior analysis; Human-action recognition; Quality assessment; Surveillance video; Video retrieval; Video surveillance; Surveys","","","ACKNOWLEDGMENT This work was supported by National Natural Science Foundation of China under Grant 82090052, and Shanghai Municipal Science and Technology Major Project 2021SHZDZX0103.","Simonyan, Karen, Two-stream convolutional networks for action recognition in videos, Advances in Neural Information Processing Systems, 1, January, pp. 568-576, (2014); Feichtenhofer, Christoph, Convolutional Two-Stream Network Fusion for Video Action Recognition, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December, pp. 1933-1941, (2016); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Tran, Du, Learning spatiotemporal features with 3D convolutional networks, Proceedings of the IEEE International Conference on Computer Vision, 2015 International Conference on Computer Vision, ICCV 2015, pp. 4489-4497, (2015); Eccv, (2014); Parmar, Paritosh, Learning to Score Olympic Events, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2017-July, pp. 76-84, (2017); Bertasius, Gedas, Am i a Baller? Basketball Performance Assessment from First-Person Videos, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 2196-2204, (2017); Winter Conference on Applications of Computer Vision Wacv, (2018); Parmar, Paritosh, What and how well you performed? a multitask learning approach to action quality assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 304-313, (2019); IEEE Transactions on Circuits and Systems for Video Technology, (2019)","","Institute of Electrical and Electronics Engineers Inc.","Institute on Networking Systems of AI (INSAI)","2021 International Conference on Networking Systems of AI, INSAI 2021","","Shanghai","178906","","9781665408592","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85129803885"
"H., Li, Huiying; Q., Lei, Qing; H., Zhang, Hongbo; J., Du, Jixiang","Li, Huiying (57196359348); Lei, Qing (23091470800); Zhang, Hongbo (56945874300); Du, Jixiang (14017783700)","57196359348; 23091470800; 56945874300; 14017783700","Skeleton Based Action Quality Assessment of Figure Skating Videos","2021","","","","","196","200","0","10","10.1109/ITME53901.2021.00048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128841617&doi=10.1109%2FITME53901.2021.00048&partnerID=40&md5=c297b9ebb57d10f63c12e45db0c0cb00","Huaqiao University, Quanzhou, China; Huaqiao University, Quanzhou, China","Li, Huiying, Department of Computer Science and Technology, Huaqiao University, Quanzhou, China; Lei, Qing, Department of Computer Science and Technology, Huaqiao University, Quanzhou, China; Zhang, Hongbo, Huaqiao University, Quanzhou, China; Du, Jixiang, Department of Computer Science and Technology, Huaqiao University, Quanzhou, China","Action quality assessment(AQA) aims at achieving automatic evaluation the performance of human actions in video. Compared with action recognition problem, AQA focuses more on subtle differences both in spatial and temporal dimensions during the whole executing process of actions. However, most existing AQA methods tried to extract features directly from RGB videos through a 3D ConvNets, which makes the features mixed with useless scene information. To overcome this problem, We propose a deep pose feature learning AQA method that captured detailed and meaningful representations for skeleton information to discover the subtle motion difference of AQA problem. We first apply pose estimation method to obtain human skeleton data from RGB videos. Then a spatio-temporal graph convolutional network (ST-GCN) is employed to extract the dynamic changes of skeleton data and obtain the representative pose features. Finally, a regressor composed of three fully connected layers is developed to reduce the dimension of the obtained pose features and predict the final score. Experiments on MIT figure skating dataset have been extensively conducted, and the results demonstrate that the proposed method has achieved improvements that outperformed current state-of-the-art methods. © 2022 Elsevier B.V., All rights reserved.","Action Quality Assessment; Figure Skating Dataset; Spatial-temporal Graph Convolutional Network; Computer Vision; Convolution; Convolutional Neural Networks; Deep Learning; Action Quality Assessment; Automatic Evaluation; Convolutional Networks; Figure Skating; Figure Skating Dataset; Performance; Quality Assessment; Spatial Temporals; Spatial-temporal Graph Convolutional Network; Temporal Graphs; Musculoskeletal System","Computer vision; Convolution; Convolutional neural networks; Deep learning; Action quality assessment; Automatic evaluation; Convolutional networks; Figure skating; Figure skating dataset; Performance; Quality assessment; Spatial temporals; Spatial-temporal graph convolutional network; Temporal graphs; Musculoskeletal system","","","Funding text 1: National Key Research and Development Program of China (Grant No.2019YFC1604700), Natural Science Foundation of Fujian Province of China (No. 2019J01082 and 2020J01085), and the Promotion Program for Young and Middle-aged Teacher in Science and Technology Research of Huaqiao University (Grant No.ZQN-YX601).; Funding text 2: Foundation of China (Grant No. 61871196, 62001176),; Funding text 3: This work was supported by the Natural Science Foundation of China (Grant No. 61871196","Parmar, Paritosh, What and how well you performed? a multitask learning approach to action quality assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 304-313, (2019); Pan, Jiahui, Action assessment by joint relation graphs, Proceedings of the IEEE International Conference on Computer Vision, pp. 6330-6339, (2019); Lei, Qing, Temporal attention learning for action quality assessment in sports video, Signal, Image and Video Processing, 15, 7, pp. 1575-1583, (2021); Nekoui, Mahdiar, FALCONS: Fast learner-grader for contorted poses in sports, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2020-June, pp. 3941-3949, (2020); Parmar, Paritosh, Learning to Score Olympic Events, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2017-July, pp. 76-84, (2017); Lei, Qing, A survey of vision-based human action evaluation methods, Sensors, 19, 19, (2019); Action Quality Assessment Across Multiple Actions, (2019); Advances in Multimedia Information Processing Pcm, (2018); Dong, Lijia, Learning and fusing multiple hidden substages for action quality assessment, Knowledge-Based Systems, 229, (2021); Eccv, (2014)","","Institute of Electrical and Electronics Engineers Inc.","China Xiamen University; Wuyishan University","11th International Conference on Information Technology in Medicine and Education, ITME 2021","","Wuyishan","178772","","9781665406796","","","English","Conference paper","Final","","Scopus","2-s2.0-85128841617"
"P., Parmar, Paritosh; J., Reddy, Jaiden; B.T., Morris, Brendan Tran","Parmar, Paritosh (57192919626); Reddy, Jaiden (57222070950); Morris, Brendan Tran (17435393600)","57192919626; 57222070950; 17435393600","Piano Skills Assessment","2021","","","","","","","0","22","10.1109/MMSP53017.2021.9733638","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127534015&doi=10.1109%2FMMSP53017.2021.9733638&partnerID=40&md5=9e71d92e63228c4179372216c22755e9","The University of British Columbia, Vancouver, Canada; Stanford Engineering, Stanford, United States; University of Nevada, Las Vegas, Las Vegas, United States","Parmar, Paritosh, Department of Computer Science, The University of British Columbia, Vancouver, Canada; Reddy, Jaiden, Stanford Engineering, Stanford, United States; Morris, Brendan Tran, Department of Electrical Engineering, University of Nevada, Las Vegas, Las Vegas, United States","Can a computer determine a piano player's skill level? Is it preferable to base this assessment on visual analysis of the player's performance or should we trust our ears over our eyes? Since current convolutional neural networks (CNNs) have difficulty processing long video videos, how can shorter clips be sampled to best reflect the players skill level? In this work, we collect and release a first-of-its-kind dataset for multimodal skill assessment focusing on assessing piano player's skill level, answer the asked questions, initiate work in automated evaluation of piano playing skills and provide baselines for future work. Dataset can be accessed from: https://github.com/ParitoshParmar/Piano-Skills-Assessment. © 2022 Elsevier B.V., All rights reserved.","Action Quality Assessment; Audio Classification; Automated Piano Skills Assessment; Computer Vision; Skills Assessment; Video Classification; Video Understanding; Audio Acoustics; Convolutional Neural Networks; Musical Instruments; Action Quality Assessment; Audio Classification; Automated Piano Skill Assessment; Quality Assessment; Skill Assessment; Skill Levels; Video Classification; Video Understanding; Visual Analysis; Computer Vision","Audio acoustics; Convolutional neural networks; Musical instruments; Action quality assessment; Audio classification; Automated piano skill assessment; Quality assessment; Skill assessment; Skill levels; Video classification; Video understanding; Visual analysis; Computer vision","","","","Lee, Jangwon, Observing pianist accuracy and form with computer vision, pp. 1505-1513, (2019); Li, Bochen, Creating a Multitrack Classical Music Performance Dataset for Multimodal Music Analysis: Challenges, Insights, and Applications, IEEE Transactions on Multimedia, 21, 2, pp. 522-535, (2019); Sophia Koepke, A., Sight to Sound: An End-to-End Approach for Visual Piano Transcription, Proceedings - ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing, 2020-May, pp. 1838-1842, (2020); Li, Jun, Robust Piano Music Transcription Based on Computer Vision, ACM International Conference Proceeding Series, pp. 92-97, (2020); Virtual Piano Using Computer Vision, (2019); Su, Kun, Audeo: Audio generation for a silent performance video, Advances in Neural Information Processing Systems, 2020-December, (2020); Foley Music Learning to Generate Music from Videos, (2020); Li, Bochen, Skeleton plays piano: Online generation of pianist body movements from MIDI performance, pp. 218-224, (2018); Doughty, Hazel, The pros and cons: Rank-aware temporal attention for skill determination in long videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7854-7863, (2019); Li, Zhenqiang, Manipulation-skill assessment from videos with spatial attention network, pp. 4385-4395, (2019)","","Institute of Electrical and Electronics Engineers Inc.","","23rd IEEE International Workshop on Multimedia Signal Processing, MMSP 2021","","Tampere","177851","","9781665432870","","","English","Conference paper","Final","","Scopus","2-s2.0-85127534015"
"T., Nagai, Takasuke; S., Takeda, Shoichiro; M., Matsumura, Masaaki; S., Shimizu, Shinya; S., Yamamoto, Susumu","Nagai, Takasuke (57203634284); Takeda, Shoichiro (57207765972); Matsumura, Masaaki (35174921800); Shimizu, Shinya (13605641900); Yamamoto, Susumu (55475460600)","57203634284; 57207765972; 35174921800; 13605641900; 55475460600","ACTION QUALITY ASSESSMENT WITH IGNORING SCENE CONTEXT","2021","Proceedings - International Conference on Image Processing, ICIP","2021-September","","","1189","1193","0","8","10.1109/ICIP42928.2021.9506257","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125596949&doi=10.1109%2FICIP42928.2021.9506257&partnerID=40&md5=961914e4aacf4055892a1cc6c0a1fd4d","Nippon Telegraph and Telephone Corporation, Tokyo, Japan","Nagai, Takasuke, Nippon Telegraph and Telephone Corporation, Tokyo, Japan; Takeda, Shoichiro, Nippon Telegraph and Telephone Corporation, Tokyo, Japan; Matsumura, Masaaki, Nippon Telegraph and Telephone Corporation, Tokyo, Japan; Shimizu, Shinya, Nippon Telegraph and Telephone Corporation, Tokyo, Japan; Yamamoto, Susumu, Nippon Telegraph and Telephone Corporation, Tokyo, Japan","We propose an action quality assessment (AQA) method that can specifically assess target action quality with ignoring scene context, which is a feature unrelated to the target action. Existing AQA methods have tried to extract spatiotemporal features related to the target action by applying 3D convolution to the video. However, since their models are not explicitly designed to extract the features of the target action, they mis-extract scene context and thus cannot assess the target action quality correctly. To overcome this problem, we impose two losses to an existing AQA model: scene adversarial loss and our newly proposed human-masked regression loss. The scene adversarial loss encourages the model to ignore scene context by adversarial training. Our human-masked regression loss does so by making the correlation between score outputs by an AQA model and human referees undefinable when the target action is not visible. These two losses lead the model to specifically assess the target action quality with ignoring scene context. We evaluated our method on a diving dataset commonly used for AQA and found that it outperformed current state-of-the-art methods. This result shows that our method is effective in ignoring scene context while assessing the target action quality. © 2025 Elsevier B.V., All rights reserved.","Action Quality Assessment; Deep Learning; Regression Problem; Scene Context; Spatiotemporal Feature; Computer Vision; Regression Analysis; 'current; Action Quality Assessment; Deep Learning; Quality Assessment; Quality Assessment Model; Regression Problem; Scene Context; Spatiotemporal Feature; State-of-the-art Methods","Computer vision; Regression analysis; 'current; Action quality assessment; Deep learning; Quality assessment; Quality assessment model; Regression problem; Scene context; Spatiotemporal feature; State-of-the-art methods","","","","Olympic Figure Skating Controversy Judging System is Most to Blame for Uproar, (2014); Choi, Jinwoo, Why Can't I dance in the mall? learning to mitigate scene bias in action recognition, Advances in Neural Information Processing Systems, 32, (2019); Chu, Xiao, Multi-context attention for human pose estimation, 2017-January, pp. 5669-5678, (2017); Cvpr, (2018); Doughty, Hazel, The pros and cons: Rank-aware temporal attention for skill determination in long videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7854-7863, (2019); Elgamal, Tarek, Multicamera summarization of rehabilitation sessions in home environment, pp. 1381-1389, (2017); Ganin, Yaroslav, Unsupervised domain adaptation by backpropagation, 2, pp. 1180-1189, (2015); Cvpm, (2019); He, Kaiming, Mask R-CNN, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 2980-2988, (2017); Lei, Qing, A survey of vision-based human action evaluation methods, Sensors, 19, 19, (2019)","","IEEE Computer Society","The Institute of Electrical and Electronics Engineers Signal Processing Society","28th IEEE International Conference on Image Processing, ICIP 2021","","Anchorage; AK","177095","15224880","9781665441155; 9781728198354; 9780780391345; 9781457713033; 1424414377; 9781479970612; 9781538662496; 9781509021758; 9781424417643; 9781728163956","","","English","Conference paper","Final","","Scopus","2-s2.0-85125596949"
"T., Wang, Tao; W., Sun, Wei; X., Min, Xiongkuo; W., Lu, Wei; Z., Zhang, Zicheng; G., Zhai, Guangtao","Wang, Tao (57226676622); Sun, Wei (59257335500); Min, Xiongkuo (56030205300); Lu, Wei (55913809400); Zhang, Zicheng (57219687211); Zhai, Guangtao (15847120000)","57226676622; 59257335500; 56030205300; 55913809400; 57219687211; 15847120000","A Multi-dimensional Aesthetic Quality Assessment Model for Mobile Game Images","2021","","","","","","","0","16","10.1109/VCIP53242.2021.9675430","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125284884&doi=10.1109%2FVCIP53242.2021.9675430&partnerID=40&md5=75f3d273091582b7d07561b3f9dd34bf","Shanghai Jiao Tong University, Shanghai, China","Wang, Tao, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Sun, Wei, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Min, Xiongkuo, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Lu, Wei, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Zhang, Zicheng, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Zhai, Guangtao, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China","With the development of the game industry and the popularization of mobile devices, mobile games have played an important role in people's entertainment life. The aesthetic quality of mobile game images determines the users' Quality of Experience (QoE) to a certain extent. In this paper, we propose a multi-task deep learning based method to evaluate the aesthetic quality of mobile game images in multiple dimensions (i.e. the fineness, color harmony, colorfulness, and overall quality). Specifically, we first extract the quality-aware feature representation through integrating the features from all intermediate layers of the convolution neural network (CNN) and then map these quality-aware features into the quality score space in each dimension via the quality regressor module, which consists of three fully connected (FC) layers. The proposed model is trained through a multi-task learning manner, where the quality-aware features are shared by different quality dimension prediction tasks, and the multi-dimensional quality scores of each image are regressed by multiple quality regression modules respectively. We further introduce an uncertainty principle to balance the loss of each task in the training stage. The experimental results show that our proposed model achieves the best performance on the Multi-dimensional Aesthetic assessment for Mobile Game image database (MAMG) among state-of-the-art image quality assessment (IQA) algorithms and aesthetic quality assessment (AQA) algorithms. © 2022 Elsevier B.V., All rights reserved.","Aesthetic Quality Assessment; Deep Learning; Mobile Game Image; Multi-task Learning; Computer Vision; Convolutional Neural Networks; Deep Learning; Image Quality; Multilayer Neural Networks; Quality Of Service; Aesthetic Qualities; Esthetic Quality Assessment; Game Industry; Mobile Game Image; Mobile Games; Multi Dimensional; Multi Tasks; Quality Assessment; Quality Assessment Model; Quality Control","Computer vision; Convolutional neural networks; Deep learning; Image quality; Multilayer neural networks; Quality of service; Aesthetic qualities; Esthetic quality assessment; Game industry; Mobile game image; Mobile games; Multi dimensional; Multi tasks; Quality assessment; Quality assessment model; Quality control","","","This work was supported in part by the National Natural Science Foundation of China under Grant 61901260, Grant 61831015, Grant 61771305, Grant U1908210, and Grant 62101326.","Global Games Market Report, (2025); Zhai, Guangtao, Perceptual image quality assessment: a survey, Science China Information Sciences, 63, 11, (2020); Min, Xiongkuo, Quality Evaluation of Image Dehazing Methods Using Synthetic Hazy Images, IEEE Transactions on Multimedia, 21, 9, pp. 2319-2333, (2019); Datta, Ritendra, Studying aesthetics in photographic images using a computational approach, Lecture Notes in Computer Science, 3953 LNCS, pp. 288-301, (2006); Ke, Yan, The design of high-level features for photo quality assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1, pp. 419-426, (2006); Lu, Xin, Rapid: Rating pictorial aesthetics using deep learning, pp. 457-466, (2014); Mai, Long, Composition-preserving deep photo aesthetics assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December, pp. 497-506, (2016); Talebi, Hossein, NIMA: Neural Image Assessment, IEEE Transactions on Image Processing, 27, 8, pp. 3998-4011, (2018); Hosu, Vlad, Effective aesthetics prediction with multi-level spatially pooled features, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 9367-9375, (2019); Zeiler, Matthew D., Visualizing and understanding convolutional networks, Lecture Notes in Computer Science, 8689 LNCS, PART 1, pp. 818-833, (2014)","","Institute of Electrical and Electronics Engineers Inc.","CAS; et al.; Huawei; IEEE; Meta; Qualcomm","2021 International Conference on Visual Communications and Image Processing, VCIP 2021","","Munich; Hotel Novotel Munich City","176486","","9781728185514","","","English","Conference paper","Final","","Scopus","2-s2.0-85125284884"
"X., Yu, Xumin; Y., Rao, Yongming; W., Zhao, Wenliang; J., Lu, Jiwen; J., Zhou, Jie","Yu, Xumin (57220749597); Rao, Yongming (57200621657); Zhao, Wenliang (57224958866); Lu, Jiwen (14065346100); Zhou, Jie (59662779500)","57220749597; 57200621657; 57224958866; 14065346100; 59662779500","Group-aware Contrastive Regression for Action Quality Assessment","2021","Proceedings of the IEEE International Conference on Computer Vision","","","","7899","7908","0","101","10.1109/ICCV48922.2021.00782","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123302460&doi=10.1109%2FICCV48922.2021.00782&partnerID=40&md5=12a66b9a7fb1786202453a88716f07bb","Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Beijing National Research Center for Information Science and Technology, Beijing, China","Yu, Xumin, Department of Automation, Tsinghua University, Beijing, China; Rao, Yongming, Department of Automation, Tsinghua University, Beijing, China; Zhao, Wenliang, Tsinghua University, Beijing, China; Lu, Jiwen, Tsinghua University, Beijing, China; Zhou, Jie, Beijing National Research Center for Information Science and Technology, Beijing, China","Assessing action quality is challenging due to the subtle differences between videos and large variations in scores. Most existing approaches tackle this problem by regressing a quality score from a single video, suffering a lot from the large inter-video score variations. In this paper, we show that the relations among videos can provide important clues for more accurate action quality assessment during both training and inference. Specifically, we reformulate the problem of action quality assessment as regressing the relative scores with reference to another video that has shared attributes (e.g., category and difficulty), instead of learning unreferenced scores. Following this formulation, we propose a new Contrastive Regression (CoRe) framework to learn the relative scores by pair-wise comparison, which highlights the differences between videos and guides the models to learn the key hints for assessment. In order to further exploit the relative information between two videos, we devise a group-aware regression tree to convert the conventional score regression into two easier sub-problems: coarse-to-fine classification and regression in small intervals. To demonstrate the effectiveness of CoRe, we conduct extensive experiments on three mainstream AQA datasets including AQA-7, MTL-AQA and JIGSAWS. Our approach outperforms previous methods by a large margin and establishes new state-of-the-art on all three benchmarks. © 2022 Elsevier B.V., All rights reserved.","Classification (of Information); Computer Vision; Coarse To Fine; Large Margins; Learn+; Pair-wise Comparison; Quality Assessment; Regression Trees; Relative Information; State Of The Art; Sub-problems; Regression Analysis","Classification (of information); Computer vision; Coarse to fine; Large margins; Learn+; Pair-wise comparison; Quality assessment; Regression trees; Relative information; State of the art; Sub-problems; Regression analysis","","","This work was supported in part by the National Natural Science Foundation of China under Grant U1813218, Grant U1713214, and Grant 61822603, in part by a grant from the Beijing Academy of Artificial Intelligence (BAAI), and in part by a grant from the Institute for Guo Qiang, Tsinghua University.","Neural Information Processing Letters and Reviews, (2007); Bertasius, Gedas, Am i a Baller? Basketball Performance Assessment from First-Person Videos, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 2196-2204, (2017); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Neurips, (2020); Whos Better Whos Best Skill Determination in Video Using Deep Ranking, (2017); Doughty, Hazel, The pros and cons: Rank-aware temporal attention for skill determination in long videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7854-7863, (2019); Feichtenhofer, Christoph, Slowfast networks for video recognition, Proceedings of the IEEE International Conference on Computer Vision, pp. 6201-6210, (2019); Miccai Workshop M2cai, (2014); Automated Video Assessment of Human Performance, (2001); He, Kaiming, Momentum Contrast for Unsupervised Visual Representation Learning, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 9726-9735, (2020)","","Institute of Electrical and Electronics Engineers Inc.","CVF; IEEE","18th IEEE/CVF International Conference on Computer Vision, ICCV 2021","","Virtual, Online","177652","15505499; 23807504","9798350307184; 9781467300629; 9781538610329; 9781728148038; 9781665401913; 9781457711015; 9781479928392; 9781467383912; 9781479930227; 9781424444205","PICVE","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85123302460"
"T., Guha, Tanaya; V., Hosu, Vlad; D., Saupe, Dietmar; B., Goldlücke, Bastian; N., Kumar, Naveen; W., Lin, Weisi; V.R., Martínez, Víctor R.; K., Somandepalli, Krishna; S.S.S., Narayanan, Shrikanth Shri S.; W., Cheng, Wenhuang","Guha, Tanaya (36628378700); Hosu, Vlad (57194236254); Saupe, Dietmar (7003734036); Goldlücke, Bastian (6505945761); Kumar, Naveen (57217716378); Lin, Weisi (8574872000); Martínez, Víctor R. (56330759800); Somandepalli, Krishna (56845601900); Narayanan, Shrikanth Shri S. (57203260136); Cheng, Wenhuang (24775341200)","36628378700; 57194236254; 7003734036; 6505945761; 57217716378; 8574872000; 56330759800; 56845601900; 57203260136; 24775341200","ATQAM/MAST'20: Joint Workshop on Aesthetic and Technical Quality Assessment of Multimedia and Media Analytics for Societal Trends","2020","","","","","4758","4760","0","3","10.1145/3394171.3421895","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106974185&doi=10.1145%2F3394171.3421895&partnerID=40&md5=3c619e009daab0ea5736ea13bf6f739c","University of Warwick, Coventry, United Kingdom; Universität Konstanz, Konstanz, Germany; The Walt Disney Company, Burbank, United States; Nanyang Technological University, Singapore City, Singapore; University of Southern California, Los Angeles, United States","Guha, Tanaya, University of Warwick, Coventry, United Kingdom; Hosu, Vlad, Universität Konstanz, Konstanz, Germany; Saupe, Dietmar, Universität Konstanz, Konstanz, Germany; Goldlücke, Bastian, Universität Konstanz, Konstanz, Germany; Kumar, Naveen, The Walt Disney Company, Burbank, United States; Lin, Weisi, Nanyang Technological University, Singapore City, Singapore; Martínez, Víctor R., University of Southern California, Los Angeles, United States; Somandepalli, Krishna, University of Southern California, Los Angeles, United States; Narayanan, Shrikanth Shri S., University of Warwick, Coventry, United Kingdom; Cheng, Wenhuang, University of Warwick, Coventry, United Kingdom","The Joint Workshop on Aesthetic and Technical Quality Assessment of Multimedia and Media Analytics for Societal Trends (ATQAM/ MAST) aims to bring together researchers and professionals working in fields ranging from computer vision, multimedia computing, multimodal signal processing to psychology and social sciences. It is divided into two tracks: ATQAM and MAST. ATQAM track: Visual quality assessment techniques can be divided into image and video technical quality assessment (IQA and VQA, or broadly TQA) and aesthetics quality assessment (AQA). While TQA is a long-standing field, having its roots in media compression, AQA is relatively young. Both have received increased attention with developments in deep learning. The topics have mostly been studied separately, even though they deal with similar aspects of the underlying subjective experience of media. The aim is to bring together individuals in the two fields of TQA and AQA for the sharing of ideas and discussions on current trends, developments, issues, and future directions. MAST track: The research area of media content analytics has been traditionally used to refer to applications involving inference of higher-level semantics from multimedia content. However, multimedia is typically created for human consumption, and we believe it is necessary to adopt a human-centered approach to this analysis, which would not only enable a better understanding of how viewers engage with content but also how they impact each other in the process. © 2021 Elsevier B.V., All rights reserved.","Aesthetics Assessment; Datasets; Media Consumption; Multimedia; Quality Assessment; Societal Impact; Visual Enhancement; Deep Learning; Semantics; Social Sciences Computing; Human Consumption; Multi-modal Signal Processing; Multimedia Computing; Multimedia Contents; Quality Assessment; Subjective Experiences; Technical Quality; Visual Quality Assessment; Signal Processing","Deep learning; Semantics; Social sciences computing; Human consumption; Multi-modal signal processing; Multimedia computing; Multimedia contents; Quality assessment; Subjective experiences; Technical quality; Visual quality assessment; Signal processing","","","Funding text 1: James Z. Wang is a professor at Pennsylvania State University. Wang’s research seeks to advance knowledge through modeling objects, concepts, aesthetics, and emotions in big visual data. He is well-known for his pioneering research in the field of aesthetics quality assessment. His research team have developed the ACQUINE aesthetic quality inference engine, SIMPLIcity semantics-sensitive image retrieval system, the ALIPR real-time computerized image tagging system, which are all widely cited. His research has been reported widely by significant media, including Discovery, Scientific American, MIT Tech Review, Public Radio, NPR, and CBS. Wang also received an NSF Career award and the endowed PNC Technologies Career Development Professorship.; Funding text 2: Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161 (Project A05).","","","Association for Computing Machinery, Inc","ACM SIGMM","28th ACM International Conference on Multimedia, MM 2020","","Virtual, Online","163870","","9781450379885","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85106974185"
"L., Zeng, Lingan; F., Hong, Fating; W., Zheng, Wei-Shi; Q., Yu, Qizhi; W., Zeng, Wei; Y., Wang, Yaowei; J., Lai, Jianhuang","Zeng, Lingan (57219787652); Hong, Fating (57202970675); Zheng, Wei-Shi (25928152800); Yu, Qizhi (57219788163); Zeng, Wei (56828661400); Wang, Yaowei (23092524700); Lai, Jianhuang (57216303737)","57219787652; 57202970675; 25928152800; 57219788163; 56828661400; 23092524700; 57216303737","Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos","2020","","","","","2526","2534","0","47","10.1145/3394171.3413560","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101280544&doi=10.1145%2F3394171.3413560&partnerID=40&md5=f30670d4cc8b7e5d2b8438f5eb0fafbe","Sun Yat-Sen University, Guangzhou, China; Zhejiang Lab, Hangzhou, China; Peking University, Beijing, China; Pengcheng Laboratory, Shenzhen, China","Zeng, Lingan, Sun Yat-Sen University, Guangzhou, China; Hong, Fating, Sun Yat-Sen University, Guangzhou, China; Zheng, Wei-Shi, Sun Yat-Sen University, Guangzhou, China; Yu, Qizhi, Zhejiang Lab, Hangzhou, China; Zeng, Wei, Peking University, Beijing, China; Wang, Yaowei, Pengcheng Laboratory, Shenzhen, China; Lai, Jianhuang, Sun Yat-Sen University, Guangzhou, China","The objective of action quality assessment is to score sports videos. However, most existing works focus only on video dynamic information (i.e., motion information) but ignore the specific postures that an athlete is performing in a video, which is important for action assessment in long videos. In this work, we present a novel hybrid dynAmic-static Context-aware attenTION NETwork (ACTION-NET) for action assessment in long videos. To learn more discriminative representations for videos, we not only learn the video dynamic information but also focus on the static postures of the detected athletes in specific frames, which represent the action quality at certain moments, along with the help of the proposed hybrid dynamic-static architecture. Moreover, we leverage a context-aware attention module consisting of a temporal instance-wise graph convolutional network unit and an attention unit for both streams to extract more robust stream features, where the former is for exploring the relations between instances and the latter for assigning a proper weight to each instance. Finally, we combine the features of the two streams to regress the final video score, supervised by ground-truth scores given by experts. Additionally, we have collected and annotated the new Rhythmic Gymnastics dataset, which contains videos of four different types of gymnastics routines, for evaluation of action quality assessment in long videos. Extensive experimental results validate the efficacy of our proposed method, which outperforms related approaches. © 2021 Elsevier B.V., All rights reserved.","Action Quality Assessment; Computer Vision; Context-aware Attention; Hybrid Dynamic-static Architecture; Rhythmic Gymnastics Dataset; Sports; Action Assessment; Context-aware; Convolutional Networks; Hybrid Dynamics; Motion Information; Quality Assessment; Static Architecture; Static Postures; Convolutional Neural Networks","Sports; Action assessment; Context-Aware; Convolutional networks; Hybrid dynamics; Motion information; Quality assessment; Static architecture; Static postures; Convolutional neural networks","","","This work was supported partially by the National Key Research and Development Program of China (2018YFB1004903), NSFC(U1911401, U1811461), Guangdong Province Science and Technology Innovation Leading Talents (2016TX03X157), Guangdong NSF Project (No. 2018B030312002), Guangzhou Research Project (201902010037), Research Projects of Zhejiang Lab (No. 2019KD0AB03), the Key-Area Research and Development Program of Guangzhou (202007030004), and Pearl River S&T Nova Program of Guangzhou (201806010056). We thank Jia-Chang Feng for useful feedback and suggestions.","IEEE Transactions on Circuits and Systems for Video Technology, (2018); Bertasius, Gedas, Am i a Baller? Basketball Performance Assessment from First-Person Videos, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 2196-2204, (2017); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Doughty, Hazel, Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 6057-6066, (2018); Doughty, Hazel, The pros and cons: Rank-aware temporal attention for skill determination in long videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7854-7863, (2019); He, Kaiming, Deep residual learning for image recognition, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December, pp. 770-778, (2016); Semi Supervised Classification with Graph Convolutional Networks, (2016); Le, Quocviet, Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3361-3368, (2011); Li, Yongjun, End-to-end learning for action quality assessment, Lecture Notes in Computer Science, 11165 LNCS, pp. 125-134, (2018); Li, Yongjun, ScoringNet: Learning Key Fragment for Action Quality Assessment with Ranking Loss in Skilled Sports, Lecture Notes in Computer Science, 11366 LNCS, pp. 149-164, (2019)","","Association for Computing Machinery, Inc","ACM SIGMM","28th ACM International Conference on Multimedia, MM 2020","","Virtual, Online","163870","","9781450379885","","","English","Conference paper","Final","","Scopus","2-s2.0-85101280544"
"J., Gao, Jibin; W., Zheng, Wei-Shi; J., Pan, Jiahui; C., Gao, Chengying; Y., Wang, Yaowei; W., Zeng, Wei; J., Lai, Jianhuang","Gao, Jibin (57214861263); Zheng, Wei-Shi (25928152800); Pan, Jiahui (55355956000); Gao, Chengying (9333513300); Wang, Yaowei (23092524700); Zeng, Wei (56828661400); Lai, Jianhuang (57216303737)","57214861263; 25928152800; 55355956000; 9333513300; 23092524700; 56828661400; 57216303737","An Asymmetric Modeling for Action Assessment","2020","Lecture Notes in Computer Science","12375 LNCS","","","222","238","0","45","10.1007/978-3-030-58577-8_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092135682&doi=10.1007%2F978-3-030-58577-8_14&partnerID=40&md5=2534e02ccdefee329ade97d473408aad","Sun Yat-Sen University, Guangzhou, China; Peng Cheng Laboratory, Shenzhen, China; Peking University, Beijing, China; Guangdong Artificial Intelligence and Digital Economy Laboratory, Guangzhou, China; Ministry of Education of the People's Republic of China, Beijing, China","Gao, Jibin, School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China, Guangdong Artificial Intelligence and Digital Economy Laboratory, Guangzhou, China; Zheng, Wei-Shi, School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China, Peng Cheng Laboratory, Shenzhen, China, Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education of the People's Republic of China, Beijing, China; Pan, Jiahui, School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China; Gao, Chengying, School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China; Wang, Yaowei, Peng Cheng Laboratory, Shenzhen, China; Zeng, Wei, School of Electronics Engineering and Computer Science, Peking University, Beijing, China; Lai, Jianhuang, School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China","Action assessment is a task of assessing the performance of an action. It is widely applicable to many real-world scenarios such as medical treatment and sporting events. However, existing methods for action assessment are mostly limited to individual actions, especially lacking modeling of the asymmetric relations among agents (e.g., between persons and objects); and this limitation undermines their ability to assess actions containingasymmetrically interactive motion patterns, since there always exists subordination between agents in many interactive actions. In this work, we model the asymmetric interactions among agents for action assessment. In particular, we propose an asymmetric interaction module (AIM), to explicitly model asymmetric interactions between intelligent agents within an action, where we group these agents into a primary one (e.g., human) and secondary ones (e.g., objects). We perform experiments on JIGSAWS dataset containing surgical actions, and additionally collect a new dataset, TASD-2, for interactive sporting actions. The experimental results on two interactive action datasets show the effectiveness of our model, and our method achieves state-of-the-art performance. The extended experiment on AQA-7 dataset also demonstrates the generalization capability of our framework to conventional action assessment. © 2020 Elsevier B.V., All rights reserved.","Intelligent Agents; Action Assessment; Asymmetric Interaction; Asymmetric Models; Generalization Capability; Medical Treatment; Motion Pattern; Real-world Scenario; State-of-the-art Performance; Computer Vision","Intelligent agents; Action assessment; Asymmetric interaction; Asymmetric models; Generalization capability; Medical treatment; Motion pattern; Real-world scenario; State-of-the-art performance; Computer vision","","","Funding text 1: This work was supported partially by the National Key Research and Development Program of China (2018YFB1004903), NSFC(U1911401, U1811461), Guangdong Province Science and Technology Innovation Leading Talents (2016TX03X157), Guangdong NSF Project (No. 2018B030312002), Guangzhou Research Project (201902010037), and Research Projects of Zhejiang Lab (No. 2019KD0AB03).; Funding text 2: Acknowledgement. This work was supported partially by the National Key Research and Development Program of China (2018YFB1004903), NSFC(U1911401, U1811461), Guangdong Province Science and Technology Innovation Leading Talents (2016TX03X157), Guangdong NSF Project (No. 2018B030312002), Guangzhou Research Project (201902010037), and Research Projects of Zhejiang Lab (No. 2019KD0AB03).","Bertasius, Gedas, Am i a Baller? Basketball Performance Assessment from First-Person Videos, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 2196-2204, (2017); Carreira, João, Quo Vadis, action recognition? A new model and the kinetics dataset, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2017-January, pp. 4724-4733, (2017); Chen, Jiaxin, Fast person Re-identification via cross-camera semantic binary transformation, 2017-January, pp. 5330-5339, (2017); Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2018); Corr, (2018); Miccai Workshop M2cai, (2014); Gattupalli, Srujana, CogniLearn: A deep learning-based interface for cognitive behavior assessment, pp. 577-587, (2017); Gers, Felix A., Learning to forget: Continual prediction with LSTM, IEE Conference Publication, 2, 470, pp. 850-855, (1999); Ilg, Winfried, Estimation of skill levels in sports based on hierarchical spatio-temporal correspondences, Lecture Notes in Computer Science, 2781, pp. 523-531, (2003); Li, Haoxin, Deep dual relation modeling for egocentric interaction recognition, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7924-7933, (2019)","Vedaldi, A.; Bischof, H.; Brox, T.; Frahm, J.-M.","Springer Science and Business Media Deutschland GmbH info@springer-sbm.com","","16th European Conference on Computer Vision, ECCV 2020","","Glasgow","249299","16113349; 03029743","9789819698936; 9789819698042; 9789819698110; 9789819698905; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141; 9783031984136","","","English","Conference paper","Final","","Scopus","2-s2.0-85092135682"
"M., Millan, Mégane; C., Achard, Catherine","Millan, Mégane (57216441350); Achard, Catherine (15060020700)","57216441350; 15060020700","Fine-tuning siamese networks to assess sport gestures quality","2020","","5","","","57","65","0","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083509396&partnerID=40&md5=2aa5b3d5d0d46d78bf15409d1bc36432","Sorbonne Université, Paris, France","Millan, Mégane, Sorbonne Université, Paris, France; Achard, Catherine, Sorbonne Université, Paris, France","This paper presents an Action Quality Assessment (AQA) approach that learns to automatically score action realization from temporal sequences like videos. To manage the small size of most of databases capturing actions or gestures, we propose to use Siamese Networks. In the literature, Siamese Networks are widely used to rank action scores. Indeed, their purpose is not to regress scores but to predict a value that respects true scores order so that it can be used to rank actions according to their quality. For AQA, we need to predict real scores, as well as the difference between these scores and their range. Thus, we first introduce a new loss function to train Siamese Networks in order to regress score gaps. Once the Siamese network is trained, a branch of this network is extracted and fine-tuned for score prediction. We tested our approach on a public database, the AQA-7 dataset, composed of videos from 7 sports. Our results outperform state of the art on AQA task. Moreover, we show that the proposed method is also more efficient for action ranking. © 2020 Elsevier B.V., All rights reserved.","Aqa; Deep Learning; Fine-tuning; Siamese Network; Computer Graphics; Forecasting; Sports; Fine Tuning; Loss Functions; Public Database; Quality Assessment; State Of The Art; Temporal Sequences; Computer Vision","Computer graphics; Forecasting; Sports; Fine tuning; Loss functions; Public database; Quality assessment; State of the art; Temporal sequences; Computer vision","","","","Corr, (2016); International Journal of Pattern Recognition and Artificial Intelligence, (1993); Bio Web of Conferences, (2011); International Conference on Learning Representations, (2019); Chung, Dahjung, A Two Stream Siamese Convolutional Neural Network for Person Re-identification, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 1992-2000, (2017); Doughty, Hazel, Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 6057-6066, (2018); Doughty, Hazel, The pros and cons: Rank-aware temporal attention for skill determination in long videos, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, pp. 7854-7863, (2019); Medical Image Computing and Computer Assisted Intervention Miccai, (2018); Funke, Isabel, Video-based surgical skill assessment using 3D convolutional neural networks, International Journal of Computer Assisted Radiology and Surgery, 14, 7, pp. 1217-1225, (2019); Miccai Workshop M2cai, (2014)","Farinella, G.M.; Radeva, P.; Braz, J.","SciTePress","Institute for Systems and Technologies of Information, Control and Communication (INSTICC)","15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP 2020","","Valletta","158743","","9789897584022","","","English","Conference paper","Final","","Scopus","2-s2.0-85083509396"
"P., Parmar, Paritosh; B.T., Morris, Brendan Tran","Parmar, Paritosh (57192919626); Morris, Brendan Tran (17435393600)","57192919626; 17435393600","What and how well you performed? a multitask learning approach to action quality assessment","2019","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June","","8954464","304","313","0","165","10.1109/CVPR.2019.00039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073532305&doi=10.1109%2FCVPR.2019.00039&partnerID=40&md5=ac64a60a2d8f00ee92aa40354470773c","University of Nevada, Las Vegas, Las Vegas, United States","Parmar, Paritosh, University of Nevada, Las Vegas, Las Vegas, United States; Morris, Brendan Tran, University of Nevada, Las Vegas, Las Vegas, United States","Can performance on the task of action quality assessment (AQA) be improved by exploiting a description of the action and its quality? Current AQA and skills assessment approaches propose to learn features that serve only one task-estimating the final score. In this paper, we propose to learn spatio-temporal features that explain three related tasks-fine-grained action recognition, commentary generation, and estimating the AQA score. A new multitask-AQA dataset, the largest to date, comprising of 1412 diving samples was collected to evaluate our approach (http://rtis.oit.unlv.edu/datasets.html). We show that our MTL approach outperforms STL approach using two different kinds of architectures: C3D-AVG and MSCADC. The C3D-AVG-MTL approach achieves the new state-of-the-art performance with a rank correlation of 90.44%. Detailed experiments were performed to show that MTL offers better generalization than STL, and representations from action recognition models are not sufficient for the AQA task and instead should be learned. © 2020 Elsevier B.V., All rights reserved.","Action Recognition; And Body Pose; Datasets And Evaluation; Face; Gesture; Video Analytics; Linearization; Action Recognition; Body Pose; Datasets And Evaluation; Face; Gesture; Video Analytics; Computer Vision","Linearization; Action recognition; Body pose; Datasets and Evaluation; Face; Gesture; Video analytics; Computer vision","","","","Bertasius, Gedas, Am i a Baller? Basketball Performance Assessment from First-Person Videos, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 2196-2204, (2017); Caruana, Rich A., Multitask Learning, Machine Learning, 28, 1, pp. 41-75, (1997); Learning Phrase Representations Using Rnn Encoder Decoder for Statistical Machine Translation, (2014); Doughty, Hazel, Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 6057-6066, (2018); Corr, (2018); Gupta, Sonal, Watch, listen & learn: Co-training on captioned images and videos, Lecture Notes in Computer Science, 5211 LNAI, PART 1, pp. 457-472, (2008); Gupta, Sonal, Using closed captions to train activity recognizers that improve video retrieval, pp. 30-37, (2009); Hara, Kensho, Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 6546-6555, (2018); Ioffe, Sergey, Batch normalization: Accelerating deep network training by reducing internal covariate shift, 1, pp. 448-456, (2015); Medical Image Computing and Computer Assisted Intervention Miccai, (2018)","","IEEE Computer Society","","32nd IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2019","","Long Beach; CA","156730","10636919","0769526462; 9798350301298; 9781665445092; 9798350353006; 9781467388511; 9781457703942; 9781728171685; 0818684976; 9781665469463; 1424411807","PIVRE","","English","Conference paper","Final","","Scopus","2-s2.0-85073532305"
"P., Parmar, Paritosh; B.T., Morris, Brendan Tran","Parmar, Paritosh (57192919626); Morris, Brendan Tran (17435393600)","57192919626; 17435393600","Action quality assessment across multiple actions","2019","","","","8658709","1468","1476","0","133","10.1109/WACV.2019.00161","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063587258&doi=10.1109%2FWACV.2019.00161&partnerID=40&md5=72a5cabd7684909a59e09fef0e6090d4","University of Nevada, Las Vegas, Las Vegas, United States","Parmar, Paritosh, University of Nevada, Las Vegas, Las Vegas, United States; Morris, Brendan Tran, University of Nevada, Las Vegas, Las Vegas, United States","Can learning to measure the quality of an action help in measuring the quality of other actions? If so, can consolidated samples from multiple actions help improve the performance of current approaches? In this paper, we carry out experiments to see if knowledge transfer is possible in the action quality assessment (AQA) setting. Experiments are carried out on our newly released AQA dataset (http://rtis.oit.unlv.edu/datasets.html) consisting of 1106 action samples from seven actions with quality as measured by expert human judges. Our experimental results show that there is utility in learning a single model across multiple actions. © 2019 Elsevier B.V., All rights reserved.","Knowledge Management; Knowledge Transfer; Quality Assessment; Single Models; Computer Vision","Knowledge management; Knowledge transfer; Quality assessment; Single models; Computer vision","","","","Classification with Many Classes Challenges and Pluses; Bertasius, Gedas, Am i a Baller? Basketball Performance Assessment from First-Person Videos, Proceedings of the IEEE International Conference on Computer Vision, 2017-October, pp. 2196-2204, (2017); Olympic Figure Skating Controversy Judging System is Most to Blame for Uproar, (2014); Deng, Jia, ImageNet: A Large-Scale Hierarchical Image Database, pp. 248-255, (2009); Whos Better Whos Best Skill Determination in Video Using Deep Ranking, (2017); A Tutorial on Correlation Coefficients, (2011); Girshick, Ross B., Region-Based Convolutional Networks for Accurate Object Detection and Segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 38, 1, pp. 142-158, (2016); Hochreiter, Sepp, Long Short-Term Memory, Neural Computation, 9, 8, pp. 1735-1780, (1997); Jarrett, Kevin, What is the best multi-stage architecture for object recognition?, Proceedings of the IEEE International Conference on Computer Vision, pp. 2146-2153, (2009); Caffe Convolutional Architecture for Fast Feature Embedding, (2014)","","Institute of Electrical and Electronics Engineers Inc.","IEEE Biometrics Council; IEEE Computer Society","19th IEEE Winter Conference on Applications of Computer Vision, WACV 2019","","Waikoloa Village; HI","145827","","9781728119755","","","English","Conference paper","Final","","Scopus","2-s2.0-85063587258"
"Y., Li, Yongjun; X., Chai, Xiujuan; X., Chen, Xilin","Li, Yongjun (57872497900); Chai, Xiujuan (7006781144); Chen, Xilin (8284171300)","57872497900; 7006781144; 8284171300","ScoringNet: Learning Key Fragment for Action Quality Assessment with Ranking Loss in Skilled Sports","2019","Lecture Notes in Computer Science","11366 LNCS","","","149","164","0","23","10.1007/978-3-030-20876-9_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066957611&doi=10.1007%2F978-3-030-20876-9_10&partnerID=40&md5=6e6bf82f198b6d4a93938d0f9c217277","Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; Chinese Academy of Agricultural Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","Li, Yongjun, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China, University of Chinese Academy of Sciences, Beijing, China; Chai, Xiujuan, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China, Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, China; Chen, Xilin, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China, University of Chinese Academy of Sciences, Beijing, China","Nowadays, scoring athletes’ performance in skilled sports automatically has drawn more and more attention from the academic community. However, extracting effective features and predicting reasonable scores for a long skilled sport video still beset researchers. In this paper, we introduce the ScoringNet, a novel network consisting of key fragment segmentation (KFS) and score prediction (SP), to address these two problems. To get the effective features, we design KFS to obtain key fragments and remove irrelevant fragments by semantic video segmentation. Then a 3D convolutional neural network extracts features from each key fragment. In score prediction, we fuse the ranking loss into the traditional loss function to make the predictions more reasonable in terms of both the score value and the ranking aspects. Through the deep learning, we narrow the gap between the predictions and ground-truth scores as well as making the predictions satisfy the ranking constraint. Widely experiments convincingly show that our method achieves the state-of-the-art results on three datasets. © 2019 Elsevier B.V., All rights reserved.","Action Quality Assessment; Key Fragment Segmentation; Ranking Loss; Deep Learning; Forecasting; Neural Networks; Semantics; Sports; Academic Community; Convolutional Neural Network; Ground Truth; Loss Functions; Quality Assessment; Sport Video; State Of The Art; Video Segmentation; Computer Vision","Deep learning; Forecasting; Neural networks; Semantics; Sports; Academic community; Convolutional neural network; Ground truth; Loss functions; Quality assessment; Sport video; State of the art; Video segmentation; Computer vision","","","This work was partially supported by 973 Program under contract No. 2015CB351802, Natural Science Foundation of China under contracts Nos. 61390511, 61472398, 61532018.","List of Olympic Games Scandals and Controversies; undefined; undefined; undefined; Pirsiavash, Hamed, Assessing the quality of actions, Lecture Notes in Computer Science, 8694 LNCS, PART 6, pp. 556-571, (2014); Tao, Lili, A comparative study of pose representation and dynamics modelling for online motion quality assessment, Computer Vision and Image Understanding, 148, pp. 136-152, (2016); Parisi, German Ignacio, Human motion assessment in real time using recurrent self-organization, pp. 71-76, (2016); Parmar, Paritosh, Measuring the quality of exercises, Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings, 2016-October, pp. 2241-2244, (2016); Parmar, Paritosh, Learning to Score Olympic Events, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2017-July, pp. 76-84, (2017); Zia, Aneeq, Automated assessment of surgical skills using frequency analysis, Lecture Notes in Computer Science, 9349, pp. 430-438, (2015)","Li, H.; Schindler, K.; Mori, G.; Jawahar, C.V.","Springer Verlag service@springer.de","","14th Asian Conference on Computer Vision, ACCV 2018","","Perth; WA","226489","16113349; 03029743","9789819698936; 9789819698042; 9789819698110; 9789819698905; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141; 9783031984136","","","English","Conference paper","Final","","Scopus","2-s2.0-85066957611"
"Y., Li, Yongjun; X., Chai, Xiujuan; X., Chen, Xilin","Li, Yongjun (57872497900); Chai, Xiujuan (7006781144); Chen, Xilin (8284171300)","57872497900; 7006781144; 8284171300","End-to-end learning for action quality assessment","2018","Lecture Notes in Computer Science","11165 LNCS","","","125","134","0","43","10.1007/978-3-030-00767-6_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057277239&doi=10.1007%2F978-3-030-00767-6_12&partnerID=40&md5=59a87b78e28e5f7d6bb50a6dfbb2ab09","Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; Chinese Academy of Agricultural Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","Li, Yongjun, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China, University of Chinese Academy of Sciences, Beijing, China; Chai, Xiujuan, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China, Agricultural Information Institute, Chinese Academy of Agricultural Sciences, Beijing, China; Chen, Xilin, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China, University of Chinese Academy of Sciences, Beijing, China","Nowadays, action quality assessment has attracted more and more attention of the researchers in computer vision. In this paper, an end-to-end framework is proposed based on fragment-based 3D convolutional neural network to realize the action quality assessment in videos. Furthermore, the ranking loss integrated with the MSE forms the loss function to make the optimization more reasonable in terms of both the score value and the ranking aspects. Through the deep learning, we narrow the gap between the predictions and ground-truth scores as well as making the predictions satisfy the ranking constraint. The proposed network can indeed learn the evaluation criteria of actions and works well with limited training data. Widely experiments conducted on three public datasets convincingly show that our method achieves the state-of-the-art results. © 2019 Elsevier B.V., All rights reserved.","3d Convolutional Neural Network; Action Quality Assessment; Deep Learning; Ranking Loss; Convolution; Deep Learning; Neural Networks; Petroleum Reservoir Evaluation; Convolutional Neural Network; End To End; Evaluation Criteria; Ground Truth; Limited Training Data; Loss Functions; Quality Assessment; State Of The Art; Quality Of Service","Convolution; Deep learning; Neural networks; Petroleum reservoir evaluation; Convolutional neural network; End to end; Evaluation criteria; Ground truth; Limited training data; Loss functions; Quality assessment; State of the art; Quality of service","","","This work was partially supported by 973 Program under contract No2015CB351802, Natural Science Foundation of China under contracts Nos. 61390511, 61472398, 61532018.","Vault; List of Olympic Games Scandals and Controversies; Fina Diving Rules; Pirsiavash, Hamed, Assessing the quality of actions, Lecture Notes in Computer Science, 8694 LNCS, PART 6, pp. 556-571, (2014); Tao, Lili, A comparative study of pose representation and dynamics modelling for online motion quality assessment, Computer Vision and Image Understanding, 148, pp. 136-152, (2016); Parmar, Paritosh, Measuring the quality of exercises, Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings, 2016-October, pp. 2241-2244, (2016); Parmar, Paritosh, Learning to Score Olympic Events, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2017-July, pp. 76-84, (2017); Zia, Aneeq, Automated assessment of surgical skills using frequency analysis, Lecture Notes in Computer Science, 9349, pp. 430-438, (2015); Carvajal, Johanna, Towards Miss Universe automatic prediction: The evening gown competition, Proceedings - International Conference on Pattern Recognition, 0, pp. 1089-1094, (2016); Tran, Du, Learning spatiotemporal features with 3D convolutional networks, Proceedings of the IEEE International Conference on Computer Vision, 2015 International Conference on Computer Vision, ICCV 2015, pp. 4489-4497, (2015)","Cheng, W.-H.; Yamasaki, T.; Ngo, C.-W.; Hong, R.; Wang, M.","Springer Verlag service@springer.de","","19th Pacific-Rim Conference on Multimedia, PCM 2018","","Hefei","218709","16113349; 03029743","9789819698936; 9789819698042; 9789819698110; 9789819698905; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141; 9783031984136","","","English","Conference paper","Final","","Scopus","2-s2.0-85057277239"
"P., Parmar, Paritosh; B.T., Morris, Brendan Tran","Parmar, Paritosh (57192919626); Morris, Brendan Tran (17435393600)","57192919626; 17435393600","Learning to Score Olympic Events","2017","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2017-July","","8014750","76","84","0","155","10.1109/CVPRW.2017.16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030226262&doi=10.1109%2FCVPRW.2017.16&partnerID=40&md5=4413f6f0cb7beaf9786c4901bc049086","University of Nevada, Las Vegas, Las Vegas, United States","Parmar, Paritosh, University of Nevada, Las Vegas, Las Vegas, United States; Morris, Brendan Tran, University of Nevada, Las Vegas, Las Vegas, United States","Estimating action quality, the process of assigning a 'score' to the execution of an action, is crucial in areas such as sports and health care. Unlike action recognition, which has millions of examples to learn from, the action quality datasets that are currently available are small-typically comprised of only a few hundred samples. This work presents three frameworks for evaluating Olympic sports which utilize spatiotemporal features learned using 3D convolutional neural networks (C3D) and perform score regression with i) SVR ii) LSTM and iii) LSTM followed by SVR. An efficient training mechanism for the limited data scenarios is presented for clip-based training with LSTM. The proposed systems show significant improvement over existing quality assessment approaches on the task of predicting scores of diving, vault, figure skating. SVR-based frameworks yield better results, LSTM-based frameworks are more natural for describing an action and can be used for improvement feedback. © 2017 Elsevier B.V., All rights reserved.","Computer Vision; Neural Networks; Sports; Action Recognition; Convolutional Neural Network; Figure Skating; Limited Data; Olympics; Quality Assessment; Spatio Temporal Features; Pattern Recognition","Computer vision; Neural networks; Sports; Action recognition; Convolutional neural network; Figure skating; Limited data; Olympics; Quality assessment; Spatio temporal features; Pattern recognition","","","","Olympic Figure Skating Controversy Judging System is Most to Blame for Uproar, (2014); Caffe Convolutional Architecture for Fast Feature Embedding, (2014); Karpathy, Andrej, Large-scale video classification with convolutional neural networks, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1725-1732, (2014); Kuehne, Hilde, HMDB: A large video database for human motion recognition, Proceedings of the IEEE International Conference on Computer Vision, pp. 2556-2563, (2011); Le, Quocviet, Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3361-3368, (2011); Parmar, Paritosh, Measuring the quality of exercises, Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings, 2016-October, pp. 2241-2244, (2016); Pirsiavash, Hamed, Assessing the quality of actions, Lecture Notes in Computer Science, 8694 LNCS, PART 6, pp. 556-571, (2014); Corr, (2012); Tran, Du, Learning spatiotemporal features with 3D convolutional networks, Proceedings of the IEEE International Conference on Computer Vision, 2015 International Conference on Computer Vision, ICCV 2015, pp. 4489-4497, (2015); Long Term Temporal Convolutions for Action Recognition, (2016)","","IEEE Computer Society help@computer.org","","30th IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPRW 2017","","Honolulu; HI","130113","21607516; 21607508","9781479943098; 9781457705298; 9781665448994; 9781538607336; 9798350302493; 9780769549903; 9781665487399; 9781467367592; 9781728125060; 9781728193601","","","English","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85030226262"
"H., Pirsiavash, Hamed; C., Vondrick, Carl; A., Torralba, A.","Pirsiavash, Hamed (22235347900); Vondrick, Carl (36618122700); Torralba, A. (7005432728)","22235347900; 36618122700; 7005432728","Assessing the quality of actions","2014","Lecture Notes in Computer Science","8694 LNCS","PART 6","","556","571","0","177","10.1007/978-3-319-10599-4_36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906351376&doi=10.1007%2F978-3-319-10599-4_36&partnerID=40&md5=98a9a92aae2fb90fa3c5c1308f6e9cff","Massachusetts Institute of Technology, Cambridge, United States","Pirsiavash, Hamed, Massachusetts Institute of Technology, Cambridge, United States; Vondrick, Carl, Massachusetts Institute of Technology, Cambridge, United States; Torralba, A., Massachusetts Institute of Technology, Cambridge, United States","While recent advances in computer vision have provided reliable methods to recognize actions in both images and videos, the problem of assessing how well people perform actions has been largely unexplored in computer vision. Since methods for assessing action quality have many real-world applications in healthcare, sports, and video retrieval, we believe the computer vision community should begin to tackle this challenging problem. To spur progress, we introduce a learning-based framework that takes steps towards assessing how well people perform actions in videos. Our approach works by training a regression model from spatiotemporal pose features to scores obtained from expert judges. Moreover, our approach can provide interpretable feedback on how people can improve their action. We evaluate our method on a new Olympic sports dataset, and our experiments suggest our framework is able to rank the athletes more accurately than a non-expert human. While promising, our method is still a long way to rivaling the performance of expert judges, indicating that there is significant opportunity in computer vision research to improve on this difficult yet important task. © 2014 Springer International Publishing. © 2014 Elsevier B.V., All rights reserved.","Regression Analysis; Olympics; Real-world; Regression Model; Reliable Methods; Video Retrieval; Vision Communities; Sports","Regression analysis; Olympics; Real-world; Regression model; Reliable methods; Video retrieval; Vision communities; Sports","","","","Proceedings of AI ED, (1995); Trajectory Based Assessment of Coordinated Human Activity, (2003); Computer Vision Winter Workshop 2007 St Lambrecht, (2007); Pirsiavash, Hamed, Detecting activities of daily living in first-person camera views, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 2847-2854, (2012); Ke, Yan, The design of high-level features for photo quality assessment, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1, pp. 419-426, (2006); Interestingness of Images, (2013); Eccv, (2006); Dhar, Sagnik, High level describable attributes for predicting aesthetics and interestingness, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1657-1664, (2011); Gupta, Abhinav, Observing human-object interactions: Using spatial and functional compatibility for recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence, 31, 10, pp. 1775-1789, (2009); Eccv, (2012)","","Springer Verlag service@springer.de","","13th European Conference on Computer Vision, ECCV 2014","","Zurich","107073","16113349; 03029743","9789819698936; 9789819698042; 9789819698110; 9789819698905; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141; 9783031984136","","","English","Conference paper","Final","All Open Access; Bronze Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-84906351376"
