DOI,Title,Authors,Year,Abstract,Include,Reason,Data Source
10.1109/ACCESS.2024.3423462,MMW-AQA: Multimodal In-the-Wild Dataset for Action Quality Assessment,T. Nagai; S. Takeda; S. Suzuki; H. Seshimo,2024,"Action quality assessment (AQA) is a task for assessing a specific action quality in videos. Since existing AQA datasets provide only two-dimensional (2D) video data captured from fewer viewpoints, existing AQA methods based on deep neural networks (DNNs) often struggle to assess complex three-dimensional (3D) actions accurately, and their robustness against diversified viewpoints remains unknown. We created a dataset called multimodal in-the-wild (MMW)-AQA in freestyle windsurfing that addresses these concerns. In addition to video data, MMW-AQA provides inertial measurement unit (IMU) and global positioning system (GPS) data. The 3D information of IMU data helps DNNs accurately assess complex 3D actions. Moreover, MMW-AQA provides wild video data captured by a single unmanned aerial vehicle (UAV). These wild video data enable us to evaluate whether AQA methods can work well on diversified viewpoints. Furthermore, we also present the baseline multimodalization framework with a transformer-based fusion module. These frameworks multimodalize existing unimodal DNN models easily to assess action quality using multimodal data. Our experimental results demonstrate that multimodal data improves the AQA accuracy compared with unimodal video data.",yes,"The article introduces a new multimodal dataset (MMW-AQA) for Action Quality Assessment, which is a computer vision task.",MMW-AQA
10.1109/ICIP42928.2021.9506257,Action Quality Assessment With Ignoring Scene Context,T. Nagai; S. Takeda; M. Matsumura; S. Shimizu; S. Yamamoto,2021,"We propose an action quality assessment (AQA) method that can specifically assess target action quality with ignoring scene context, which is a feature unrelated to the target action. Existing AQA methods have tried to extract spatiotemporal features related to the target action by applying 3D convolution to the video. However, since their models are not explicitly designed to extract the features of the target action, they mis-extract scene context and thus cannot assess the target action quality correctly. To overcome this problem, we impose two losses to an existing AQA model: scene adversarial loss and our newly proposed human-masked regression loss. The scene adversarial loss encourages the model to ignore scene context by adversarial training. Our human-masked regression loss does so by making the correlation between score outputs by an AQA model and human referees undefinable when the target action is not visible. These two losses lead the model to specifically assess the target action quality with ignoring scene context. We evaluated our method on a diving dataset commonly used for AQA and found that it outperformed current state-of-the-art methods. This result shows that our method is effective in ignoring scene context while assessing the target action quality.",yes,The article proposes an AQA method using a diving dataset and vision-based techniques.,
10.1109/CVPR52688.2022.00296,FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment,J. Xu; Y. Rao; X. Yu; G. Chen; J. Zhou; J. Lu,2022,"Most existing action quality assessment methods rely on the deep features of an entire video to predict the score, which is less reliable due to the non-transparent inference process and poor interpretability. We argue that understanding both high-level semantics and internal temporal structures of actions in competitive sports videos is the key to making predictions accurate and interpretable. Towards this goal, we construct a new fine-grained dataset, called FineDiving, developed on diverse diving events with detailed annotations on action procedures. We also propose a procedure-aware approach for action quality assessment, learned by a new Temporal Segmentation Attention module. Specifically, we propose to parse pairwise query and exemplar action instances into consecutive steps with diverse semantic and temporal correspondences. The procedure-aware cross-attention is proposed to learn embeddings between query and exemplar steps to discover their semantic, spatial, and temporal correspondences, and further serve for fine-grained contrastive regression to derive a reliable scoring mechanism. Extensive experiments demonstrate that our approach achieves substantial improvements over the state-of-the-art methods with better interpretability. The dataset and code are available at https://github.com/xujinglin/FineDiving.",yes,"The article introduces a new dataset (FineDiving) for Action Quality Assessment, a computer vision task, and makes it publicly available.",FineDiving
10.1109/CVPR52729.2023.00238,LOGO: A Long-Form Video Dataset for Group Action Quality Assessment,S. Zhang; W. Dai; S. Wang; X. Shen; J. Lu; J. Zhou; Y. Tang,2023,"Action quality assessment (AQA) has become an emerging topic since it can be extensively applied in numerous scenarios. However, most existing methods and datasets focus on single-person short-sequence scenes, hindering the application of AQA in more complex situations. To address this issue, we construct a new multi-person long-form video dataset for action quality assessment named LOGO. Distinguished in scenario complexity, our dataset contains 200 videos from 26 artistic swimming events with 8 athletes in each sample along with an average duration of 204.2 seconds. As for richness in annotations, LOGO includes formation labels to depict group information of multiple athletes and detailed annotations on action procedures. Furthermore, we propose a simple yet effective method to model relations among athletes and reason about the potential temporal logic in long-form videos. Specifically, we design a group-aware attention module, which can be easily plugged into existing AQA methods, to enrich the clip-wise representations based on contextual group information. To benchmark LOGO, we systematically conduct investigations on the performance of several popular methods in AQA and action segmentation. The results reveal the challenges our dataset brings. Extensive experiments also show that our approach achieves state-of-the-art on the LOGO dataset. The dataset and code will be released at https://github.com/shiyi-zh0408/LOGO.",yes,"The article introduces a new video dataset (LOGO) for Group Action Quality Assessment, a computer vision task, and states it will be released.",LOGO
10.1109/ICMLC58545.2023.10327994,Action Quality Assessment for ASD Behaviour Evaluation,D. Zhang; D. Zhou; H. Liu,2023,"Given the current increasing prevalence of autism, expensive and time-consuming manual diagnosis is highly detrimental to the management of the condition. With the development of computer-based methods of human behavioural analysis, these methods are expected to provide more accurate, objective and reproducible methods of early screening and diagnosis of autism. To advance the field of behavioural quantification in autism research, this study utilises human skeletal behavioural data from publicly available autism datasets and ADOS scores from clinical professionals in a first attempt to build deep neural networks that can predict ADOS scores from behavioural data using the AQA approach. This paper finds a moderately correlated between the ground truth ADOS score and the predicted ADOS score, it reveals the potential use of the AQA method in ASD diagnoses.",yes,"The article applies AQA methods to publicly available autism datasets for behavioral evaluation, which is a computer science task.",
10.1109/WACV57701.2024.00012,PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment,A. Dadashzadeh; S. Duan; A. Whone; M. Mirmehdi,2024,"The limited availability of labelled data in Action Quality Assessment (AQA), has forced previous works to fine-tune their models pretrained on large-scale domain-general datasets. This common approach results in weak generalisation, particularly when there is a significant domain shift. We propose a novel, parameter efficient, continual pretraining framework, PECoP, to reduce such domain shift via an additional pretraining stage. In PECoP, we introduce 3D-Adapters, inserted into the pretrained model, to learn spatiotemporal, in-domain information via self-supervised learning where only the adapter modules’ parameters are updated. We demonstrate PECoP’s ability to enhance the performance of recent state-of-the-art methods (MUSDL, CoRe, and TSA) applied to AQA, leading to considerable improvements on benchmark datasets, JIGSAWS (↑ 6.0%), MTL-AQA (↑ 0.99%), and FineDiving (↑ 2.54%). We also present a new Parkinson’s Disease dataset, PD4T, of real patients performing four various actions, where we surpass (↑ 3.56%) the state-of-the-art in comparison. Our code, pretrained models, and the PD4T dataset are available at https://github.com/Plrbear/PECoP.",yes,"The article proposes a method for Action Quality Assessment, evaluates it on multiple benchmark datasets, and introduces a new dataset (PD4T), with all data publicly available.",JIGSAWS;MTL-AQA;FineDiving;PD4T
10.1109/MMSP55362.2022.9949464,Tai Chi Action Quality Assessment and Visual Analysis with a Consumer RGB-D Camera,J. Li; H. Hu; Q. Xing; X. Wang; J. Li; Y. Shen,2022,"Visual-based human action analysis is an important research topic in the field of computer vision, and has great application prospect in intelligent sports. Home-based fitness is increasingly common in recent years, however lacking of accurate feedback and scientific guidance main easily lead to problems such as exercise injuries. In this paper, we propose an analysis system for Tai Chi action quality assessment and visual analysis with a consumer RGB-D camera. The main innovative work is as follows: 1) for home-based fitness action evaluation, we design a real-time intelligent analysis system combined with expert rules through a consumer RGB-D camera; 2) we transform the evaluation of 24-form Tai Chi Chuan into an artificial intelligence (AI) model, and realize action recognition and assessment through computer vision; 2) to train the AI model, we build a new dataset named TaiChi-24, which contains 1,408 samples with RGB-D images and 3D skeletons. We carry out evaluation experiments and analyses, and the experimental results have shown the advantage of applying our evaluation method on the proposed TaiChi-24 dataset.",yes,The article proposes a system for Tai Chi Action Quality Assessment using a consumer RGB-D camera and builds a new dataset (TaiChi-24).,TaiChi-24
10.1109/INSAI54028.2021.00029,A Survey of Video-based Action Quality Assessment,S. Wang; D. Yang; P. Zhai; Q. Yu; T. Suo; Z. Sun; K. Li; L. Zhang,2021,"Human action recognition and analysis have great demand and important application significance in video surveillance, video retrieval, and human-computer interaction. The task of human action quality evaluation requires the intelligent system to automatically and objectively evaluate the action completed by the human. The action quality assessment model can reduce the human and material resources spent in action evaluation and reduce subjectivity. In this paper, we provide a comprehensive survey of existing papers on video-based action quality assessment. Different from human action recognition, the application scenario of action quality assessment is relatively narrow. Most of the existing work focuses on sports and medical care. We first introduce the definition and challenges of human action quality assessment. Then we present the existing datasets and evaluation metrics. In addition, we summarized the methods of sports and medical care according to the model categories and publishing institutions according to the characteristics of the two fields. At the end, combined with recent work, the promising development direction in action quality assessment is discussed.",no,The article is a survey of video-based action quality assessment.,
10.1109/CVPR52733.2024.01386,FineParser: A Fine-Grained Spatio-Temporal Action Parser for Human-Centric Action Quality Assessment,J. Xu; S. Yin; G. Zhao; Z. Wang; Y. Peng,2024,"Existing action quality assessment (AQA) methods mainly learn deep representations at the video level for scoring diverse actions. Due to the lack of a fine-grained understanding of actions in videos, they harshly suffer from low credibility and interpretability, thus insufficient for stringent applications, such as Olympic diving events. We argue that a fine-grained understanding of actions requires the model to perceive and parse actions in both time and space, which is also the key to the credibility and inter-pretability of the AQA technique. Based on this insight, we propose a new fine-grained spatial-temporal action parser named FineParser. It learns human-centric foreground action representations by focusing on target action regions within each frame and exploiting their fine-grained alignments in time and space to minimize the impact of in-valid backgrounds during the assessment. In addition, we construct fine-grained annotations of human-centric fore-ground action masks for the FineDiving dataset, called FineDiving-HM. With refined annotations on diverse target action procedures, FineDiving-HM can promote the development of real-world AQA systems. Through extensive experiments, we demonstrate the effectiveness of FineParser, which outperforms state-of-the-art methods while supporting more tasks of fine-grained action understanding. Data and code are available at https://github.com/PKU-ICST-MIPL/FineParser_CVPR2024.",yes,"The article proposes a method for Action Quality Assessment, uses the FineDiving dataset, and constructs new annotations (FineDiving-HM), with data publicly available.",FineDiving;FineDiving-HM
10.1109/CVPRW63382.2024.00324,FineRehab: A Multi-modality and Multi-task Dataset for Rehabilitation Analysis,J. Li; J. Xue; R. Cao; X. Du; S. Mo; K. Ran; Z. Zhang,2024,"The assessment of rehabilitation exercises for neurological and musculoskeletal disorders are crucial for recovery. Traditionally, assessment methods have been subjective, with inherent uncertainty and limitations. This paper introduces a novel multi-modality dataset named FineRehab§ to prompt the study of rehabilitation movement analysis, leveraging advancements in sensor technology and artificial intelligence. FineRehab collects 16 actions from 50 participants, including both patients with musculoskeletal disorders and healthy individuals, and consists of 4,215 action samples captured by two Kinect cameras and 17 IMUs. To benchmark FineRehab, we present a reliable approach to analyze rehabilitation exercises, and make experiments to evaluate the comprehensive movement quality from across multi-dimensions. Comparative experimental analyses have verified the validity of our dataset in distinguishing between the movement of the normal population and patients, which can offer a quantifiable basis for personalized rehabilitation feedback. The introduction of FineRehab will encourage researchers to apply, develop and adapt various methods for rehabilitation exercise analysis.",yes,"The article introduces a new multi-modality dataset (FineRehab) for rehabilitation movement analysis, which involves assessing the quality of human actions using vision-based methods.",FineRehab
10.1109/CVPR42600.2020.00986,Uncertainty-Aware Score Distribution Learning for Action Quality Assessment,Y. Tang; Z. Ni; J. Zhou; D. Zhang; J. Lu; Y. Wu; J. Zhou,2020,"Assessing action quality from videos has attracted growing attention in recent years. Most existing approaches usually tackle this problem based on regression algorithms, which ignore the intrinsic ambiguity in the score labels caused by multiple judges or their subjective appraisals. To address this issue, we propose an uncertainty-aware score distribution learning (USDL) approach for action quality assessment (AQA). Specifically, we regard an action as an instance associated with a score distribution, which describes the probability of different evaluated scores. Moreover, under the circumstance where finer-grained score labels are available (e.g., difficulty degree of an action or multiple scores from different judges), we further devise a multi-path uncertainty-aware score distribution learning (MUSDL) method to explore the disentangled components of a score. In order to demonstrate the effectiveness of our proposed methods, We conduct experiments on two AQA datasets containing various Olympic actions. Our approaches set new state-of-the-arts under the Spearman's Rank Correlation (i.e., 0.8102 on AQA-7 and 0.9273 on MTL-AQA).",yes,The article proposes a method for Action Quality Assessment and evaluates it on two AQA datasets.,AQA-7;MTL-AQA
10.1109/ICCV48922.2021.00782,Group-aware Contrastive Regression for Action Quality Assessment,X. Yu; Y. Rao; W. Zhao; J. Lu; J. Zhou,2021,"Assessing action quality is challenging due to the subtle differences between videos and large variations in scores. Most existing approaches tackle this problem by regressing a quality score from a single video, suffering a lot from the large inter-video score variations. In this paper, we show that the relations among videos can provide important clues for more accurate action quality assessment during both training and inference. Specifically, we reformulate the problem of action quality assessment as regressing the relative scores with reference to another video that has shared attributes (e.g., category and difficulty), instead of learning unreferenced scores. Following this formulation, we propose a new Contrastive Regression (CoRe) framework to learn the relative scores by pair-wise comparison, which highlights the differences between videos and guides the models to learn the key hints for assessment. In order to further exploit the relative information between two videos, we devise a group-aware regression tree to convert the conventional score regression into two easier sub-problems: coarse-to-fine classification and regression in small intervals. To demonstrate the effectiveness of CoRe, we conduct extensive experiments on three mainstream AQA datasets including AQA-7, MTL-AQA and JIGSAWS. Our approach outperforms previous methods by a large margin and establishes new state-of-the-art on all three benchmarks.",yes,The article proposes a method for Action Quality Assessment and evaluates it on three mainstream AQA datasets.,AQA-7;MTL-AQA;JIGSAWS
10.1109/TCE.2024.3482560,ResFNN: Residual Structure-Based Feedforward Neural Network for Action Quality Assessment in Sports Consumer Electronics,H. Gao; S. Yu; M. Iqbal; M. Guizani,2024,"With the development of artificial intelligence (AI) and sports consumer electronics, AI-empowered Olympic sport technologies are being implemented more extensively. Action quality assessment (AQA), a sport action recognition and video refereeing technology, aims to automatically score action performance in videos obtained from sports consumer electronics deployed in arenas. It has gained much attention for its wide range of applications, such as sports event scoring, specific skill assessment, and rehabilitation medicine. General methods score action performance by directly regressing the initial video features to score, which neglects the possibility that the initial features are insufficiently effective. To address this issue, we propose a residual structure-based feedforward neural network (ResFNN) that enables efficient action feature learning to attain improved score assessment performance. First, the input videos are downsampled to clips and passed through inflated 3D convolutional networks (ConvNets) to obtain initial action video features. These features contain spatiotemporal information about the human actions occurring in the videos. Second, these features are aggregated and learned through our ResFNN. The ResFNN is composed of feedforward neural network residual blocks, which have strong function fitting and feature conversion capabilities. Therefore, the network learns features well and obtains more effective features. Third, a score distribution regression method is applied to obtain the underlying score distribution. This step establishes a more accurate mapping between the videos and scores. Finally, our method is demonstrated to outperform the majority of the existing methods through experiments conducted on the AQA-7, MTL-AQA, and JIGSAWS datasets.",yes,The article proposes a method for Action Quality Assessment using vision-based techniques and evaluates it on three AQA datasets.,AQA-7;MTL-AQA;JIGSAWS
10.1109/WACV.2019.00161,Action Quality Assessment Across Multiple Actions,P. Parmar; B. Morris,2019,"Can learning to measure the quality of an action help in measuring the quality of other actions? If so, can consolidated samples from multiple actions help improve the performance of current approaches? In this paper, we carry out experiments to see if knowledge transfer is possible in the action quality assessment (AQA) setting. Experiments are carried out on our newly released AQA dataset (http://rtis.oit.unlv.edu/datasets.html) consisting of 1106 action samples from seven actions with quality as measured by expert human judges. Our experimental results show that there is utility in learning a single model across multiple actions.",yes,The article investigates knowledge transfer in Action Quality Assessment and releases a new AQA dataset.,AQA-7
10.1109/CVPR.2019.00039,What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment,P. Parmar; B. T. Morris,2019,"Can performance on the task of action quality assessment (AQA) be improved by exploiting a description of the action and its quality? Current AQA and skills assessment approaches propose to learn features that serve only one task - estimating the final score. In this paper, we propose to learn spatio-temporal features that explain three related tasks - fine-grained action recognition, commentary generation, and estimating the AQA score. A new multitask-AQA dataset, the largest to date, comprising of 1412 diving samples was collected to evaluate our approach (http://rtis.oit.unlv.edu/datasets.html). We show that our MTL approach outperforms STL approach using two different kinds of architectures: C3D-AVG and MSCADC. The C3D-AVG-MTL approach achieves the new state-of-the-art performance with a rank correlation of 90.44%. Detailed experiments were performed to show that MTL offers better generalization than STL, and representations from action recognition models are not sufficient for the AQA task and instead should be learned.",yes,"The article proposes a multitask learning approach for Action Quality Assessment and collects a new multitask-AQA dataset, which is publicly available.",MTL-AQA
10.1109/MMSP53017.2021.9733638,Piano Skills Assessment,P. Parmar; J. Reddy; B. Morris,2021,"Can a computer determine a piano player’s skill level? Is it preferable to base this assessment on visual analysis of the player’s performance or should we trust our ears over our eyes? Since current convolutional neural networks (CNNs) have difficulty processing long video videos, how can shorter clips be sampled to best reflect the players skill level? In this work, we collect and release a first-of-its-kind dataset for multimodal skill assessment focusing on assessing piano player’s skill level, answer the asked questions, initiate work in automated evaluation of piano playing skills and provide baselines for future work. Dataset can be accessed from: https://github.com/ParitoshParmar/Piano-Skills-Assessment.",yes,"The article introduces a new dataset for piano skills assessment, which involves assessing the quality of human actions using visual analysis, and makes it publicly available.",PISA
10.1109/TNSRE.2023.3317411,A Contrastive Learning Network for Performance Metric and Assessment of Physical Rehabilitation Exercises,L. Yao; Q. Lei; H. Zhang; J. Du; S. Gao,2023,"Human activity analysis in the legal monitoring environment plays an important role in the physical rehabilitation field, as it helps patients with physical injuries improve their postoperative conditions and reduce their medical costs. Recently, several deep learning-based action quality assessment (AQA) frameworks have been proposed to evaluate physical rehabilitation exercises. However, most of them treat this problem as a simple regression task, which requires both the action instance and its score label as input. This approach is limited by the fact that the annotations in this field usually consist of healthy or unhealthy labels rather than quality scores provided by professional physicians. Additionally, most of these methods cannot provide informative feedback on a patient’s motion defects, which weakens their practical application. To address these problems, we propose a multi-task contrastive learning framework to learn subtle and critical differences from skeleton sequences to deal with the performance metric and AQA problems of physical rehabilitation exercises. Specifically, we propose a performance metric network that takes triplets of training samples as input for score generation. For the AQA task, the same contrast learning strategy is used, but pairwise training samples are fed into the action quality assessment network for score prediction. Notably, we propose quantifying the deviation of the joint attention matrix between different skeleton sequences and introducing it into the loss function of our learning network. It is proven that considering both score prediction loss and joint attention deviation loss improves physical exercises AQA performance. Furthermore, it helps to obtain informative feedback for patients to improve their motion defects by visualizing the joint attention matrix’s difference. The proposed method is verified on the UI-PRMD and KIMORE datasets. Experimental results show that the proposed method achieves state-of-the-art performance.",yes,"The article proposes a method for assessing the quality of physical rehabilitation exercises, explicitly mentioning AQA frameworks, and evaluates it on UI-PRMD and KIMORE datasets using vision-derived skeleton data.",UI-PRMD;KIMORE
10.1109/ACCESS.2023.3316009,AI Trainer: Autoencoder Based Approach for Squat Analysis and Correction,M. Chariar; S. Rao; A. Irani; S. Suresh; C. S. Asha,2023,"Artificial intelligence and computer vision have widespread applications in workout analysis. It has been extensively used in sports and the athlete industry to identify errors and improve performance. Furthermore, these methods prevent injuries caused by a lack of instructors or costly infrastructure. One such exercise is the squat, which is a movement in which a standing person descends to a posture with their torso vertical and their knees firmly bent, then returns to their original upright position. Each person’s squat is distinct, with varying limb lengths causing their form to change when observed. It has been observed that the mobility of various joints and muscular strength have a role in this. A squat improves the user by increasing overall leg strength, strengthening knee and hip joints, and lowering the risk of heart disease due to cardiovascular development. This paper presents a method for classifying squat types and recommending the right squat version. This study uses MediaPipe and a deep learning-based technique to decide if squatting is good or bad. A stacked Bidirectional Gated Recurrent Unit (Bi-GRU) model with an attention layer is proposed to consistently and fairly assess each user, categorizing squats into seven classes. This stacked Bi-GRU model with an attention unit is then compared to other cutting-edge models, both with and without the attention layer. The model outperforms other models by attaining an accuracy of 94% and is demonstrated to work the best and most consistently for our dataset. Furthermore, the individual executing the incorrect squat is corrected to the best of their ability, depending on their performance and body proportions, by providing the correct form.",yes,"The article proposes an AI Trainer for squat analysis and correction, which involves assessing the quality of human actions using computer vision and a deep learning model, and uses a custom dataset.",
10.1109/TVCG.2023.3247092,A Video-Based Augmented Reality System for Human-in-the-Loop Muscle Strength Assessment of Juvenile Dermatomyositis,K. Zhou; R. Cai; Y. Ma; Q. Tan; X. Wang; J. Li; H. P. H. Shum; F. W. B. Li; S. Jin; X. Liang,2023,"As the most common idiopathic inflammatory myopathy in children, juvenile dermatomyositis (JDM) is characterized by skin rashes and muscle weakness. The childhood myositis assessment scale (CMAS) is commonly used to measure the degree of muscle involvement for diagnosis or rehabilitation monitoring. On the one hand, human diagnosis is not scalable and may be subject to personal bias. On the other hand, automatic action quality assessment (AQA) algorithms cannot guarantee 100% accuracy, making them not suitable for biomedical applications. As a solution, we propose a video-based augmented reality system for human-in-the-loop muscle strength assessment of children with JDM. We first propose an AQA algorithm for muscle strength assessment of JDM using contrastive regression trained by a JDM dataset. Our core insight is to visualize the AQA results as a virtual character facilitated by a 3D animation dataset, so that users can compare the real-world patient and the virtual character to understand and verify the AQA results. To allow effective comparisons, we propose a video-based augmented reality system. Given a feed, we adapt computer vision algorithms for scene understanding, evaluate the optimal way of augmenting the virtual character into the scene, and highlight important parts for effective human verification. The experimental results confirm the effectiveness of our AQA algorithm, and the results of the user study demonstrate that humans can more accurately and quickly assess the muscle strength of children using our system.",yes,"The article proposes an AQA algorithm for muscle strength assessment using a JDM dataset and a video-based augmented reality system, which is a computer vision application.",
10.1109/CVPR52733.2024.01744,Narrative Action Evaluation with Prompt-Guided Multimodal Interaction,S. Zhang; S. Bai; G. Chen; L. Chen; J. Lu; J. Wang; Y. Tang,2024,"In this paper, we investigate a new problem called narrative action evaluation (NAE). NAE aims to generate professional commentary that evaluates the execution of an action. Unlike traditional tasks such as score-based action qual-ity assessment and video captioning involving superficial sentences, NAE focuses on creating detailed narratives in natural language. These narratives provide intricate descriptions of actions along with objective evaluations. NAE is a more challenging task because it requires both narrative flex-ibility and evaluation rigor. One existing possible solution is to use multi-task learning, where narrative language and evaluative information are predicted separately. However, this approach results in reduced performance for individual tasks because of variations between tasks and differences in modality between language information and evaluation information. To address this, we propose a prompt-guided multimodal interaction framework. This framework utilizes a pair of transformers to facilitate the interaction between different modalities of information. It also uses prompts to transform the score regression task into a video-text matching task, thus enabling task interactivity. To support further research in this field, we re-annotate the MTL-AQA and FineGym datasets with high-quality and comprehensive action narration. Additionally, we establish benchmarks for NAE. Extensive experiment results prove that our method outperforms separate learning methods and naive multi-task learning methods. Data and code are released at here.",yes,"The article investigates Narrative Action Evaluation, a task related to Action Quality Assessment, and re-annotates MTL-AQA and FineGym datasets, making them publicly available.",MTL-AQA;FineGym
10.1109/CVPR52688.2022.00323,Likert Scoring with Grade Decoupling for Long-term Action Assessment,A. Xu; L. -A. Zeng; W. -S. Zheng,2022,"Long-term action quality assessment is a task of evaluating how well an action is performed, namely, estimating a quality score from a long video. Intuitively, long-term actions generally involve parts exhibiting different levels of skill, and we call the levels of skill as performance grades. For example, technical highlights and faults may appear in the same long-term action. Hence, the final score should be determined by the comprehensive effect of different grades exhibited in the video. To explore this latent relationship, we design a novel Likert scoring paradigm in-spired by the Likert scale in psychometrics, in which we quantify the grades explicitly and generate the final quality score by combining the quantitative values and the corresponding responses estimated from the video, instead of performing direct regression. Moreover, we extract grade-specific features, which will be used to estimate the responses of each grade, through a Transformer decoder architecture with diverse learnable queries. The whole model is named as Grade-decoupling Likert Transformer (GDLT), and we achieve state-of-the-art results on two long-term action assessment datasets.11Project page https://isee-ai.cn/-angchi/CVPR22_GDLT.html",yes,"The article uses datasets for long-term action quality assessment, which is a vision-based computer science task. No exclusion criteria are violated.",
10.1109/VCIP53242.2021.9675430,A Multi-dimensional Aesthetic Quality Assessment Model for Mobile Game Images,T. Wang; W. Sun; X. Min; W. Lu; Z. Zhang; G. Zhai,2021,"With the development of the game industry and the popularization of mobile devices, mobile games have played an important role in people's entertainment life. The aesthetic quality of mobile game images determines the users' Quality of Experience (QoE) to a certain extent. In this paper, we propose a multi-task deep learning based method to evaluate the aesthetic quality of mobile game images in multiple dimensions (i.e. the fineness, color harmony, colorfulness, and overall quality). Specifically, we first extract the quality-aware feature representation through integrating the features from all intermediate layers of the convolution neural network (CNN) and then map these quality-aware features into the quality score space in each dimension via the quality regressor module, which consists of three fully connected (FC) layers. The proposed model is trained through a multi-task learning manner, where the quality-aware features are shared by different quality dimension prediction tasks, and the multi-dimensional quality scores of each image are regressed by multiple quality regression modules respectively. We further introduce an uncertainty principle to balance the loss of each task in the training stage. The experimental results show that our proposed model achieves the best performance on the Multi-dimensional Aesthetic assessment for Mobile Game image database (MAMG) among state-of-the-art image quality assessment (IQA) algorithms and aesthetic quality assessment (AQA) algorithms.",no,"The article is about aesthetic quality assessment of images, not human action quality assessment.",
10.1109/CVPRW.2017.16,Learning to Score Olympic Events,P. Parmar; B. T. Morris,2017,"Estimating action quality, the process of assigning a ""score"" to the execution of an action, is crucial in areas such as sports and health care. Unlike action recognition, which has millions of examples to learn from, the action quality datasets that are currently available are small-typically comprised of only a few hundred samples. This work presents three frameworks for evaluating Olympic sports which utilize spatiotemporal features learned using 3D convolutional neural networks (C3D) and perform score regression with i) SVR ii) LSTM and iii) LSTM followed by SVR. An efficient training mechanism for the limited data scenarios is presented for clip-based training with LSTM. The proposed systems show significant improvement over existing quality assessment approaches on the task of predicting scores of diving, vault, figure skating. SVR-based frameworks yield better results, LSTM-based frameworks are more natural for describing an action and can be used for improvement feedback.",yes,"The article uses datasets for action quality assessment in Olympic sports, which is a vision-based computer science task. No exclusion criteria are violated.",
10.1109/TMM.2023.3328180,Learning Semantics-Guided Representations for Scoring Figure Skating,Z. Du; D. He; X. Wang; Q. Wang,2024,"This paper explores semantic-aware representations for scoring figure skating videos. Most existing approaches to sports video analysis only focus on reasoning action scores based on visual input, limiting their ability to depict high-level semantic representations. Here, we propose a teacher-student-based network with an attention mechanism to realize an adaptive knowledge transfer from the semantic domain to the visual domain, which is termed semantics-guided network (SGN). Specifically, we use a set of learnable atomic queries in the student branch to mimic the semantic-aware distribution in the teacher branch, which is represented by the visual and semantic inputs. In addition, we propose three auxiliary losses to align features in different domains. With aligned feature representations, the adapted teacher is capable of transferring the semantic knowledge to the student. To verify the effectiveness of our method, we collect a new dataset OlympicFS for scoring figure skating. Besides action scores, OlympicFS also provides professional comments on actions for learning semantic representations. By evaluating four challenging datasets, our method achieves state-of-the-art performance.",yes,"The article uses datasets, including a newly collected one (OlympicFS), for scoring figure skating videos, which is a vision-based computer science task related to AQA. No exclusion criteria are violated.",OlympicFS
10.1007/s10489-023-05166-3,Improving action quality assessment with across-staged temporal reasoning on imbalanced data,"P., Lian, Puxiang; Z., Shao, Zhigang",2023,"Action quality assessment is a significant research domain in computer vision, aimed at evaluating the accuracy of human movement and providing feedback and guidance for training and rehabilitation. However, the uneven nature of the data, which has a significant impact on the labels with less samples, is not taken into consideration by the generally used approaches in this field. To address this issue, we propose using kernel density estimation (KDE) to recalculate the label density and weight the loss function by the reciprocal of the square root of each label density. Additionally, we divide the entire motion into three sub-stages, including the takeoff, aerial movement, and entry for diving, and connect the three stages using an across-staged temporal reasoning module (ASTRM). Our approach achieves a performance of 0.9222 Spearman correlation coefficient (ρ) and 0.3304 (× 100) Relative ℓ<inf>2</inf> -distance (R - ℓ<inf>2</inf>) on the FineDiving dataset, demonstrating competitiveness compared to other methods. Furthermore, numerous comprehensive ablation experiments validate the effectiveness of the methods and modules we adopted. © 2024 Elsevier B.V., All rights reserved.",yes,"The article uses the FineDiving dataset for action quality assessment, which is a vision-based computer science task. No exclusion criteria are violated.",FineDiving
10.1109/TCSVT.2023.3281413,Hierarchical Graph Convolutional Networks for Action Quality Assessment,"K., Zhou, Kanglei; Y., Ma, Yue; H.P., Shum, Hubert P.H.; X., Liang, Xiaohui",2023,"Action quality assessment (AQA) automatically evaluates how well humans perform actions in a given video, a technique widely used in fields such as rehabilitation medicine, athletic competitions, and specific skills assessment. However, existing works that uniformly divide the video sequence into small clips of equal length suffer from intra-clip confusion and inter-clip incoherence, hindering the further development of AQA. To address this issue, we propose a hierarchical graph convolutional network (GCN). First, semantic information confusion is corrected through clip refinement, generating the 'shot' as the basic action unit. We then construct a scene graph by combining several consecutive shots into meaningful scenes to capture local dynamics. These scenes can be viewed as different procedures of a given action, providing valuable assessment cues. The video-level representation is finally extracted via sequential action aggregation among scenes to regress the predicted score distribution, enhancing discriminative features and improving assessment performance. Experiments on the AQA-7, MTL-AQA, and JIGSAWS datasets demonstrate the superiority of the proposed hierarchical GCN over state-of-the-art methods. © 2023 Elsevier B.V., All rights reserved.",yes,"The article uses AQA-7, MTL-AQA, and JIGSAWS datasets for action quality assessment, which is a vision-based computer science task. No exclusion criteria are violated.",AQA-7;MTL-AQA;JIGSAWS
10.1016/j.heliyon.2023.e21361,The Establishment of a precise intelligent evaluation system for sports events: Diving,"N., Hao, Ning; S., Ruan, Sihan; Y., Song, Yiheng; J., Chen, Jiashun; L., Tian, Longgang",2023,"The introduction of action quality assessment technology in sports events to achieve precise intelligent evaluation can greatly enhance the objectivity and effectiveness of competition results. Taking diving as the specific application background, this study proposes a novel Multi-granularity Extraction Approach for Temporal-spatial features in judge scoring prediction (MEAT) under the conditions of action quality assessment. On the one hand, it uses dual-modal inflated 3D ConvNet to extract the temporal and spatial features of each modal diving video at the video granularity parallelly and to merge them to form a global feature. On the other hand, the human body pose is modeled, and the simulated athlete's three-dimensional splash state is taken as local characteristics at the object granularity. Finally, the global and local features are concatenated into the fully connected layer, and heuristic method inspired by competition rules using labeled distribution learning are employed to output the probability distribution of the average score of all referees. The maximum probability score is selected and multiplied by the difficulty coefficient to obtain the final diving score. Through comprehensive experiments, comparing the Spearman's rank correlation (SRC) evaluation results of existing methods on the UNIV-Dive dataset, this framework reflects the greater accuracy advantage and further lays the foundation for the actual implementation of the technology. © 2023 Elsevier B.V., All rights reserved.",yes,"The article uses the UNIV-Dive dataset for action quality assessment in diving, which is a vision-based computer science task. No exclusion criteria are violated.",UNIV-Dive
10.1145/3606038.3616150,Video-based Skill Assessment for Golf: Estimating Golf Handicap,"C.K., Ingwersen, Christian Keilstrup; A., Xarles, Artur; A., Clapés, Albert; M., Madadi, Meysam; J.N., Jensen, Janus Nortoft; M.R., Hannemose, Morten Rieger; A.B., Dahl, Anders Bjørholm; S., Escalera, Sergio",2023,"Automated skill assessment in sports using video-based analysis holds great potential for revolutionizing coaching methodologies. This paper focuses on the problem of skill determination in golfers by leveraging deep learning models applied to a large database of video recordings of golf swings. We investigate different regression, ranking and classification based methods and compare to a simple baseline approach. The performance is evaluated using mean squared error (MSE) as well as computing the percentages of correctly ranked pairs based on the Kendall correlation. Our results demonstrate an improvement over the baseline, with a 35% lower mean squared error and 68% correctly ranked pairs. However, achieving fine-grained skill assessment remains challenging. This work contributes to the development of AI-driven coaching systems and advances the understanding of video-based skill determination in the context of golf. © 2023 Elsevier B.V., All rights reserved.",yes,"The article uses a database of golf swing videos for skill assessment, which is a vision-based computer science task related to AQA. No exclusion criteria are violated.",
10.1145/3581783.3613774,A Figure Skating Jumping Dataset for Replay-Guided Action Quality Assessment,"Y., Liu, Yanchao; X., Cheng, Xina; T., Ikenaga, Takeshi",2023,"In competitive sports, judges often scrutinize replay videos from multiple views to adjudicate uncertain or contentious actions, and ultimately ascertain the definitive score. Most existing action quality assessment methods regress from a single video or a pairwise exemplar and input videos, which are limited by the viewpoint and zoom scale of videos. To end this, we construct a Replay Figure Skating Jumping dataset (RFSJ), containing additional view information provided by the post-match replay video and fine-grained annotations. We also propose a Replay-Guided approach for action quality assessment, learned by a Triple-Stream Contrastive Transformer and a Temporal Concentration Module. Specifically, besides the pairwise input and exemplar, we contrast the input and its replay by an extra contrastive module. Then the consistency of scores guides the model to learn features of the same action under different views and zoom scales. In addition, based on the fact that errors or highlight moments of athletes are crucial factors affecting scoring, these moments are concentrated in parts of the video rather than a uniform distribution. The proposed temporal concentration module encourages the model to concentrate on these features, then cooperates with the contrastive regression module to obtain an effective scoring mechanism. Extensive experiments demonstrate that our method achieves Spearman's Rank Correlation of 0.9346 on the proposed RFSJ dataset, improving over the existing state-of-the-art methods. © 2025 Elsevier B.V., All rights reserved.",yes,"The article constructs and uses the RFSJ dataset for action quality assessment, which is a vision-based computer science task. No exclusion criteria are violated.",RFSJ
10.1145/3577190.3614117,MMASD: A Multimodal Dataset for Autism Intervention Analysis,"J., Li, Jicheng; V., Chheang, Vuthea; P., Kullu, Pinar; E., Brignac, Eli; Z., Guo, Zhang; A.N., Bhat, Anjana Narayan; K.E., Barner, Kenneth E.; R.L., Barmaki, Roghayeh Leila",2023,"Autism spectrum disorder (ASD) is a developmental disorder characterized by significant impairments in social communication and difficulties perceiving and presenting communication signals. Machine learning techniques have been widely used to facilitate autism studies and assessments. However, computational models are primarily concentrated on very specific analysis and validated on private, non-public datasets in the autism community, which limits comparisons across models due to privacy-preserving data-sharing complications. This work presents a novel open source privacy-preserving dataset, MMASD as a MultiModal ASD benchmark dataset, collected from play therapy interventions for children with autism. The MMASD includes data from 32 children with ASD, and 1,315 data samples segmented from more than 100 hours of intervention recordings. To promote the privacy of children while offering public access, each sample consists of four privacy-preserving modalities, some of which are derived from original videos: (1) optical flow, (2) 2D skeleton, (3) 3D skeleton, and (4) clinician ASD evaluation scores of children. MMASD aims to assist researchers and therapists in understanding children's cognitive status, monitoring their progress during therapy, and customizing the treatment plan accordingly. It also inspires downstream social tasks such as action quality assessment and interpersonal synchrony estimation. The dataset is publicly accessible via the MMASD project website. © 2023 Elsevier B.V., All rights reserved.",yes,"The article introduces a multimodal dataset (MMASD) that can be used for action quality assessment, which is a vision-based computer science task. The dataset is publicly available.",MMASD
10.1088/1742-6596/2632/1/012027,Prior Knowledge-guided Hierarchical Action Quality Assessment with 3D Convolution and Attention Mechanism,"H., Zhou, Haoyang; T., Hou, Teng; J., Li, Jitao",2023,"Recently, there has been a growing interest in the field of computer vision and deep learning regarding a newly emerging problem known as action quality assessment (AQA). However, most researchers still rely on the traditional approach of using models from the video action recognition field. Unfortunately, this approach overlooks crucial features in AQA, such as movement fluency and degree of completion. Alternatively, some researchers have employed the transformer paradigm to capture action details and overall action integrity, but the high computational cost associated with transformers makes them impractical for real-time tasks. Due to the diversity of action types, it is challenging to rely solely on a shared model for quality assessment of various types of actions. To address these issues, we propose a novel network structure for AQA, which is the first to integrate multi-model capabilities through a classification model. Specifically, we utilize a pre-trained I3D model equipped with a self-attention block for classification. This allows us to evaluate various categories of actions using just one model. Furthermore, we introduce self-attention mechanisms and multi-head attention into the traditional convolutional neural network. By systematically replacing the last few layers of the conventional convolutional network, our model gains a greater ability to sense the global coordination of different actions. We have verified the effectiveness of our approach on the AQA-7 dataset. In comparison to other popular models, our model achieves satisfactory performance while maintaining a low computational cost. © 2023 Elsevier B.V., All rights reserved.",yes,"The article uses the AQA-7 dataset for action quality assessment, which is a vision-based computer science task. No exclusion criteria are violated.",AQA-7
10.1109/TIP.2023.3331212,Fine-Grained Spatio-Temporal Parsing Network for Action Quality Assessment,"G.A., Kumie, Gedamu Alemu; Y., Ji, Yanli; Y., Yang, Yang; J., Shao, Jie; H., Shen, Hengtao",2023,"Action Quality Assessment (AQA) plays an important role in video analysis, which is applied to evaluate the quality of specific actions, i.e., sports activities. However, it is still challenging because there are lots of small action discrepancies with similar backgrounds, but current approaches mostly adopt holistic video representations. So that fine-grained intra-class variations are unable to be captured. To address the aforementioned challenge, we propose a Fine-grained Spatio-temporal Parsing Network (FSPN) which is composed of the intra-sequence action parsing module and spatiotemporal multiscale transformer module to learn fine-grained spatiotemporal sub-action representations for more reliable AQA. The intra-sequence action parsing module performs semantical sub-action parsing by mining sub-actions at fine-grained levels. It enables a correct description of the subtle differences between action sequences. The spatiotemporal multiscale transformer module learns motion-oriented action features and obtains their long-range dependencies among sub-actions at different scales. Furthermore, we design a group contrastive loss to train the model and learn more discriminative feature representations for sub-actions without explicit supervision. We exhaustively evaluate our proposed approach in the FineDiving, AQA-7, and MTL-AQA datasets. Extensive experiment results demonstrate the effectiveness and feasibility of our proposed approach, which outperforms the state-of-the-art methods by a significant margin. © 2023 Elsevier B.V., All rights reserved.",yes,"The article uses FineDiving, AQA-7, and MTL-AQA datasets for action quality assessment, which is a vision-based computer science task. No exclusion criteria are violated.",FineDiving;AQA-7;MTL-AQA
10.1155/2023/3649217,A Novel Model for Intelligent Pull-Ups Test Based on Key Point Estimation of Human Body and Equipment,"G., Liu, Guozhong; J., Wang, Jian; Z., Zhang, Zhibo; Q., Liu, Qingyi; Y., Ren, Yande; M., Zhang, Mengjiao; S., Chen, Shan; P., Bai, Peirui",2023,"Applying computer vision and machine learning techniques into sport tests is an effective way to realize ""intelligent sports.""Facing practical application, we design a real-time and lightweight deep learning network to realize intelligent pull-ups test in this study. The main contributions are as follows: (1) a new self-produced pull-ups dataset is established under the requirement of including a human body and horizontal bar. In addition, a semiautomatic annotating software is developed to enhance annotation efficiency and increase labeling accuracy. (2) A novel lightweight deep network named PEPoseNet is designed to estimate and analyze a human pose in real time. The backbone of the network is made up of the heatmap network and key point network, which conduct human pose estimation based on the key points extracted from a human body and horizontal bar. The depth-wise separable convolution is adopted to speed up the training and convergence. (3) An evaluation criterion of intelligent pull-ups test is defined based on action quality assessment (AQA). The action quality of five states, i.e., ready or end, hang, pull, achieved, and resume in one pull-ups test cycle is automatically graded using a random forest classifier. A mobile application is developed to realize intelligent pull-ups test in real time. The performance of the proposed model and software is confirmed by verification and ablation experiments. The experimental results demonstrated that the proposed PEPoseNet has competitive performance to the state of the art. Its PCK @ 0.2 and frames per second (FPS) achieved were 83.8 and 30 fps, respectively. The mobile application has promising application prospects in pull-ups test under complex scenarios. © 2023 Elsevier B.V., All rights reserved.",yes,"The article establishes and uses a self-produced pull-ups dataset for action quality assessment, which is a vision-based computer science task. No exclusion criteria are violated.",
10.1109/TCSVT.2022.3143549,Semi-Supervised Action Quality Assessment With Self-Supervised Segment Feature Recovery,"S., Zhang, Shaojie; J., Pan, Jiahui; J., Gao, Jibin; W., Zheng, Wei-Shi",2022,"Action Quality Assessment aims to evaluate how well an action performs. Existing methods have achieved remarkable progress on fully-supervised action assessment. However, in real-world applications, with expert's experience, it is not always feasible to manually label all samples. Therefore, it is important to study the problem of semi-supervised action assessment with only a small amount of samples annotated. A major challenge for semi-supervised action assessment is how to exploit the temporal pattern from unlabeled videos. Inspired by the temporal dependencies of the action execution, we propose a self-supervised learning on the unlabeled videos by recovering the feature of a masked segment of an unlabeled video. Furthermore, we leverage adversarial learning to align the representation distribution of the labeled and the unlabeled samples to close their gap in the sample space since unlabeled samples always come from unseen actions. Finally, we propose an adversarial self-supervised framework for semi-supervised action quality assessment. The extensive experimental results on the MTL-AQA and the Rhythmic Gymnastics datasets will demonstrate the effectiveness of our framework, achieving the state-of-the-art performances of semi-supervised action quality assessment. © 2024 Elsevier B.V., All rights reserved.",yes,"The article uses MTL-AQA and Rhythmic Gymnastics datasets for semi-supervised action quality assessment, which is a vision-based computer science task. No exclusion criteria are violated.",MTL-AQA;Rhythmic Gymnastics
10.1007/978-3-031-18913-5_17,Skeleton-Based Action Quality Assessment via Partially Connected LSTM with Triplet Losses,"X., Wang, Xinyu; J., Li, Jianwei; H., Hu, Haiqing",2022,"Human action quality assessment (AQA) recently has attracted increasing attentions in computer vision for its practical applications, such as skill training, physical rehabilitation and scoring sports events. In this paper, we propose a partially connected LSTM with triplet losses to evaluate different skill levels. Compared to human action recognition (HAR), we explain and discuss two characteristics and countermeasures of AQA. To ignore the negative influence of complex joint movements in actions, the skeleton is not regarded as a single graph. The fully connected layer in the LSTM model is replaced by the partially connected layer, using a diagonal matrix which activates the corresponding weights, to explore hierarchical relations in the skeleton graph. Furthermore, to improve the generalization ability of models, we introduce additional functions of triplet loss to the loss function, which make samples with similar skill levels close to each other. We carry out experiments to test our model and compare it with seven LSTM architectures and three GNN architectures on the UMONS-TAICHI dataset and walking gait dataset. Experimental results demonstrate that our model achieves outstanding performance. © 2022 Elsevier B.V., All rights reserved.",yes,"The article uses UMONS-TAICHI and walking gait datasets for skeleton-based action quality assessment, which is a vision-based computer science task. No exclusion criteria are violated.",UMONS-TAICHI;Walking Gait
10.1007/978-3-031-19772-7_25,Action Quality Assessment with Temporal Parsing Transformer,"Y., Bai, Yang; D., Zhou, Desen; S., Zhang, Songyang; J., Wang, Jian; E., DIng, Errui; Y., Guan, Yu; Y., Long, Yang; J., Wang, Jingdong",2022,"Action Quality Assessment(AQA) is important for action understanding and resolving the task poses unique challenges due to subtle visual differences. Existing state-of-the-art methods typically rely on the holistic video representations for score regression or ranking, which limits the generalization to capture fine-grained intra-class variation. To overcome the above limitation, we propose a temporal parsing transformer to decompose the holistic feature into temporal part-level representations. Specifically, we utilize a set of learnable queries to represent the atomic temporal patterns for a specific action. Our decoding process converts the frame representations to a fixed number of temporally ordered part representations. To obtain the quality score, we adopt the state-of-the-art contrastive regression based on the part representations. Since existing AQA datasets do not provide temporal part-level labels or partitions, we propose two novel loss functions on the cross attention responses of the decoder: a ranking loss to ensure the learnable queries to satisfy the temporal order in cross attention and a sparsity loss to encourage the part representations to be more discriminative. Extensive experiments show that our proposed method outperforms prior work on three public AQA benchmarks by a considerable margin. © 2025 Elsevier B.V., All rights reserved.",yes,"The article uses AQA benchmarks for action quality assessment, which is a vision-based computer science task. No exclusion criteria are violated.",
10.1007/978-3-031-04881-4_46,Improving Action Quality Assessment Using Weighted Aggregation,"S., Farabi, Shafkat; H., Himel, Hasibul; F., Gazzali, Fakhruddin; M.B., Hasan, Md Bakhtiar; M.H., Kabir, Md Hasanul; M.R., Farazi, Moshiur R.",2022,"Action quality assessment (AQA) aims at automatically judging human action based on a video of the said action and assigning a performance score to it. The majority of works in the existing literature on AQA divide RGB videos into short clips, transform these clips to higher-level representations using Convolutional 3D (C3D) networks, and aggregate them through averaging. These higher-level representations are used to perform AQA. We find that the current clip level feature aggregation technique of averaging is insufficient to capture the relative importance of clip level features. In this work, we propose a learning-based weighted-averaging technique. Using this technique, better performance can be obtained without sacrificing too much computational resources. We call this technique Weight-Decider(WD). We also experiment with ResNets for learning better representations for action quality assessment. We assess the effects of the depth and input clip size of the convolutional neural network on the quality of action score predictions. We achieve a new state-of-the-art Spearman’s rank correlation of 0.9315 (an increase of 0.45%) on the MTL-AQA dataset using a 34 layer (2+1)D ResNet with the capability of processing 32 frame clips, with WD aggregation. © 2022 Elsevier B.V., All rights reserved.",yes,"The article uses the MTL-AQA dataset for action quality assessment, which is a vision-based computer science task. No exclusion criteria are violated.",MTL-AQA
10.1145/3462244.3479891,Improving the Movement Synchrony Estimation with Action Quality Assessment in Children Play Therapy,"J., Li, Jicheng; A.N., Bhat, Anjana Narayan; R.L., Barmaki, Roghayeh Leila",2021,"Movement synchrony refers to the dynamic temporal connection between the motions of interacting people. The automatic measurement of movement synchrony is worth studying for social behavior analysis applications, for instance, in play therapy of children in the autism spectrum. Existing approaches based on motion energy analysis are strongly reliant on the region of interest, and thus limit the interaction between individuals, especially for highly engaging activities like play therapy. Inspired by action quality assessment, a task to assess how well an action has been performed, in this paper, we propose an end-to-end deep learning method to integrate the following major tasks: (1) the automatic assessment of children's performance in play therapy, and (2) the automatic estimation of movement synchrony between children and therapists, facilitated by an auxiliary task of intervention activity recognition. This multi-task paradigm generally improves the performance of our model across all tasks. Furthermore, when annotations are subjective, the typical exclusive annotation strategy may reduce tagging quality. As a result, we explored applying distribution learning to mitigate human bias in movement synchrony estimation. We allowed the second and third labels for each instance, namely the uncertainty-preserved annotation approach. We tested our method on Play Therapy 13 (PT13), a dataset collected from video recordings of play therapy interventions. The findings of the experiments indicated that our framework can accurately quantify movement synchronization and assess the quality of children's actions in play therapy. Moreover, the uncertainty-preserved annotation approach produced a comparable outcome to standard methods at a far reduced cost, demonstrating its efficacy in mitigating biases. © 2021 Elsevier B.V., All rights reserved.",yes,"The article uses the PT13 dataset for action quality assessment in play therapy, which is a vision-based computer science task. No exclusion criteria are violated.",Play Therapy 13 (PT13)
10.1145/3474085.3475438,TSA-Net: Tube Self-Attention Network for Action Quality Assessment,"S., Wang, Shunli; D., Yang, Dingkang; P., Zhai, Peng; C., Chen, Chixiao; L., Zhang, Lihua",2021,"In recent years, assessing action quality from videos has attracted growing attention in computer vision community and human-computer interaction. Most existing approaches usually tackle this problem by directly migrating the model from action recognition tasks, which ignores the intrinsic differences within the feature map such as foreground and background information. To address this issue, we propose a Tube Self-Attention Network (TSA-Net) for action quality assessment (AQA). Specifically, we introduce a single object tracker into AQA and propose the Tube Self-Attention Module (TSA), which can efficiently generate rich spatio-temporal contextual information by adopting sparse feature interactions. The TSA module is embedded in existing video networks to form TSA-Net. Overall, our TSA-Net is with the following merits: 1) High computational efficiency, 2) High flexibility, and 3) The state-of-the-art performance. Extensive experiments are conducted on popular action quality assessment datasets including AQA-7 and MTL-AQA. Besides, a dataset named Fall Recognition in Figure Skating (FR-FS) is proposed to explore the basic action assessment in the figure skating scene. Our TSA-Net achieves the Spearman's Rank Correlation of 0.8476 and 0.9393 on AQA-7 and MTL-AQA, respectively, which are the new state-of-the-art results. The results on FR-FS also verify the effectiveness of the TSA-Net. The code and FR-FS dataset are publicly available at https://github.com/Shunli-Wang/TSA-Net. © 2021 Elsevier B.V., All rights reserved.",yes,"The article uses AQA-7, MTL-AQA, and FR-FS datasets for action quality assessment, which is a vision-based computer science task. The FR-FS dataset is publicly available. No exclusion criteria are violated.",AQA-7;MTL-AQA;FR-FS
10.1145/3453892.3461624,Towards Improved and Interpretable Action Quality Assessment with Self-Supervised Alignment,"K., Roditakis, Konstantinos; A., Makris, Alexandros; A.A., Argyros, Antonis A.",2021,"Action Quality Assessment (AQA) is a video understanding task aiming at the quantification of the execution quality of an action. One of the main challenges in relevant, deep learning-based approaches is the collection of training data annotated by experts. Current methods perform fine-tuning on pre-trained backbone models and aim to improve performance by modeling the subjects and the scene. In this work, we consider embeddings extracted using a self-supervised training method based on a differential cycle consistency loss between sequences of actions. These are shown to improve the state-of-the-art without the need for additional annotations or scene modeling. The same embeddings are also used to temporally align the sequences prior to quality assessment which further increases the accuracy, provides robustness to variance in execution speed and enables us to provide fine-grained interpretability of the assessment score. The experimental evaluation of the method on the MTL-AQA dataset demonstrates significant accuracy gain compared to the state-of-the-art baselines, which grows even more when the action execution sequences are not well aligned. © 2021 Elsevier B.V., All rights reserved.",yes,"The article uses the MTL-AQA dataset for action quality assessment, which is a vision-based computer science task. No exclusion criteria are violated.",MTL-AQA
10.1016/j.eswa.2020.114553,Deep reinforcement learning based trading agents: Risk curiosity driven learning for financial rules-based policy,"B., Hirchoua, Badr; B., Ouhbi, Brahim; B., Frikh, Bouchra",2021,"Financial markets are complex dynamic systems influenced by a high number of active agents, which produce a behavior with high randomness and noise. Trading strategies are well depicted as an online decision-making problem involving imperfect information and aiming to maximize the return while restraining the risk. However, it is challenging to obtain an optimal strategy in the complex and dynamic stock market. Therefore, recent developments in similar environments have pushed researchers towards exciting new horizons. In this paper, a novel rule-based policy approach is proposed to train a deep reinforcement learning agent for automated financial trading. Precisely, a continuous virtual environment has been created, with different versions of agents trading against one another. During this multiplex process, the agents which are trained on 504 risky datasets, use the fundamental concepts of proximal policy optimization to improve their own decision making by adjusting their action choice against the uncertainty of states. Risk curiosity-driven learning acts as an intrinsic reward function and is heavily laden with signals to find salient relationships between actions and market behaviors. The trained agent based on curiosity-driven risk has steadily and progressively improved actions quality. The self-learned rules driven by the agent curiosity push the policy towards actions that yield a high performance over the environment. Experiments on 8 real-world stocks are given to verify the appropriateness and efficiency of the self-learned rules. The proposed system has achieved promising performances, made better trades using fewer transactions, and outperformed the state-of-the-art baselines. © 2021 Elsevier B.V., All rights reserved.",no,The article is not related to vision-based methods or Human Action Quality Assessment (AQA). It focuses on financial trading.,
10.1109/ITME53901.2021.00048,Skeleton Based Action Quality Assessment of Figure Skating Videos,"H., Li, Huiying; Q., Lei, Qing; H., Zhang, Hongbo; J., Du, Jixiang",2021,"Action quality assessment(AQA) aims at achieving automatic evaluation the performance of human actions in video. Compared with action recognition problem, AQA focuses more on subtle differences both in spatial and temporal dimensions during the whole executing process of actions. However, most existing AQA methods tried to extract features directly from RGB videos through a 3D ConvNets, which makes the features mixed with useless scene information. To overcome this problem, We propose a deep pose feature learning AQA method that captured detailed and meaningful representations for skeleton information to discover the subtle motion difference of AQA problem. We first apply pose estimation method to obtain human skeleton data from RGB videos. Then a spatio-temporal graph convolutional network (ST-GCN) is employed to extract the dynamic changes of skeleton data and obtain the representative pose features. Finally, a regressor composed of three fully connected layers is developed to reduce the dimension of the obtained pose features and predict the final score. Experiments on MIT figure skating dataset have been extensively conducted, and the results demonstrate that the proposed method has achieved improvements that outperformed current state-of-the-art methods. © 2022 Elsevier B.V., All rights reserved.",yes,The article uses vision-based methods for Human Action Quality Assessment and explicitly mentions a dataset.,MIT Figure Skating
10.1145/3394171.3421895,ATQAM/MAST'20: Joint Workshop on Aesthetic and Technical Quality Assessment of Multimedia and Media Analytics for Societal Trends,"T., Guha, Tanaya; V., Hosu, Vlad; D., Saupe, Dietmar; B., Goldlücke, Bastian; N., Kumar, Naveen; W., Lin, Weisi; V.R., Martínez, Víctor R.; K., Somandepalli, Krishna; S.S.S., Narayanan, Shrikanth Shri S.; W., Cheng, Wenhuang",2020,"The Joint Workshop on Aesthetic and Technical Quality Assessment of Multimedia and Media Analytics for Societal Trends (ATQAM/ MAST) aims to bring together researchers and professionals working in fields ranging from computer vision, multimedia computing, multimodal signal processing to psychology and social sciences. It is divided into two tracks: ATQAM and MAST. ATQAM track: Visual quality assessment techniques can be divided into image and video technical quality assessment (IQA and VQA, or broadly TQA) and aesthetics quality assessment (AQA). While TQA is a long-standing field, having its roots in media compression, AQA is relatively young. Both have received increased attention with developments in deep learning. The topics have mostly been studied separately, even though they deal with similar aspects of the underlying subjective experience of media. The aim is to bring together individuals in the two fields of TQA and AQA for the sharing of ideas and discussions on current trends, developments, issues, and future directions. MAST track: The research area of media content analytics has been traditionally used to refer to applications involving inference of higher-level semantics from multimedia content. However, multimedia is typically created for human consumption, and we believe it is necessary to adopt a human-centered approach to this analysis, which would not only enable a better understanding of how viewers engage with content but also how they impact each other in the process. © 2021 Elsevier B.V., All rights reserved.",no,"The article is a workshop description, not a research paper using datasets. Also, the AQA mentioned is 'Aesthetics Quality Assessment', not 'Human Action Quality Assessment'.",
10.1145/3394171.3413560,Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos,"L., Zeng, Lingan; F., Hong, Fating; W., Zheng, Wei-Shi; Q., Yu, Qizhi; W., Zeng, Wei; Y., Wang, Yaowei; J., Lai, Jianhuang",2020,"The objective of action quality assessment is to score sports videos. However, most existing works focus only on video dynamic information (i.e., motion information) but ignore the specific postures that an athlete is performing in a video, which is important for action assessment in long videos. In this work, we present a novel hybrid dynAmic-static Context-aware attenTION NETwork (ACTION-NET) for action assessment in long videos. To learn more discriminative representations for videos, we not only learn the video dynamic information but also focus on the static postures of the detected athletes in specific frames, which represent the action quality at certain moments, along with the help of the proposed hybrid dynamic-static architecture. Moreover, we leverage a context-aware attention module consisting of a temporal instance-wise graph convolutional network unit and an attention unit for both streams to extract more robust stream features, where the former is for exploring the relations between instances and the latter for assigning a proper weight to each instance. Finally, we combine the features of the two streams to regress the final video score, supervised by ground-truth scores given by experts. Additionally, we have collected and annotated the new Rhythmic Gymnastics dataset, which contains videos of four different types of gymnastics routines, for evaluation of action quality assessment in long videos. Extensive experimental results validate the efficacy of our proposed method, which outperforms related approaches. © 2021 Elsevier B.V., All rights reserved.",yes,The article uses vision-based methods for Human Action Quality Assessment and introduces a new dataset.,Rhythmic Gymnastics
10.1007/978-3-030-58577-8_14,An Asymmetric Modeling for Action Assessment,"J., Gao, Jibin; W., Zheng, Wei-Shi; J., Pan, Jiahui; C., Gao, Chengying; Y., Wang, Yaowei; W., Zeng, Wei; J., Lai, Jianhuang",2020,"Action assessment is a task of assessing the performance of an action. It is widely applicable to many real-world scenarios such as medical treatment and sporting events. However, existing methods for action assessment are mostly limited to individual actions, especially lacking modeling of the asymmetric relations among agents (e.g., between persons and objects); and this limitation undermines their ability to assess actions containingasymmetrically interactive motion patterns, since there always exists subordination between agents in many interactive actions. In this work, we model the asymmetric interactions among agents for action assessment. In particular, we propose an asymmetric interaction module (AIM), to explicitly model asymmetric interactions between intelligent agents within an action, where we group these agents into a primary one (e.g., human) and secondary ones (e.g., objects). We perform experiments on JIGSAWS dataset containing surgical actions, and additionally collect a new dataset, TASD-2, for interactive sporting actions. The experimental results on two interactive action datasets show the effectiveness of our model, and our method achieves state-of-the-art performance. The extended experiment on AQA-7 dataset also demonstrates the generalization capability of our framework to conventional action assessment. © 2020 Elsevier B.V., All rights reserved.",yes,The article uses vision-based methods for Human Action Quality Assessment and explicitly mentions multiple datasets.,JIGSAWS;TASD-2;AQA-7
10.1007/978-3-030-20876-9_10,ScoringNet: Learning Key Fragment for Action Quality Assessment with Ranking Loss in Skilled Sports,"Y., Li, Yongjun; X., Chai, Xiujuan; X., Chen, Xilin",2019,"Nowadays, scoring athletes’ performance in skilled sports automatically has drawn more and more attention from the academic community. However, extracting effective features and predicting reasonable scores for a long skilled sport video still beset researchers. In this paper, we introduce the ScoringNet, a novel network consisting of key fragment segmentation (KFS) and score prediction (SP), to address these two problems. To get the effective features, we design KFS to obtain key fragments and remove irrelevant fragments by semantic video segmentation. Then a 3D convolutional neural network extracts features from each key fragment. In score prediction, we fuse the ranking loss into the traditional loss function to make the predictions more reasonable in terms of both the score value and the ranking aspects. Through the deep learning, we narrow the gap between the predictions and ground-truth scores as well as making the predictions satisfy the ranking constraint. Widely experiments convincingly show that our method achieves the state-of-the-art results on three datasets. © 2019 Elsevier B.V., All rights reserved.",yes,The article uses vision-based methods for Human Action Quality Assessment and mentions using datasets.,
10.1007/978-3-030-00767-6_12,End-to-end learning for action quality assessment,"Y., Li, Yongjun; X., Chai, Xiujuan; X., Chen, Xilin",2018,"Nowadays, action quality assessment has attracted more and more attention of the researchers in computer vision. In this paper, an end-to-end framework is proposed based on fragment-based 3D convolutional neural network to realize the action quality assessment in videos. Furthermore, the ranking loss integrated with the MSE forms the loss function to make the optimization more reasonable in terms of both the score value and the ranking aspects. Through the deep learning, we narrow the gap between the predictions and ground-truth scores as well as making the predictions satisfy the ranking constraint. The proposed network can indeed learn the evaluation criteria of actions and works well with limited training data. Widely experiments conducted on three public datasets convincingly show that our method achieves the state-of-the-art results. © 2019 Elsevier B.V., All rights reserved.",yes,The article uses vision-based methods for Human Action Quality Assessment and mentions using datasets.,
10.1007/978-3-319-10599-4_36,Assessing the quality of actions,"H., Pirsiavash, Hamed; C., Vondrick, Carl; A., Torralba, A.",2014,"While recent advances in computer vision have provided reliable methods to recognize actions in both images and videos, the problem of assessing how well people perform actions has been largely unexplored in computer vision. Since methods for assessing action quality have many real-world applications in healthcare, sports, and video retrieval, we believe the computer vision community should begin to tackle this challenging problem. To spur progress, we introduce a learning-based framework that takes steps towards assessing how well people perform actions in videos. Our approach works by training a regression model from spatiotemporal pose features to scores obtained from expert judges. Moreover, our approach can provide interpretable feedback on how people can improve their action. We evaluate our method on a new Olympic sports dataset, and our experiments suggest our framework is able to rank the athletes more accurately than a non-expert human. While promising, our method is still a long way to rivaling the performance of expert judges, indicating that there is significant opportunity in computer vision research to improve on this difficult yet important task. © 2014 Springer International Publishing. © 2014 Elsevier B.V., All rights reserved.",yes,The article uses vision-based methods for Human Action Quality Assessment and mentions using a dataset.,
10.1007/s00521-023-09068-w,Auto-encoding score distribution regression for action quality assessment,"Zhang, Boyu; Chen, Jiayuan; Xu, Yinfei; Zhang, Hui; Yang, Xu; Geng, Xin",2023,"Assessing the quality of actions in videos is a challenging vision task, as the relationship between videos and action scores can be difficult to model. Consequently, extensive research has been conducted on action quality assessment (AQA) in the literature. Traditional AQA methods treat the problem as a regression task to learn the underlying mappings between videos and action scores. However, previous approaches overlook the presence of data uncertainty in AQA datasets. To address aleatoric uncertainty, we have developed a plug-and-play module called distribution auto-encoder (DAE). DAE encodes videos into distributions and utilizes the reparameterization trick to sample scores, which enables a more accurate mapping between videos and scores. Additionally, we use a likelihood loss to learn the uncertainty parameters. We have evaluated our approach on publicly available datasets, and extensive experiments demonstrate that DAE achieves state-of-the-art performance with the Spearman's correlation metric of 82.58%, 92.32%, and 76.00% on the AQA-7, MTL-AQA, and JIGSAWSS datasets, respectively. Furthermore, plug-and-play experiments also demonstrate the extensibility of DAE. Our code is available at https://github.com/InfoX-SEU/DAE-AQA.",yes,The article uses vision-based methods for Human Action Quality Assessment and explicitly mentions multiple datasets.,AQA-7;MTL-AQA;JIGSAWSS
10.1007/s40747-022-00892-6,Gaussian guided frame sequence encoder network for action quality assessment,"Li, Ming-Zhe; Zhang, Hong-Bo; Dong, Li-Jia; Lei, Qing; Du, Ji-Xiang",2023,"Can a computer evaluate an athlete's performance automatically? Many action quality assessment (AQA) methods have been proposed in recent years. Limited by the randomness of video sampling and the simple strategy of model training, the performance of the existing AQA methods can still be further improved. To achieve this goal, a Gaussian guided frame sequence encoder network is proposed in this paper. In the proposed method, the image feature of each video frame is extracted by Resnet model. And then, a frame sequence encoder network is applied to model temporal information and generate action quality feature. Finally, a fully connected network is designed to predict action quality score. To train the proposed method effectively, inspired by the final score calculation rule in Olympic game, Gaussian loss function is employed to compute the error between the predicted score and the label score. The proposed method is implemented on the AQA-7 and MTL-AQA datasets. The experimental results confirm that compared with the state-of-the-art methods, our proposed method achieves the better performance. And detailed ablation experiments are conducted to verify the effectiveness of each component in the module.",yes,The article uses vision-based methods for Human Action Quality Assessment and explicitly mentions multiple datasets.,AQA-7;MTL-AQA
10.1016/j.ins.2024.120347,Two-path target-aware contrastive regression for action quality assessment,"Ke, Xiao; Xu, Huangbiao; Lin, Xiaofeng; Guo, Wenzhong",2024,"Action quality assessment (AQA) is a challenging vision task due to the complexity and variance of the scoring rules embedded in the videos. Recent approaches have reduced the prediction difficulty of AQA via learning action differences between videos, but there are still challenges in learning scoring rules and capturing feature differences. To address these challenges, we propose a two -path target -aware contrastive regression (T2CR) framework. We propose to fuse direct and contrastive regression and exploit the consistency of information across multiple visual fields. Specifically, we first directly learn the relational mapping between global video features and scoring rules, which builds occupational domain prior knowledge to better capture local differences between videos. Then, we acquire the auxiliary visual fields of the videos through sparse sampling to learn the commonality of feature representations in multiple visual fields and eliminate the effect of subjective noise from a single visual field. To demonstrate the effectiveness of T2CR, we conduct extensive experiments on four AQA datasets (MTL-AQA, FineDiving, AQA-7, JIGSAWS). Our method is superior to state-of-the-art methods without elaborate structural design and fine-grained information.",yes,The article uses vision-based methods for Human Action Quality Assessment and explicitly mentions multiple datasets.,MTL-AQA;FineDiving;AQA-7;JIGSAWS
10.1109/TIP.2024.3468870,Self-Supervised Sub-Action Parsing Network for Semi-Supervised Action Quality Assessment,"Gedamu, Kumie; Ji, Yanli; Yang, Yang; Shao, Jie; Shen, Heng Tao",2024,"Semi-supervised Action Quality Assessment (AQA) using limited labeled and massive unlabeled samples to achieve high-quality assessment is an attractive but challenging task. The main challenge relies on how to exploit solid and consistent representations of action sequences for building a bridge between labeled and unlabeled samples in the semi-supervised AQA. To address the issue, we propose a Self-supervised sub-Action Parsing Network (SAP-Net) that employs a teacher-student network structure to learn consistent semantic representations between labeled and unlabeled samples for semi-supervised AQA. We perform actor-centric region detection and generate high-quality pseudo-labels in the teacher branch and assists the student branch in learning discriminative action features. We further design a self-supervised sub-action parsing solution to locate and parse fine-grained sub-action sequences. Then, we present the group contrastive learning with pseudo-labels to capture consistent motion-oriented action features in the two branches. We evaluate our proposed SAP-Net on four public datasets: the MTL-AQA, FineDiving, Rhythmic Gymnastics, and FineFS datasets. The experiment results show that our approach outperforms state-of-the-art semi-supervised methods by a significant margin.",yes,The article uses vision-based methods for Human Action Quality Assessment and explicitly mentions multiple datasets.,MTL-AQA;FineDiving;Rhythmic Gymnastics;FineFS
10.1007/s10489-024-05349-6,Assessing action quality with semantic-sequence performance regression and densely distributed sample weighting,"Huang, Feng; Li, Jianjun",2024,"Action Quality Assessment (AQA) is a critical branch of video understanding, offering impartial evaluations for competitive sports. Existing paradigms tend to assess action quality using equal-length clips that lack sufficient semantics, leading to suboptimal predictions. To address this issue, we propose to conduct AQA with Semantic-Sequence Performance Regression (SSPR). SSPR first divides an action into a series of unequal-length segments according to the semantic continuity of the video, such as jumping, dropping, and entering the water in diving. Specifically, the latest Temporal Convolutional Network (TCN) is adopted for semantic-sequence segmentation. To better achieve SSPR, we design a feature fusion module that integrates the semantics of each segment using cascaded 1D convolutions. Furthermore, the imbalanced distribution phenomenon is usually ignored in AQA and we attempt to propose a new loss called positive-weighting MSE (PW-MSE) to deal with it. PW-MSE encourages the network to focus more on densely distributed samples during training, which further improves the network's ranking performance. Experimental results on the benchmark datasets (i.e., UNLV-Dive and AQA-7) demonstrate that our proposed method outperforms the current state-of-the-arts.",yes,The article uses vision-based methods for Human Action Quality Assessment and explicitly mentions multiple datasets.,UNLV-Dive;AQA-7
10.1007/s10489-023-04613-5,Multi-skeleton structures graph convolutional network for action quality assessment in long videos,"Lei, Qing; Li, Huiying; Zhang, Hongbo; Du, Jixiang; Gao, Shangce",2023,"In most existing action quality assessment (AQA) methods, how to score simple actions in short-term sport videos has been widely explored. Recently, a few studies have attempted to solve the AQA problem of long-duration activity by extracting dynamic or static information directly from RGB video. However, these methods may ignore specific postures defined by dynamic changes in human body joints, which makes the results inaccurate and unexplainable. In this work, we propose a novel graph convolution network based on multiple skeleton structure modelling to address the problem of effective pose feature learning to improve the performance of AQA in complex activity. Specifically, three kinds of skeleton structures, including the joints' self-connection, the intra-part connection, and the inter-part connection, are defined to model the motion patterns of joints and body parts. Moreover, a temporal attention learning module is designed to extract temporal relations between skeleton subsequences. We evaluate the proposed method on two benchmark datasets, the MIT-skate dataset and the Rhythmic Gymnastics dataset. Extensive experiments are conducted to verify the effectiveness of the proposed method. The experimental results show that our method achieves state-of-the-art performance.",yes,The article uses vision-based methods for Human Action Quality Assessment and explicitly mentions multiple datasets.,MIT-Skate;Rhythmic Gymnastics
10.1109/TIM.2024.3398072,Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics,"Zahan, Sania; Hassan, Ghulam Mubashar; Mian, Ajmal",2024,"Automated athlete performance measurement from sports videos can significantly enhance sports evaluation. It requires modeling extended sequences, as the intricate spatio-temporal progression significantly influences overall performance. However, the lack of comprehensive datasets with long-duration samples has hindered researchers from focusing on temporal aspects, leading them to primarily concentrate on spatial structures for assessing short-duration videos. Consequently, in-depth analysis of longer videos has received limited attention. This study aims to explore long-term videos and analyze local discriminative spatial dependencies and global semantics for sports action quality assessment (AQA). A new dataset, coined AGF-Olympics, is presented in this paper incorporating artistic gymnastic floor routines. It features highly challenging scenarios with extensive variations in background, viewpoint, and scale, with a duration of up to 2 min. Finally, a discriminative non-local attention (DNLA) is introduced for score regression that effectively maps dense feature space to a sparse representation by disentangling complex associations in long-duration sports videos. DNLA encodes crucial features by analyzing cross-space-time correlations and filtering out features with lower significance. Thus, it ensures that the model prioritizes significant joints in the spatial domain and frames in the temporal domain. Experimental results demonstrate that the proposed method achieves superior performances and provides a benchmark for the AGF-Olympics dataset. Overall, the proposed method achieves a 7% higher regression rate with a 65.14% reduction in FLOPS and 52.82% faster inference time compared to the current state-of-the-art method.",yes,The article uses vision-based methods for Human Action Quality Assessment and introduces a new dataset.,AGF-Olympics
10.1109/TIP.2024.3362135,Multimodal Action Quality Assessment,"Zeng, Ling-An; Zheng, Wei-Shi",2024,"Action quality assessment (AQA) is to assess how well an action is performed. Previous works perform modelling by only the use of visual information, ignoring audio information. We argue that although AQA is highly dependent on visual information, the audio is useful complementary information for improving the score regression accuracy, especially for sports with background music, such as figure skating and rhythmic gymnastics. To leverage multimodal information for AQA, i.e., RGB, optical flow and audio information, we propose a Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models modality-specific information and mixed-modality information. Our model consists of with three modality-specific branches that independently explore modality-specific information and a mixed-modality branch that progressively aggregates the modality-specific information from the modality-specific branches. To build the bridge between modality-specific branches and the mixed-modality branch, three novel modules are proposed. First, a Modality-specific Feature Decoder module is designed to selectively transfer modality-specific information to the mixed-modality branch. Second, when exploring the interaction between modality-specific information, we argue that using an invariant multimodal fusion policy may lead to suboptimal results, so as to take the potential diversity in different parts of an action into consideration. Therefore, an Adaptive Fusion Module is proposed to learn adaptive multimodal fusion policies in different parts of an action. This module consists of several FusionNets for exploring different multimodal fusion strategies and a PolicyNet for deciding which FusionNets are enabled. Third, a module called Cross-modal Feature Decoder is designed to transfer cross-modal features generated by Adaptive Fusion Module to the mixed-modality branch. Our extensive experiments validate the efficacy of the proposed method, and our method achieves state-of-the-art performance on two public datasets. Code is available at https://github.com/qinghuannn/PAMFN.",yes,The article uses vision-based methods for Human Action Quality Assessment and mentions using datasets.,
10.1007/s11760-021-01890-w,Temporal attention learning for action quality assessment in sports video,"Lei, Qing; Zhang, Hongbo; Du, Jixiang",2021,"This paper proposes an end-to-end temporal attention learning method to improve the performance of action quality assessment in sports video. For temporal weighted training, an attention-learning module is built to simulate the attention mechanism and judgement preference of human perception on action quality assessment. The weights are learned based on the loss of the segmented prediction errors and used to balance the significance of segmented features. We evaluate the proposed method on diving and gym-vault action of the benchmark AQA-7 dataset. The experimental results show that the proposed attention-aware feature training method is more effective than temporal aggregation and existing temporal relationship learning methods. Furthermore, only using the distance loss between the predicated score and the ground-truth score, without considering the ranking loss of different videos for training, this paper has achieved the state-of-the-art performance on both of the spearman rank correlation and mean Euclidean distance of the predicted scores against the judge's scores.",yes,The article uses vision-based methods for Human Action Quality Assessment and explicitly mentions a dataset.,AQA-7
10.3390/s19194129,A Survey of Vision-Based Human Action Evaluation Methods,"Lei, Qing; Du, Ji-Xiang; Zhang, Hong-Bo; Ye, Shuang; Chen, Duan-Sheng",2019,"The fields of human activity analysis have recently begun to diversify. Many researchers have taken much interest in developing action recognition or action prediction methods. The research on human action evaluation differs by aiming to design computation models and evaluation approaches for automatically assessing the quality of human actions. This line of study has become popular because of its explosively emerging real-world applications, such as physical rehabilitation, assistive living for elderly people, skill training on self-learning platforms, and sports activity scoring. This paper presents a comprehensive survey of approaches and techniques in action evaluation research, including motion detection and preprocessing using skeleton data, handcrafted feature representation methods, and deep learning-based feature representation methods. The benchmark datasets from this research field and some evaluation criteria employed to validate the algorithms' performance are introduced. Finally, the authors present several promising future directions for further studies.",no,The article is a survey.,
10.1109/TPAMI.2022.3194311,NAAQA: A Neural Architecture for Acoustic Question Answering,"Abdelnour, Jerome; Rouat, Jean; Salvi, Giampiero",2023,"The goal of the Acoustic Question Answering (AQA) task is to answer a free-form text question about the content of an acoustic scene. It was inspired by the Visual Question Answering (VQA) task. In this paper, based on the previously introduced CLEAR dataset, we propose a new benchmark for AQA, namely CLEAR2, that emphasizes the specific challenges of acoustic inputs. These include handling of variable duration scenes, and scenes built with elementary sounds that differ between training and test set. We also introduce NAAQA, a neural architecture that leverages specific properties of acoustic inputs. The use of 1D convolutions in time and frequency to process 2D spectro-temporal representations of acoustic content shows promising results and enables reductions in model complexity. We show that time coordinate maps augment temporal localization capabilities which enhance performance of the network by similar to 17 percentage points. On the other hand, frequency coordinate maps have little influence on this task. NAAQA achieves 79.5% of accuracy on the AQA task with similar to four times fewer parameters than the previously explored VQA model. We evaluate the performance of NAAQA on an independent data set reconstructed from DAQA. We also test the addition of a MALiMo module in our model on both CLEAR2 and DAQA. We provide a detailed analysis of the results for the different question types. We release the code to produce CLEAR2 as well as NAAQA to foster research in this newly emerging machine learning task.",no,The article is not related to vision-based methods or Human Action Quality Assessment (AQA). It focuses on Acoustic Question Answering.,
10.1016/j.jvcir.2022.103625,Skeleton-based deep pose feature learning for action quality assessment on figure skating videos,"Li, Huiying; Lei, Qing; Zhang, Hongbo; Du, Jixiang; Gao, Shangce",2022,"Most of the existing Action Quality Assessment (AQA) methods for scoring sports videos have deeply researched how to evaluate the single action or several sequential-defined actions that performed in short-term sport videos, such as diving, vault, etc. They attempted to extract features directly from RGB videos through 3D ConvNets, which makes the features mixed with ambiguous scene information. To investigate the effectiveness of deep pose feature learning on automatically evaluating the complicated activities in long-duration sports videos, such as figure skating and artistic gymnastic, we propose a skeleton-based deep pose feature learning method to address this problem. For pose feature extraction, a spatial-temporal pose extraction module (STPE) is built to capture the subtle changes of human body movements and obtain the detail representations for skeletal data in space and time dimensions. For temporal information representation, an inter-action temporal relation extraction module (ATRE) is implemented by recurrent neural network to model the dynamic temporal structure of skeletal subsequences. We evaluate the proposed method on figure skating activity of MIT-skate and FIS-V datasets. The experimental results show that the proposed method is more effective than RGB video-based deep feature learning methods, including SENet and C3D. Significant performance progress has been achieved for the Spearman Rank Correlation (SRC) on MIT-Skate dataset. On FIS-V dataset, for the Total Element Score (TES) and the Program Component Score (PCS), better SRC and MSE have been achieved between the predicted scores against the judge's ones when compared with SENet and C3D feature methods.",yes,The article uses vision-based methods for Human Action Quality Assessment and explicitly mentions multiple datasets.,MIT-Skate;FIS-V
10.1007/s11263-024-02146-z,Procedure-Aware Action Quality Assessment: Datasets and Performance Evaluation,"Xu, Jinglin; Rao, Yongming; Zhou, Jie; Lu, Jiwen",2024,"In this paper, we investigate the problem of procedure-aware action quality assessment, which analyzes the action quality by delving into the semantic and spatial-temporal relationships among various composed steps of the action. Most existing action quality assessment methods regress on deep features of entire videos to learn diverse scores, which ignore the relationships among different fine-grained steps in actions and result in limitations in visual interpretability and generalization ability. To address these issues, we construct a fine-grained competitive sports video dataset called FineDiving with detailed semantic and temporal annotations, which helps understand the internal structures of each action. We also propose a new approach (i.e., spatial-temporal segmentation attention, STSA) that introduces procedure segmentation to parse an action into consecutive steps, learns powerful representations from these steps by constructing spatial motion attention and procedure-aware cross-attention, and designs a fine-grained contrastive regression to achieve an interpretable scoring mechanism. In addition, we build a benchmark on the FineDiving dataset to evaluate the performance of representative action quality assessment methods. Then, we expand FineDiving to FineDiving+ and construct three new benchmarks to investigate the transferable abilities between different diving competitions, between synchronized and individual dives, and between springboard and platform dives to demonstrate the generalization abilities of our STSA in unknown scenarios, scoring rules, action types, and difficulty degrees. Extensive experiments demonstrate that our approach, designed for procedure-aware action quality assessment, achieves substantial improvements. Our dataset and code are available at https://github.com/xujinglin/FineDiving.",yes,"The article proposes a new method for action quality assessment, constructs and uses the FineDiving and FineDiving+ datasets, and the data is publicly available. It is related to computer science and uses vision-based methods.",FineDiving;FineDiving+
10.1038/s41597-022-01188-7,Functional movement screen dataset collected with two Azure Kinect depth sensors,"Xing, Qing-Jun; Shen, Yuan-Yuan; Cao, Run; Zong, Shou-Xin; Zhao, Shu-Xiang; Shen, Yan-Fei",2022,"This paper presents a dataset for vision-based autonomous Functional Movement Screen (FMS) collected from 45 human subjects of different ages (18-59 years old) executing the following movements: deep squat, hurdle step, in-line lunge, shoulder mobility, active straight raise, trunk stability push-up and rotary stability. Specifically, shoulder mobility was performed only once by different subjects, while the other movements were repeated for three episodes each. Each episode was saved as one record and was annotated from 0 to 3 by three FMS experts. The main strength of our database is twofold. One is the multimodal data provided, including color images, depth images, quaternions, 3D human skeleton joints and 2D pixel trajectories of 32 joints. The other is the multiview data collected from the two synchronized Azure Kinect sensors in front of and on the side of the subjects. Finally, our dataset contains a total of 1812 recordings, with 3624 episodes. The size of the dataset is 190 GB. This dataset provides the opportunity for automatic action quality evaluation of FMS.",yes,"The article presents a new vision-based dataset for automatic action quality evaluation of FMS, which is related to computer science.",Functional Movement Screen (FMS)
10.1007/s11263-022-01695-5,Automatic Modelling for Interactive Action Assessment,"Gao, Jibin; Pan, Jia-Hui; Zhang, Shao-Jie; Zheng, Wei-Shi",2023,"Action assessment, the task of visually assessing the quality of performing an action, has attracted much attention in recent years, with promising applications in areas such as medical treatment and sporting events. However, most existing methods of action assessment mainly target the actions performed by a single person; in particular, they neglect the asymmetric relations among agents (e.g., between persons and objects), limiting their performance in many nonindividual actions. In this work, we formulate a framework for modelling asymmetric interactions among agents for action assessment, considering the subordinations among agents in many interactive actions. Specifically, we propose an asymmetric interaction learner consisting of an automatic assigner and an asymmetric interaction network search module. The automatic assigner is designed to automatically group agents within an action into a primary agent (e.g., human) and secondary agents (e.g., objects); the asymmetric interaction network search module adaptively learns the asymmetric interactions between these agents. We conduct experiments on the JIGSAWS dataset containing surgical actions and additionally collect two new datasets, TASD-2 and PaSk, for action assessment on interactive sporting actions. The experimental results on these three datasets demonstrate the effectiveness of our framework in achieving state-of-the-art performance. The extensive experiments on the AQA-7 dataset also indicate the robustness of our model in conventional action assessment settings.",yes,"The article proposes a framework for action assessment, uses multiple datasets (JIGSAWS, AQA-7) and collects two new datasets (TASD-2, PaSk). It is related to computer science and uses vision-based methods for action quality assessment.",JIGSAWS;TASD-2;PaSk;AQA-7
10.1109/TCSVT.2020.3017727,Action Quality Assessment Using Siamese Network-Based Deep Metric Learning,"Jain, Hiteshi; Harit, Gaurav; Sharma, Avinash",2021,"Automated vision-based score estimation models can be used to provide an alternate opinion to avoid judgment bias. Existing works have learned score estimation models by regressing the video representation to ground truth score provided by judges. However, such regression-based solutions lack interpretability in terms of giving reasons for the awarded score. One solution to make the scores more explicable is to compare the given action video with a reference video, which would capture the temporal variations vis-a-vis the reference video and map those variations to the final score. In this work, we propose a new action scoring system termed as Reference Guided Regression (RGR), which comprises (1) a Deep Metric Learning Module that learns similarity between any two action videos based on their ground truth scores given by the judges, and (2) a Score Estimation Module that uses the first module to find the resemblance of a video with a reference video to give the assessment score. The proposed scoring model is tested for Olympics Diving and Gymnastic vaults and the model outperforms the existing state-of-the-art scoring models.",maybe,"The article proposes a new vision-based method for action quality assessment using deep metric learning. It implies the use of data for ""Olympics Diving and Gymnastic vaults"" but does not explicitly name any specific dataset.",
10.3390/electronics9040568,Learning Effective Skeletal Representations on RGB Video for Fine-Grained Human Action Quality Assessment,"Lei, Qing; Zhang, Hong-Bo; Du, Ji-Xiang; Hsiao, Tsung-Chih; Chen, Chih-Cheng",2020,"In this paper, we propose an integrated action classification and regression learning framework for the fine-grained human action quality assessment of RGB videos. On the basis of 2D skeleton data obtained per frame of RGB video sequences, we present an effective representation of joint trajectories to train action classifiers and a class-specific regression model for a fine-grained assessment of the quality of human actions. To manage the challenge of view changes due to camera motion, we develop a self-similarity feature descriptor extracted from joint trajectories and a joint displacement sequence to represent dynamic patterns of the movement and posture of the human body. To weigh the impact of joints for different action categories, a class-specific regression model is developed to obtain effective fine-grained assessment functions. In the testing stage, with the supervision of the action classifier's output, the regression model of a specific action category is selected to assess the quality of skeleton motion extracted from the action video. We take advantage of the discrimination of the action classifier and the viewpoint invariance of the self-similarity feature to boost the performance of the learning-based quality assessment method in a realistic scene. We evaluate our proposed method using diving and figure skating videos of the publicly available MIT Olympic Scoring dataset, and gymnastic vaulting videos of the recent benchmark University of Nevada Las Vegas (UNLV) Olympic Scoring dataset. The experimental results show that the proposed method achieved an improved performance, which is measured by the mean rank correlation coe fficient between the predicted regression scores and the ground truths.",yes,The article proposes a new vision-based method for human action quality assessment using skeletal representations from RGB videos. It uses the publicly available MIT Olympic Scoring dataset and the UNLV Olympic Scoring dataset.,MIT Olympic Scoring;UNLV Olympic Scoring
10.3390/app142311130,Evaluation of Human Action Based on Feature-Weighted Dynamic Time Warping,"Yan, Mingdie; Liu, Xia; Li, Zhaoyang; Guo, Naiyu",2024,"Action evaluation can automatically detect abnormal actions by evaluating the quality of human actions in specific postures, which is widely used in the field of rehabilitation medicine. This paper proposes an intelligent rehabilitation action evaluation system to evaluate the quality of patients' actions during rehabilitation training, which helps medical professionals to more effectively monitor and guide the process, thus improving rehabilitation effects. Firstly, we collected human skeletal key-point data based on a depth camera and processed these data with gap filling and filtering; then, the effective data segments were segmented from the whole action dataset, angle and distance features were extracted, and the feature matrix was obtained; then, we used the Euclidean Barycenter Dynamic Time Warping-Barycenter Averaging algorithm to produce action templates; finally, we proposed a Feature-Weighted Dynamic Time Warping algorithm to calculate the similarity between the detected action and the template action and established an action achievement score mechanism to evaluate the rehabilitation action. The experimental results show that compared with the action evaluation method based on feature-matrix DTW, the proposed method significantly improves the similarity between healthy people and patients, and the similarity improvement for patients is more significant. Based on similarity scores, the difference between the actions of healthy people and patients and the template actions is more than 80%, which shows that the method can evaluate action quality in people with different health conditions and effectively reduce error in action evaluation. The confidence level of the action achievement score mechanism reaches 99%, which meets the actual application requirements.",yes,"The article proposes an intelligent rehabilitation action evaluation system using a new algorithm. It collected human skeletal key-point data using a depth camera for action quality evaluation, which is related to computer science and uses vision-based methods.",
10.1109/JIOT.2023.3301795,Efficient Massive-Device Orchestration Through Reinforcement Learning With Boosted Deep Deterministic Policy Gradient,"Shi, Haowei; Zou, Jiadao; Zhang, Qingxue",2024,"Big data innovations are boosted by massive devices that capture a large amount of dynamics from human or environment and further mine the insights hidden in the dynamics. However, the challenge arises in the complex massive-device orchestration, meaning that it is essential to configure and manage the massive devices and the gateway/server. The complexity, on the massive wearable or Internet of Things devices, lies in the diverse energy budget, computing efficiency, and communication channel conditions. On the phone or server side, it lies in how the global diversity can be analyzed and how the system configuration can be optimized. Targeting this obstacle, we propose a new reinforcement learning architecture, called boosted deep deterministic policy gradient, with enhanced actor-critic co-learning and multiview state-transformation. More specifically, the proposed actor-critic co-learning allows for enhanced dynamics abstraction through the shared neural network component. Further, the state-transformation, with multiple parallel learning agents, greatly boosts the action quality and learning process. Evaluated on complex massive-device orchestration tasks, the proposed deep reinforcement learning framework has achieved much more efficient system configurations with enhanced computing capabilities and energy efficiency. This study will greatly advance massive-device system configuration through deep learning and reinforcement rewarding mechanisms, toward efficient big data practices.",no,The article is not related to vision-based methods and does not focus on Human Action Quality Assessment. It also does not explicitly mention the use or production of a specific dataset.,
